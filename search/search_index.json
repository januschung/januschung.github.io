{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Home","text":""},{"location":"#hello-im-janus-chung","title":"Hello, I'm Janus Chung","text":"I'm a Software Engineer with over 20 years of experience across DevOps, Developer, QA and Support.   I\u2019ve recently joined    Flagler Health      Flagler Health empowers providers by leveraging the power of AI to help provide clinical and back-office support  where I\u2019m leading DevOps efforts and helping scale the infrastructure from the ground up.   Previously, at     Ad Hoc.     a digital services company that helps the federal government better serve people   I worked on standing up Kubernetes (EKS) and other AWS infrastructure for the U.S. Department of Veterans Affairs applications, and collaborated with our government partners to implement GitHub Actions, Argo CD, and Argo Rollouts for continuous delivery and progressive deployment workflows.   Before that, I spent over seven years with         Integral Ad Sciences,     a global leader in digital media quality. IAS makes every impression count, ensuring that ads are viewable by real people, in safe and suitable environments, activating contextual targeting, and driving supply path optimization.  leading efforts to migrate monoliths to AWS Cloud and modernize CI/CD pipeline."},{"location":"#projects","title":"ProjectsJob WinnerMath Worksheet GeneratorAsian Character Worksheet Generator","text":"<p>I love building stuffs and learning new technology. I build open source projects with what I learned from time to time. Checkout the Opensource projects section.</p> Java Reactive React Material UI PostgreSQL Graphql Docker Github Actions Python Pytest Github Actions <p> Python Pytest Github Actions </p>"},{"location":"#resume","title":"Resume","text":"<p> Work Experience  Resume</p>"},{"location":"#professional-skills","title":"Professional Skills","text":"DevOps <ul> <li> Terraform</li> <li> AWS, Oracle Cloud</li> <li> Jenkins</li> <li> Github Actions</li> <li> Docker</li> <li> Kubernetes</li> <li> Argo CD, Argo Rollouts</li> <li> DataDog</li> <li> Auth0</li> <li> Vault</li> </ul> Development <ul> <li> Java, Springboot, Spring Reactive</li> <li> Python</li> <li> PHP </li> <li> JavaScript, React, Angular</li> <li> Go</li> <li> MySQL,  Postgres, :simple-amazondynamodb: DynamoDB</li> <li>REST,  GraphQL</li> </ul> QA <ul> <li>JUnit, Mockito</li> <li>TDD, Cucumber</li> <li> Pytest</li> <li> Selenium WebDriver</li> </ul>"},{"location":"#what-i-like","title":"What I like","text":"Photography <p>I used to take a lot of pictures with my Sony Alpha A900. During that period of time, people portrait and wedding documentary were my favorite. It was my honor to help two of my best friends and one of my primary school classmate to capture their big moments. I will upload some of my work in the coming future.</p> Hi-Fi <p> </p> <p>The first time I listened to a real Hi-End Hi-Fi, I knew it my life would no longer be the same.  Thanks to my late Uncle 17 who brought me to the Hi-Fi world when I was 12. My current setup is DM602 S2 with a Mid range Yamaha AV Receiver.</p> Reading <p> </p> <p>Inspired by my secondary school class teacher, who told me his story about reading latest books from the bookstore when he could not afford it during his adolesencent age , I picked up reading as a hobby. My reading record is 54 books in a year.</p> Woodworking <p> </p> <p>I built couple furnitures for my home with the skills I learned from secondary school D&amp;T class, as well as from a summer job in a double deck bus repairing factory. Working with wood always gives me a peaceful feeling.</p>"},{"location":"about-me/about-this-homepage/","title":"About this Homepage","text":"<p>While chatting with my ex-coworker and buddy about job hunting and project, we came up with an idea to build a personal homepage since a lot of job application is asking for it.</p> <p>I think it is a good idea since it is a good way to showcase not only the skillset and work experience in a more presentable way, compare to boring pdf/word document, but also provides more content about how we are outside of work.</p>"},{"location":"about-me/about-this-homepage/#mkdocs-material","title":"Mkdocs Material","text":"<p>From the new trick that I learned in my new job, I built this personal homepage once again (the last time was a school project using plain HTML and javascript). It uses Mkdocs Material which provides a simple but yet professional theme that works well in any devices with built in responsiveness. It is easy to use without any coding skill whatsoever, as it just requires some light modification of a configuration yaml file and adding/editing markdown file(s) for your contents.</p>"},{"location":"about-me/about-this-homepage/#free-publishing-and-hosting-with-github-page","title":"Free Publishing and Hosting with Github Page","text":"<p>Another good thing is, not only the source code of the homepage can be hosted on Github, but also the web hosting is provided for free with Github Pages. Powered with Github Action, Github will build and deploy the homepage automatically. The process is fully automated so that the next time you when add or edit your repo, the change will be deployed within a minute without further work required.</p> <p>I hope this can help my buddy to build a good porfolio, as well as to get a dream job soon in this troublesome tech job market. If anyone of you is interested in learning more, feel free to contact and I am happy to help.</p>"},{"location":"about-me/about-this-homepage/#further-readings","title":"Further readings","text":"<p>Mkdocs Material</p> <p>Github Action to publish your site</p> <p>Github Pages</p>"},{"location":"about-me/certificate/","title":"Certificate","text":""},{"location":"about-me/certificate/#learn-devops-infrastructure-automation-with-terraform","title":"Learn DevOps: Infrastructure Automation With Terraform","text":"<ul> <li>Terraform course material</li> </ul>"},{"location":"about-me/certificate/#the-complete-github-actions-workflows-guide","title":"The Complete GitHub Actions &amp; Workflows Guide","text":"<ul> <li>GitHub Documentation</li> <li>Simple Docke NodeJs API repo</li> <li>Send Slack Message with Docker</li> <li>Surge - Service to deploy and test static website</li> <li>Semantic Versioning</li> </ul>"},{"location":"about-me/certificate/#argo-cd-essential-guide-for-end-users-with-practice","title":"Argo CD Essential Guide for End Users with Practice","text":"<ul> <li>ArgoCD GitHub Repo</li> <li>Example App</li> </ul>"},{"location":"about-me/certificate/#build-reactive-microservices-using-spring-webfluxspringboot","title":"Build Reactive MicroServices using Spring WebFlux/SpringBoot","text":"<ul> <li>Course GitHub Repo</li> </ul>"},{"location":"about-me/certificate/#become-a-superlearner-2-learn-speed-reading-boost-memory","title":"Become a SuperLearner\u00ae 2: Learn Speed Reading &amp; Boost Memory","text":""},{"location":"about-me/certificate/#apache-kafka-series-learn-apache-kafka-for-beginners-v3","title":"Apache Kafka Series - Learn Apache Kafka for Beginners v3","text":"<ul> <li> <p>Kafka basics from Conduktor.io</p> </li> <li> <p>Sample Code</p> </li> </ul>"},{"location":"about-me/certificate/#web-developer-bootcamp-with-flask-and-python","title":"Web Developer Bootcamp with Flask and Python","text":"<ul> <li>Github repo for Habbit Tracker Project</li> <li>Github repo for Profolio Project</li> <li>Github repo for Movie Watchlist Project</li> </ul>"},{"location":"about-me/certificate/#go-the-complete-developers-guide-golang","title":"Go: The Complete Developer's Guide (Golang)","text":""},{"location":"about-me/certificate/#the-complete-ruby-on-rails-developer-course","title":"The Complete Ruby on Rails Developer Course","text":""},{"location":"about-me/certificate/#istio-hands-on-for-kubernetes","title":"Istio Hands-On for Kubernetes","text":""},{"location":"about-me/certificate/#spring-security-6","title":"Spring Security 6","text":"<ul> <li>Github repo for Easy Bytes Spring Secutiry 6</li> </ul>"},{"location":"about-me/certificate/#restful-web-services-java-spring-boot-spring-mvc-and-jpa","title":"RESTful Web Services, Java, Spring Boot, Spring MVC and JPA","text":"<ul> <li>Github repo for the course</li> </ul>"},{"location":"about-me/hackathon-champion/","title":"Hackathon Champion","text":"<p>It was one of my best achievements in career to win the company hackathon leading a team of five engineers from different teams.</p> <p></p>"},{"location":"about-me/hackathon-champion/#background","title":"Background","text":"<p>It began with the time when I was bored working as a QA Automation Engineer in the company for 4 years and I would like to do more outside of my conform zone.</p> <p>I was bored</p> <p>Working as a QA Automation Engineer with day to day repetitive tasks was very boring. I was looking for new challenge.</p> <p>In a quarterly PI planning, when all of the engineers around the world came to the NYC office, I told my team lead that I wanted to install our flagship platform on my laptop as a local development environment so that I could learn more about how it worked. He pointed me to a Confluence page with instruction that took 20 scrolls to reach to the bottom.</p>"},{"location":"about-me/hackathon-champion/#prerequisite","title":"Prerequisite","text":"<ol> <li>Java</li> <li>Web container</li> <li>Database</li> <li>Cache server</li> <li>Five internal repo</li> </ol> <p>After that came with lengthy instruction to setup configurtation for various servers and repo</p>"},{"location":"about-me/hackathon-champion/#configuration","title":"Configuration","text":"<ol> <li>Web container configuration</li> <li>Java Spring properties</li> <li>Application configuration</li> <li>Database configuration</li> <li>Script to prepopulate cache</li> <li>download third party files</li> <li>10+ more things</li> </ol> <p>A whole week to setup</p> <p>It took an average of a week to setup the monolith for new hired. New hires were asking for help in the support slack channel desperately.</p> <p>That was a tedious process and it took me 2 days to set it up! During the process, I had to consult with three different senior developers. It turned out that I found at least two outdated instructions from the Confluence page.</p> <p>Not ideal.</p> <p>From that point, I spent a week of free time trying to simplify the installation. I did not want anyone to suffer anymore, especially for the new hires. I tried to Dockerize the monolith plaform. However, I failed due to my lack of domain knowledge from a development standpoint. Besides, my Docker skill was still green.</p>"},{"location":"about-me/hackathon-champion/#hackathon","title":"Hackathon","text":"<pre><code>flowchart RL\n  subgraph Web Container Image\n  F{{config a}} --&gt; A\n  G{{config b}} --&gt; A\n  H{{config c}} --&gt; A\n  war([Monolith artifact]) --&gt; A\n  js([js artifact]) --&gt; A\n  end\n  A(((Web Container))) --&gt; B[(Database)]\n  subgraph Cache Service Image\n  A --&gt; C[(Cache Server)]\n  D[Cache Warming Job] --&gt; C\n  end\n  subgraph Database Image\n  E[Liquibase/Flyway] --&gt; B\n  J{{DB Setup Script}} --&gt; B\n  end</code></pre> <p>Two years later, after I transferred into a development and devops role, the Hackathon came. While brainstorming with a devops buddy for potential team idea, he challenged me to Dockerize the monolith platform. That reignited my hidden burning desire to give it another try.</p>"},{"location":"about-me/hackathon-champion/#easy-first-draft","title":"Easy First Draft","text":"<p>At first, I thought it would be hard since I failed two years ago. However, it turned out to be a 2 hours easy work to build the first working draft!</p> <p>When I think back now, here were the reasons:</p> <ol> <li> <p>The database had already been containerized in those two years. It was a team effort between another developer and me with our free time.</p> </li> <li> <p>I gained more domain knowledge of the monolith plaform. What didn't make sense from the server and application logs in the past became meaningful during the troubleshooting process.</p> </li> <li> <p>I was more familiar with most of the configuration files and fields since I had been working on the codebase for the past two years.</p> </li> <li> <p>My Docker skill had grown so much from the work as well as from the home server setup that I did during my free time.</p> </li> </ol> <p>The first draft was working with the following setup:</p> <ol> <li>Database in a Docker</li> <li>Local cache server with warm up script ran manually</li> <li>Monolith Java war file in a Docker web container with required configuration files</li> <li>Monolith was setup to wire with the database in Docker and the local cache server</li> </ol> <p>It worked! Since I had one and half day left, I spent the rest of the time to automate everything together. My plan was to have the monolith be brought up with a single command.</p>"},{"location":"about-me/hackathon-champion/#final-draft","title":"Final Draft","text":"<p>I continuted to Dockerize other components, including the cache server and its cache warm up process. I also made the new repo as the single source of the truth for all the configuration files. Here is the final docker-compose file (with some naming modification):</p> <pre><code>version: '2.3'\n\nnetworks:\n  internal:\n    driver: bridge\n    ipam:\n      driver: default\n      config:\n        - subnet: 172.16.238.0/24\n\nservices:\n  service-a:\n    image: the-cache-server\n    ports:\n      - \"22122:22122\"\n    networks:\n      internal:\n        ipv4_address: 172.16.238.2\n  db:\n    image: internal-cr/the-db-image:latest\n    init: true\n    ports:\n      - \"1234:1234\"\n    networks:\n      internal:\n        ipv4_address: 172.16.238.3\n  monolith:\n    image: internal-cr/monolith:latest\n    ports:\n      - \"9090:9090\"\n      - \"9091:9091\"\n    depends_on:\n      - service-a\n      - db\n    volumes:\n      - ./etc/webcontainer/:/etc/webcontainer/\n      - ./artifact/monolith/:/usr/local/webcontainer/monolith/\n      - ./artifact/monolith-js:/var/data/deploy/monolith-js\n      - ./var/data/conf:/var/data/conf\n      - ./config:/var/data/config\n    hostname: monolith\n    networks:\n      internal:\n        ipv4_address: 172.16.238.4\n</code></pre> <p>Compared to following a 10 pages Confluence page and two days to setup a local development environment for the monolith, the process became a simple 4 steps task:</p> <ol> <li>Build and drop the Java war file to the <code>artifact/monolith</code> directory</li> <li>Get the latest monolith-js artifact and drop it to the <code>artifact/monolith-js</code> directory</li> <li>Build the Docker image</li> <li>Bring up the monolith with <code>docker-compose up</code></li> </ol>"},{"location":"about-me/hackathon-champion/#follow-up-meeting","title":"Follow Up Meeting","text":"<p> At that point, I didn't really care much if I would win the hackathon since I've already achieved something that I couldn't do in the past. I also believed that the project will help everyone in the future to get onboard within an hour or two compares to days of tedious manual setup.</p> <p>However, I understood that if my voice was not being heard, all the effort would just stay as a dead project without really helping anyone.</p> <p>For that, I made a bold move to call off a meeting with all the related party, plus some big guys:</p> <ul> <li>Director and Manager of SysOps</li> <li>Director of DevOps</li> <li>Managers and Tech Leads from two teams</li> <li>CTO, VP of Engineering</li> </ul> <p>In an hour of meeting, besides descring what the project has solved and the reason behind it, I brought up the following that the project could potentially help:</p> <ul> <li>To migrate the CICD process of the Dockerized monolith application to Kubernetes </li> <li>To run acceptance test and regression in parallel faster with horizontal scaling</li> <li>To test web container, Java and database update easily</li> </ul> <p>Suprisingly, these were some of the problem which they would like to solve on their roadmap or backlog for the next two years. We ended up creating epics for respective team to further develop the project. It was a big success.</p>"},{"location":"about-me/hackathon-champion/#hackathon-result","title":"Hackathon Result","text":"<p>It still feel so surreal and overwhelming to win the hackathon for the company. The most valuable part was that I learned to step up and organized the meetings with the right people to drive cross team collaboratoin and further develop the project. I also gained the trust within the Engineering and Product teams and built great connection and friendship with the team members.</p>"},{"location":"about-me/work-experience/","title":"Work experience","text":"Flagler Health  New York, NY (Remotely Based) AD HOC LLC  WASHINGTON, DC (Remotely Based) INTEGRAL AD SCIENCE (IAS)  NEW YORK, NY (Remotely Based) IV INTERACTIVE  JERERY CITY, NJ MEDIA THREE CORPORATION  NEW YORK, NY L &amp; L TRAVEL ENTERPRISE, INC  NEW YORK, NY MERCK &amp; CO., INC  MORRISTOWN, NJ DATASYNAPSE, INC  NEW YORK, NY APPLIMATION, INC  NEW YORK, NY"},{"location":"about-me/work-experience/#flagler-health","title":"Flagler Health","text":"<p> SOFTWARE ENGINEER  Apr 2025 - Present </p> <ul> <li>Implemented Sealed Secrets to securely manage sensitive Kubernetes configuration, enabling safe GitOps workflows across the team.</li> <li>Established Terraform S3 backend with DynamoDB state locking, providing a reliable and consistent infrastructure state management solution.</li> <li>Bootstrapped Terraform adoption, including importing and managing existing AWS resources to improve visibility and IaC control.</li> <li>Set up Turborepo remote caching using AWS Lambda and S3, reducing CI build times by over 80% and improving developer productivity.</li> <li>Defined GitOps as the source of truth using Argo CD, including decoupling Helm-based deployment logic from application code repositories for better modularity and scalability.</li> <li>Upgraded Argo CD from v2.12 to v3.0.2, ensuring continued support, access to new features, and security improvements with minimal disruption.</li> <li>Integrated Slack notifications with Argo CD, enabling real-time deployment alerts to improve team visibility and incident response.</li> <li>Developed reusable Terraform modules for:<ul> <li>Production static site delivery (S3 + CloudFront + ACM)</li> <li>Backend service storage (S3 + IAM)</li> <li>Loki logging stack (S3 + IAM + IRSA)</li> <li>Airbyte data ingestion (S3 + IAM + KMS)</li> <li>Private ECR repositories with lifecycle policies</li> <li>Tailscale bastion host for subnet routing (EC2 + Tailscale)</li> </ul> </li> <li>Migrated EKS networking from NordLayer to Tailscale, using a self-managed EC2 subnet router to provide secure, private access to internal services.</li> <li>Deployed Airbyte OSS on EKS with Helm, replacing a local setup with a scalable, production-grade architecture integrated with S3 and IAM.</li> <li>Set up Loki and Grafana in EKS for centralized logging and metrics, improving observability across services.</li> <li>Built an internal Slack bot using Python and the Slack Bolt SDK to automate infrastructure-related tasks like version reporting.</li> <li>Introduced Argo Rollouts for progressive delivery (canary and blue-green), enabling safer and more controlled Kubernetes deployments.</li> </ul>"},{"location":"about-me/work-experience/#ad-hoc-llc","title":"AD HOC LLC","text":"<p> SENIOR SOFTWARE ENGINEER DEVOPS  Feb 2023 - Mar 2025 </p> <ul> <li>Built and maintained CI/CD pipelines using Github Actions, Argo CD, DataDog and Hashicorp Vault.</li> <li>Built and deployed Infrastructure as Code using Terraform to manage AWS resources.</li> <li>Leveraged blue green deployments with Argo Rollouts to improve release strategies and minimize downtime.</li> <li>Created standardized platform EKS service to get Veterans Affairs API services onboard to AWS.</li> <li>Created sample Java, Python and Ruby app and helm charts as reference for tenant to get onboard easier.</li> <li>Built and managed Code VA, a catalog for discoverying software within Department of Veterans Affairs.</li> <li>Migrated cache and database components to Elasticache and RDS using Kubernetes CRD.</li> <li>Migrated integration tests from Github Actions to Argo Rollouts's pre and post analysis template.</li> </ul>"},{"location":"about-me/work-experience/#integral-ad-science","title":"INTEGRAL AD SCIENCE","text":"<p> STAFF SOFTWARE ENGINEER  Jun 2020 - Dec 2022 </p> <ul> <li>Built multiple microservices to migrate legacy monolith applications over to Kubernetes in AWS EKS which made feature updates faster and improved developer experience.</li> <li>Designed and developed APIs for IAS pre-bid and post-bid platform using Spring boot with a MySQL backend running in Tomcat. This project enabled IAS\u2019 clients to integrate with our targeting segments quickly.</li> <li>Created a new microservice in AWS via ECS/Fargate. It was called Reporting API and it allowed clients to request reports programmatically.</li> <li>Worked on integrating systems developed by the acquired company (Admantx). We had to receive their data and incorporate it as part of our pre-pid feature set.</li> <li>Created multiple mock services with Spring Boot and Cucumber framework to improve development in non-production environment and enhance automated tests.</li> <li>Mentored and helped new engineers integrate in IAS as part of the co-pilot program.</li> <li>Created automated test-suites in Python for regression and acceptance tests.</li> <li>Dockerized an existing monolith Spring application to improve developer experience. This project won in the company hackathon.</li> <li>Conducted bi-weekly Lunch and Learn sessions for the team to present and discuss new technologies, and brainstorm improvement for our current tech stack.</li> </ul> <p> STAFF QAA ENGINEER  Jun 2015 - May 2020 </p> <ul> <li>Created headless XVFB and Chrome/Firefox test framework to speed up testing time by 80%.</li> <li>Created a reusable testing environment for different products using Puppet that spins up AWS EC2 environments on demand.</li> <li>Refactored legacy regression suite and cut testing time from 90 to 25 minutes.</li> </ul>"},{"location":"about-me/work-experience/#iv-interactive","title":"IV INTERACTIVE","text":"<p> FULL STACK SOFTWARE ENGINEER  Aug 2014 - Jun 2015 </p> <ul> <li>Designed and created a HIPPA compliant Web Application for IV Interactive using Laravel(PHP framework), MySQL, jQuery, Bootstrap, Skeleton, SASS/CSS3 and HTML5.</li> <li>Created and maintained unit tests for the above using PHPUnit.</li> <li>Proposed and implemented secure backups of codebase and database with PGP data encryption.</li> </ul>"},{"location":"about-me/work-experience/#media-three-corporation","title":"MEDIA THREE CORPORATION","text":"<p> FULL STACK SOFTWARE ENGINEER  Jun 2013 - Aug 2014 </p> <ul> <li>Implemented customer support system using CakePHP(PHP framework), MySQL, JQuery, CSS3. This was used to monitor customer complaints and inquiries.</li> </ul>"},{"location":"about-me/work-experience/#l-l-travel-enterprise-inc","title":"L &amp; L TRAVEL ENTERPRISE, INC","text":"<p> SENIOR SOFTWARE ENGINEER DEVOPS  Aug 2012 - Jun 2013 </p> <ul> <li>Created and maintained the Online booking/tracking system for 500 travel agents using IIS, ASP, Javascript and SQL Server.</li> </ul>"},{"location":"about-me/work-experience/#merck-co-inc","title":"MERCK &amp; CO., INC","text":"<p> INFORMATICA DEVELOPER (Consultant)  Jun Jun 2011 - Feb 2012 </p> <ul> <li>Worked on adding features and upgraded Informatica from version 4.2 to 5.3</li> </ul>"},{"location":"about-me/work-experience/#datasynapse-inc","title":"DATASYNAPSE, INC","text":"<p> SENIOR SUPPORT ENGINEER  May 2007 - Nov 2009 </p> <ul> <li>Provided technical support for customers using DataSynapse software, a distributed computing application, on their mission-critical production systems in various platforms.</li> <li>Developed sample code/examples of the DataSynapse API to make integrations for clients easier.</li> </ul>"},{"location":"about-me/work-experience/#applimation-inc","title":"APPLIMATION, INC","text":"<p> INFORMIA SUPPORT CONSULTANT  May 2005 - Apr 2007 </p> <ul> <li>Provided technical support for customers using Applimation software.</li> <li>Developed custom Oracle/PeopleSoft/Siebel SQL metadata for customers.</li> </ul>"},{"location":"blog/","title":"My Blog Posts","text":""},{"location":"blog/2025/07/25/replatforming-airbyte-from-developer-laptop-to-eks/","title":"Replatforming Airbyte: From Developer Laptop to EKS","text":""},{"location":"blog/2025/07/25/replatforming-airbyte-from-developer-laptop-to-eks/#originally-posted-at-linkedin-at-july-25-2025","title":"originally posted at LinkedIn at July 25, 2025","text":"<p>In early-stage engineering teams, it's natural for tools to start out simple \u2014 often running on a single developer machine, just to get things moving. That\u2019s how our Airbyte setup began: quick to spin up, good enough for testing connectors, and easy to iterate on.</p> <p>But as our team grew and data pipelines became more embedded in how we operated, we knew it was time to treat Airbyte like real infrastructure. That meant moving beyond local environments and into a scalable, secure, and repeatable deployment.</p> <p>We migrated Airbyte OSS to Amazon EKS, using Helm and AWS-native services like S3 and IAM Roles for Service Accounts (IRSA). Our goal wasn\u2019t to fix something broken, but to build on what was working and make it production-ready\u2014without sacrificing developer velocity.</p> <p>This post shares how we did it, what we learned, and what you might want to consider if you\u2019re operationalizing Airbyte (or any similar open-source tool) in a small but growing cloud-native team.</p> <p></p>"},{"location":"blog/2025/07/25/replatforming-airbyte-from-developer-laptop-to-eks/#from-local-dev-to-platform-service","title":"\ud83d\udce6 From Local Dev to Platform Service","text":"<p>The starting point was a typical OSS install: Airbyte running via Docker Compose, with internal Postgres and MinIO providing storage.</p> <p>For a single user, this setup works. But as sync jobs became more important \u2014 and more cloud-integrated \u2014 the limitations grew obvious: - Only one person could run or inspect jobs - Jobs stopped if the machine rebooted or slept - The setup didn\u2019t integrate cleanly with AWS services like KMS or IAM</p> <p>We needed to make Airbyte a first-class internal service \u2014 which meant replatforming it onto our Kubernetes infrastructure.</p>"},{"location":"blog/2025/07/25/replatforming-airbyte-from-developer-laptop-to-eks/#helm-first-on-eks","title":"\ud83d\ude80 Helm-First on EKS","text":"<p>We deployed Airbyte using the official Helm chart on Amazon EKS, with all configuration defined in a single <code>values.yaml</code> file.</p> <p>This made it easy to: - Version control the deployment - Propagate changes to dev, stage, or prod clusters - Configure each Airbyte component with its own replica count, resources, and ingress</p> <p>We enabled the core stack: - <code>webapp</code>, <code>server</code>, and <code>worker</code> components - Internal Postgres with persistent volumes - An Ingress controller (in our case, Tailscale) for secure access</p> <p>All of this is managed by Helm, making deployment and upgrades consistent and CI/CD-friendly.</p>"},{"location":"blog/2025/07/25/replatforming-airbyte-from-developer-laptop-to-eks/#why-we-abandoned-minio","title":"\u2601\ufe0f Why We Abandoned MinIO","text":"<p>MinIO ships by default in Airbyte\u2019s OSS Helm chart \u2014 and it\u2019s fine if you're storing state internally.</p> <p>But as soon as we configured an S3 destination connector, we ran into a hard blocker.</p> <p>Airbyte\u2019s connector expects actual AWS S3 \u2014 not a MinIO alias. The authentication flows, endpoint discovery, and SDK behavior are incompatible. The result? Jobs fail, and you lose the ability to use one of Airbyte\u2019s most valuable features.</p> <p>So we turned off MinIO and went all-in on real S3: <pre><code>global:\n  storage:\n    type: s3\n    bucket:\n      log: &lt;BUCKET_NAME&gt;\n      state: &lt;BUCKET_NAME&gt;\n      workloadOutput: &lt;BUCKET_NAME&gt;\n    s3:\n      region: &lt;AWS_REGION&gt;\n      authenticationType: instanceProfile\nminio:\n  enabled: false\n</code></pre></p> <p>This cleared the path for both internal object storage and S3 destination connectors \u2014 no hacks required.</p>"},{"location":"blog/2025/07/25/replatforming-airbyte-from-developer-laptop-to-eks/#evolving-iam-from-node-role-to-irsa","title":"\ud83d\udd10 Evolving IAM: From Node Role to IRSA","text":"<p>To integrate Airbyte with S3 securely, we started with the simplest approach: attaching S3 access to the EKS node role. This allowed Airbyte to access S3 without static credentials\u2014a good start, but too broad.</p> <p>Later, we switched to IAM Roles for Service Accounts (IRSA) to scope permissions only to Airbyte pods.</p> <p>We configured: - An IRSA role (<code>airbyte-irsa-role-prod</code>) in Terraform. - A trust policy bound to the <code>airbyte-admin</code> service account. - A Helm annotation to bind that IAM role: <pre><code>serviceAccount:\n  annotations:\n    eks.amazonaws.com/role-arn: arn:aws:iam::&lt;AWS_ACCOUNT&gt;:role/airbyte-irsa-role-prod\n</code></pre></p> <p>This let us remove reliance on the NodeInstanceRole and grant least-privilege S3 + KMS access directly to Airbyte.</p>"},{"location":"blog/2025/07/25/replatforming-airbyte-from-developer-laptop-to-eks/#tuning-resources-for-job-efficiency","title":"\u2699\ufe0f Tuning Resources for Job Efficiency","text":"<p>One thing that stood out during testing: Airbyte jobs default to higher resource allocations than expected \u2014 likely to support heavy syncs across large datasets.</p> <p>But for our workloads, that was overkill.</p> <p>We explicitly set <code>jobs.resources</code> to more modest requests and limits: <pre><code>global:\n  jobs:\n    resources:\n      requests:\n        memory: 256Mi\n        cpu: 250m\n      limits:\n        memory: 1Gi\n        cpu: 200m\n</code></pre></p> <p>This helped reduce noise in our Kubernetes scheduler, especially when running multiple concurrent jobs. If you're deploying Airbyte in an environment with resource quotas or shared node pools, this is a worthwhile tweak.</p>"},{"location":"blog/2025/07/25/replatforming-airbyte-from-developer-laptop-to-eks/#stable-auditable-cloud-integrated","title":"\ud83e\uddf1 Stable, Auditable, Cloud-Integrated","text":"<p>With the final setup: - Airbyte OSS runs fully inside EKS. - Storage is backed by real S3, with KMS support. - IAM is scoped via IRSA \u2014 no static secrets anywhere. - Ingress is handled through our standard controller (Tailscale). - PostgreSQL is deployed in-cluster with persistent volume claims.</p> <p>It\u2019s reproducible, secure, and fits neatly into how we manage infrastructure.</p>"},{"location":"blog/2025/07/25/replatforming-airbyte-from-developer-laptop-to-eks/#key-lessons-for-platform-teams","title":"\u2705 Key Lessons for Platform Teams","text":"<p>If you're supporting internal data or integration teams: - MinIO doesn't work with S3 destination connectors \u2014 we learned this the hard way and switched to real S3 - Start simple with IAM, then evolve to IRSA as your use case grows - Use Helm to its full extent \u2014 the Airbyte chart is surprisingly complete - Tune job resources \u2014 the defaults may be more than you need</p> <p>By replatforming Airbyte this way, we made it stable, scalable, and production-ready \u2014 not just for one user, but for the whole company.</p>"},{"location":"blog/2023/04/28/auto-increment-with-postgres-and-jpa/","title":"Auto Increment with Postgres and JPA","text":""},{"location":"blog/2023/04/28/auto-increment-with-postgres-and-jpa/#background","title":"Background","text":"<p>I want to have Postgres to auto generate ID for my Springboot application entity, using Liquibase.</p>"},{"location":"blog/2023/04/28/auto-increment-with-postgres-and-jpa/#what-didt-work","title":"What did't work","text":"<p>At first I had the following pair for the Java Entity Class and Liquibase setup: </p> <pre><code>@Id\n@GeneratedValue(strategy = GenerationType.AUTO)\n@Column(name = \"ID\")\nprivate Long id;\n</code></pre> <pre><code>&lt;createTable tableName=\"TICKET\"&gt;\n    &lt;column autoIncrement=\"true\" name=\"ID\" type=\"BIGINT\" remarks=\"Primary key\"&gt;\n        &lt;constraints primaryKey=\"true\" /&gt;\n    &lt;/column&gt;\n</code></pre> <p>which produced the following error:</p> <pre><code>org.postgresql.util.PSQLException: ERROR: relation \"support_system.ticket_seq\" does not exist\n</code></pre> <p>Since it was complainting about the missing sequence, I modified the Liquibase setting to</p> <pre><code>&lt;createTable tableName=\"TICKET\"&gt;\n    &lt;column defaultValueSequenceNext=\"seq_name\" name=\"ID\" type=\"BIGINT\" remarks=\"Primary key\"&gt;\n        &lt;constraints primaryKey=\"true\" /&gt;\n    &lt;/column&gt;\n</code></pre> <p>which brought me to a different bug since I am using a non public schema.</p>"},{"location":"blog/2023/04/28/auto-increment-with-postgres-and-jpa/#the-solution","title":"The Solution","text":"<p>After some desperating debugging hours, I consulted my good friend Mark, the SQL guru, who convinced me that I do not need to setup a sequence. Instead, let Postgres generates one for me.</p> <p>Mark's tips</p> <p>You don\u2019t generally need to reference the sequence object explicitly</p> <p>By doing so, I can just create a column using the standard SQL identity, for example, <code>id int generated always as identity</code>. That will create the sequence automatically and automatically call nextval to get a new id value when a new row is inserted.</p> <p>After listened to Mark, I was convinced that I actually did not need to setup a sequence. Therefore, I reversed my chance in Liquibase and continued to try different thing on the JPA side. Here are the setting that finally works:</p> <pre><code>@Id\n@GeneratedValue(strategy = GenerationType.IDENTITY)\n@Column(name = \"ID\")\nprivate Long id;\n</code></pre> <pre><code>&lt;createTable tableName=\"TICKET\"&gt;\n    &lt;column autoIncrement=\"true\" name=\"ID\" type=\"BIGINT\" remarks=\"Primary key\"&gt;\n        &lt;constraints primaryKey=\"true\" /&gt;\n    &lt;/column&gt;\n</code></pre>"},{"location":"blog/2023/04/28/auto-increment-with-postgres-and-jpa/#notes","title":"Notes","text":"<p>The PRs which fixed the problem:</p> <ul> <li>Liquibase</li> <li>JPA</li> </ul>"},{"location":"blog/2025/05/17/automatically-renew-aws-sso-session-and-refresh-kubeconfig-for-eks-access/","title":"Automatically Renew AWS SSO Session and Refresh Kubeconfig for EKS Access","text":"<p>Working with AWS EKS clusters via AWS SSO is secure but sometimes frustrating. If your session expires, <code>kubectl</code> commands will fail until you manually renew the session and update your kubeconfig.</p> <p>Let's automate that with a small Bash script.</p> <p></p>"},{"location":"blog/2025/05/17/automatically-renew-aws-sso-session-and-refresh-kubeconfig-for-eks-access/#problem","title":"Problem","text":"<ul> <li>AWS SSO sessions expire every 8\u201312 hours by default.</li> <li><code>kubectl</code> will throw <code>ExpiredTokenException</code> or connection errors.</li> <li>You need to manually run <code>aws sso login</code> and <code>aws eks update-kubeconfig</code>.</li> </ul> <p>Frustration</p> <p>Manually logging back in slows you down and interrupts your work.</p>"},{"location":"blog/2025/05/17/automatically-renew-aws-sso-session-and-refresh-kubeconfig-for-eks-access/#solution-bash-script","title":"Solution: Bash Script","text":"<p>Create a script called <code>aws_sso_kubeconfig.sh</code>:</p> <pre><code>#!/bin/bash\n\nPROFILE=\"prod\"\nCLUSTER_NAME=\"your-eks-cluster-name\"\nREGION=\"your-eks-cluster-region\"\n\n# Check if AWS SSO session is valid\nif ! aws sts get-caller-identity --profile \"$PROFILE\" &gt; /dev/null 2&gt;&amp;1; then\n  echo \"\ud83d\udd12 AWS SSO session expired. Logging in again...\"\n  aws sso login --profile \"$PROFILE\"\nelse\n  echo \"\u2705 AWS SSO session still valid.\"\nfi\n\n# Always refresh kubeconfig\naws eks update-kubeconfig --name \"$CLUSTER_NAME\" --region \"$REGION\" --profile \"$PROFILE\"\necho \"\u2705 Kubeconfig updated.\"\n</code></pre>"},{"location":"blog/2025/05/17/automatically-renew-aws-sso-session-and-refresh-kubeconfig-for-eks-access/#how-to-use","title":"How to Use","text":"<p>Make it executable:</p> <pre><code>chmod +x aws_sso_kubeconfig.sh\n</code></pre> <p>Run it before using kubectl:</p> <pre><code>./aws_sso_kubeconfig.sh\n</code></pre> <p>Quick Tip</p> <p>Add an alias to your .bashrc or .zshrc for faster use: bash alias krenew=\"~/path/to/aws_sso_kubeconfig.sh\"</p> <p>Then just type: <pre><code>krenew\n</code></pre></p>"},{"location":"blog/2025/05/17/automatically-renew-aws-sso-session-and-refresh-kubeconfig-for-eks-access/#bonus-automate-it-with-a-cron-job","title":"Bonus: Automate it with a Cron Job","text":"<p>You can set up a cron job to run the script every 4\u20136 hours automatically, keeping your session fresh.</p> <p>Edit your crontab:</p> <pre><code>crontab -e\n</code></pre> <p>Add this line (every 6 hours):</p> <pre><code>0 */6 * * * /path/to/aws_sso_kubeconfig.sh &gt;&gt; /tmp/aws_sso_renew.log 2&gt;&amp;1\n</code></pre> <p>This:</p> <ul> <li>Runs the script every 6 hours</li> <li>Redirects output to /tmp/aws_sso_renew.log for easy debugging if needed</li> </ul> <p>Important</p> <p>Make sure your environment variables ($PATH) are properly available inside cron. Sometimes you need to load your AWS credentials manually inside the script if your environment is not loaded.</p>"},{"location":"blog/2025/05/17/automatically-renew-aws-sso-session-and-refresh-kubeconfig-for-eks-access/#why-it-matters","title":"Why It Matters","text":"<ul> <li>\u23f3 No more session expiration surprises.</li> <li>\ud83d\udd12 Maintain secure access to your Kubernetes cluster.</li> <li>\ud83d\ude80 Speed up your daily development workflow.</li> </ul>"},{"location":"blog/2025/05/17/automatically-renew-aws-sso-session-and-refresh-kubeconfig-for-eks-access/#final-thoughts","title":"Final Thoughts","text":"<p>AWS SSO is great for security, but it can disrupt your Kubernetes operations without automation. This small script saves time, reduces frustration, and helps you maintain a smooth EKS workflow without manual steps.</p> <p>Happy Kubernetes hacking! \ud83d\ude80</p>"},{"location":"blog/2025/05/30/automated-tls-and-dns-in-kubernetes-with-externaldns-ingress-and-lets-encrypt/","title":"Automated TLS and DNS in Kubernetes with ExternalDNS, Ingress, and Let's Encrypt","text":"<p>Managing DNS and TLS certificates for Kubernetes applications can be tedious and error-prone. Thankfully, tools like ExternalDNS, Ingress, and Cert-Manager automate the entire process \u2014 from setting DNS records to provisioning Let's Encrypt certificates.</p> <p>In this guide, we'll walk through how to:</p> <ul> <li>Use ExternalDNS to automatically create DNS records.</li> <li>Annotate Ingress resources to request a Let's Encrypt TLS cert.</li> <li>Get HTTPS with minimal manual intervention.</li> <li>Understand how these components interact.</li> </ul> <p></p>"},{"location":"blog/2025/05/30/automated-tls-and-dns-in-kubernetes-with-externaldns-ingress-and-lets-encrypt/#prerequisites","title":"Prerequisites","text":"<ul> <li>A Kubernetes cluster (EKS, GKE, K3s, etc.)</li> <li>A domain name (e.g., <code>example.com</code>) with access to its DNS provider</li> <li>A public DNS provider supported by ExternalDNS (e.g., Route53, Cloudflare, Google DNS)</li> <li>Helm 3 installed locally</li> <li><code>kubectl</code> configured for your cluster</li> </ul>"},{"location":"blog/2025/05/30/automated-tls-and-dns-in-kubernetes-with-externaldns-ingress-and-lets-encrypt/#step-1-set-up-externaldns","title":"Step 1: Set Up ExternalDNS","text":"<p>ExternalDNS watches your Kubernetes resources (like <code>Ingress</code>) and creates the corresponding DNS records in your provider.</p>"},{"location":"blog/2025/05/30/automated-tls-and-dns-in-kubernetes-with-externaldns-ingress-and-lets-encrypt/#install-externaldns-via-helm","title":"Install ExternalDNS via Helm","text":"<p>For Route53 (AWS example):</p> <pre><code>helm repo add bitnami https://charts.bitnami.com/bitnami\nhelm install external-dns bitnami/external-dns \\\n  --set provider=aws \\\n  --set aws.zoneType=public \\\n  --set domainFilters[0]=example.com \\\n  --set policy=sync \\\n  --set registry=txt \\\n  --set txtOwnerId=my-cluster \\\n  --set serviceAccount.create=true \\\n  --set serviceAccount.name=external-dns\n</code></pre> <p>\u26a0\ufe0f IAM permissions for Route53 are required on the node or pod role.</p>"},{"location":"blog/2025/05/30/automated-tls-and-dns-in-kubernetes-with-externaldns-ingress-and-lets-encrypt/#iam-policy-for-externaldns-route53-example","title":"IAM Policy for ExternalDNS (Route53 Example)","text":"<p>If you are using AWS Route53, ExternalDNS needs permission to manage DNS records.</p> <p>Here is a sample IAM policy for managing multiple domains:</p> <pre><code>{\n  \"Version\": \"2012-10-17\",\n  \"Statement\": [\n    {\n      \"Effect\": \"Allow\",\n      \"Action\": [\n        \"route53:ChangeResourceRecordSets\"\n      ],\n      \"Resource\": [\n        \"arn:aws:route53:::hostedzone/ZONE_ID_1\",\n        \"arn:aws:route53:::hostedzone/ZONE_ID_2\",\n        \"arn:aws:route53:::hostedzone/ZONE_ID_3\"\n      ]\n    },\n    {\n      \"Effect\": \"Allow\",\n      \"Action\": [\n        \"route53:ListHostedZones\",\n        \"route53:ListResourceRecordSets\"\n      ],\n      \"Resource\": [\"*\"]\n    }\n  ]\n}\n</code></pre> <p>\ud83d\udee0 Replace <code>ZONE_ID_X</code> with your actual Route53 hosted zone IDs (you can find them in the AWS Console under Route53).</p> <p>If you're using EKS, create an IAM role with this policy and bind it to the Kubernetes ServiceAccount used by ExternalDNS using IRSA:</p> <pre><code>apiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: external-dns\n  namespace: default\n  annotations:\n    eks.amazonaws.com/role-arn: arn:aws:iam::&lt;ACCOUNT_ID&gt;:role/ExternalDNSRole\n</code></pre>"},{"location":"blog/2025/05/30/automated-tls-and-dns-in-kubernetes-with-externaldns-ingress-and-lets-encrypt/#step-2-set-up-cert-manager-for-lets-encrypt","title":"Step 2: Set Up Cert-Manager for Let's Encrypt","text":"<p>Cert-Manager issues and renews TLS certs from Let\u2019s Encrypt.</p>"},{"location":"blog/2025/05/30/automated-tls-and-dns-in-kubernetes-with-externaldns-ingress-and-lets-encrypt/#install-cert-manager-via-helm","title":"Install Cert-Manager via Helm","text":"<pre><code>helm repo add jetstack https://charts.jetstack.io\nhelm repo update\n\nhelm install cert-manager jetstack/cert-manager \\\n  --namespace cert-manager \\\n  --create-namespace \\\n  --set installCRDs=true\n</code></pre>"},{"location":"blog/2025/05/30/automated-tls-and-dns-in-kubernetes-with-externaldns-ingress-and-lets-encrypt/#create-lets-encrypt-issuer","title":"Create Let's Encrypt Issuer","text":"<p>Create a production Issuer or Staging one (for testing) with HTTP-01 challenge:</p> <p><code>issuer.yaml</code> <pre><code>apiVersion: cert-manager.io/v1\nkind: ClusterIssuer\nmetadata:\n  name: letsencrypt-http\nspec:\n  acme:\n    server: https://acme-v02.api.letsencrypt.org/directory\n    email: your-email@example.com\n    privateKeySecretRef:\n      name: letsencrypt-http-private-key\n    solvers:\n      - http01:\n          ingress:\n            class: nginx\n</code></pre></p> <p>Apply it:</p> <pre><code>kubectl apply -f issuer.yaml\n</code></pre>"},{"location":"blog/2025/05/30/automated-tls-and-dns-in-kubernetes-with-externaldns-ingress-and-lets-encrypt/#step-3-create-ingress-with-dns-and-tls-annotations","title":"Step 3: Create Ingress with DNS and TLS Annotations","text":"<p>Now that DNS and Cert-Manager are configured, create an Ingress resource.</p> <p><code>ingress.yaml</code> <pre><code>apiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  name: my-app\n  annotations:\n    kubernetes.io/ingress.class: \"nginx\"\n    cert-manager.io/cluster-issuer: \"letsencrypt-http\"\n    external-dns.alpha.kubernetes.io/hostname: app.example.com\nspec:\n  tls:\n    - hosts:\n        - app.example.com\n      secretName: my-app-tls\n  rules:\n    - host: app.example.com\n      http:\n        paths:\n          - path: /\n            pathType: Prefix\n            backend:\n              service:\n                name: my-app-service\n                port:\n                  number: 80\n</code></pre></p> <p>Apply it:</p> <pre><code>kubectl apply -f ingress.yaml\n</code></pre>"},{"location":"blog/2025/05/30/automated-tls-and-dns-in-kubernetes-with-externaldns-ingress-and-lets-encrypt/#how-it-all-works-together","title":"How It All Works Together","text":"<ol> <li>ExternalDNS sees the Ingress annotation and creates a DNS A or CNAME record for <code>app.example.com</code> pointing to your Ingress controller's IP or hostname.</li> <li>Cert-Manager notices the <code>cert-manager.io/cluster-issuer</code> annotation and requests a cert from Let's Encrypt using the <code>ClusterIssuer</code>.</li> <li>It completes the HTTP-01 challenge via your NGINX ingress controller.</li> <li>If successful, a TLS secret is created (<code>my-app-tls</code>) and used by the Ingress to serve HTTPS.</li> </ol>"},{"location":"blog/2025/05/30/automated-tls-and-dns-in-kubernetes-with-externaldns-ingress-and-lets-encrypt/#additional-tips","title":"Additional Tips","text":"<ul> <li>You can use wildcard DNS with Let's Encrypt using DNS-01 challenge, which requires API access to your DNS provider.</li> <li>For custom Ingress controllers (like Traefik or HAProxy), make sure their class matches the annotation.</li> <li>Always start with Let's Encrypt Staging to avoid rate limits.</li> <li>Monitor logs of <code>cert-manager</code>, <code>external-dns</code>, and the ingress controller for debugging.</li> </ul>"},{"location":"blog/2025/05/30/automated-tls-and-dns-in-kubernetes-with-externaldns-ingress-and-lets-encrypt/#sample-dns-record","title":"Sample DNS Record","text":"<p>After everything is set up, you should see a record like this in your DNS provider:</p> <pre><code>app.example.com  -&gt;  &lt;Ingress LoadBalancer IP or hostname&gt;\n</code></pre> <p>And your app should be accessible via:</p> <pre><code>https://app.example.com\n</code></pre> <p>with a valid Let's Encrypt certificate.</p>"},{"location":"blog/2025/05/30/automated-tls-and-dns-in-kubernetes-with-externaldns-ingress-and-lets-encrypt/#conclusion","title":"Conclusion","text":"<p>By combining ExternalDNS, Ingress, and Cert-Manager, you can fully automate the setup of public DNS and TLS for Kubernetes workloads. This reduces manual effort and increases reliability \u2014 all with a simple set of annotations.</p>"},{"location":"blog/2025/05/15/bootstrapping-my-linux-desktop-and-macbook-for-dev-work/","title":"Bootstrapping My Linux Desktop and MacBook for Dev Work","text":"<p>After transitioning through two new jobs recently, I had the opportunity (and challenge) to set up fresh dev environments on both a Linux desktop and a MacBook. Here\u2019s my comprehensive checklist and setup notes, including tooling, config files, and references I found useful.</p> <p></p>"},{"location":"blog/2025/05/15/bootstrapping-my-linux-desktop-and-macbook-for-dev-work/#software-setup","title":"Software Setup","text":""},{"location":"blog/2025/05/15/bootstrapping-my-linux-desktop-and-macbook-for-dev-work/#1-docker-desktop","title":"1. Docker Desktop","text":"<ul> <li>Mac: Install Docker Desktop for Mac</li> <li>Linux:   <pre><code>sudo apt-get update\nsudo apt-get install docker.io\nsudo systemctl enable --now docker\nsudo usermod -aG docker $USER\n</code></pre></li> </ul>"},{"location":"blog/2025/05/15/bootstrapping-my-linux-desktop-and-macbook-for-dev-work/#2-visual-studio-code","title":"2. Visual Studio Code","text":"<ul> <li>Mac/Linux:   Download from https://code.visualstudio.com/</li> </ul>"},{"location":"blog/2025/05/15/bootstrapping-my-linux-desktop-and-macbook-for-dev-work/#3-intellij-idea","title":"3. IntelliJ IDEA","text":"<ul> <li>Mac/Linux:   Download from https://www.jetbrains.com/idea/download/   Or use JetBrains Toolbox.</li> </ul>"},{"location":"blog/2025/05/15/bootstrapping-my-linux-desktop-and-macbook-for-dev-work/#4-slack","title":"4. Slack","text":"<ul> <li>Mac:   Download from https://slack.com/downloads/mac</li> <li>Linux:   <pre><code>sudo snap install slack --classic\n</code></pre></li> </ul>"},{"location":"blog/2025/05/15/bootstrapping-my-linux-desktop-and-macbook-for-dev-work/#5-github-copilot","title":"5. GitHub Copilot","text":"<ul> <li>Install the GitHub Copilot plugin in:</li> <li>VS Code</li> <li>IntelliJ</li> </ul>"},{"location":"blog/2025/05/15/bootstrapping-my-linux-desktop-and-macbook-for-dev-work/#6-homebrew-for-mac","title":"6. Homebrew for Mac","text":"<p>Install only if you are using it for <code>terraform</code>, <code>npm</code>, or <code>jq</code>. <pre><code>/bin/bash -c \"$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)\"\n</code></pre></p>"},{"location":"blog/2025/05/15/bootstrapping-my-linux-desktop-and-macbook-for-dev-work/#7-kubectl","title":"7. Kubectl","text":"<ul> <li>Mac:   <pre><code>curl -LO \"https://dl.k8s.io/release/$(curl -s https://dl.k8s.io/release/stable.txt)/bin/darwin/amd64/kubectl\"\nchmod +x kubectl\nsudo mv kubectl /usr/local/bin/\n</code></pre></li> <li>Linux:   <pre><code>curl -LO \"https://dl.k8s.io/release/$(curl -s https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl\"\nchmod +x kubectl\nsudo mv kubectl /usr/local/bin/\n</code></pre></li> </ul>"},{"location":"blog/2025/05/15/bootstrapping-my-linux-desktop-and-macbook-for-dev-work/#8-java-openjdk","title":"8. Java (OpenJDK)","text":"<ul> <li>Mac:   Download OpenJDK DMG from https://jdk.java.net/</li> <li>Linux:   <pre><code>sudo apt install openjdk-17-jdk\n</code></pre></li> </ul>"},{"location":"blog/2025/05/15/bootstrapping-my-linux-desktop-and-macbook-for-dev-work/#9-python","title":"9. Python","text":"<ul> <li>Mac:   Download official installer from https://www.python.org/downloads/mac-osx/</li> <li>Linux:   <pre><code>sudo apt install python3 python3-pip\n</code></pre></li> </ul>"},{"location":"blog/2025/05/15/bootstrapping-my-linux-desktop-and-macbook-for-dev-work/#10-terraform","title":"10. Terraform","text":"<p><pre><code>brew tap hashicorp/tap\nbrew install hashicorp/tap/terraform\n</code></pre> Terraform Install Guide</p>"},{"location":"blog/2025/05/15/bootstrapping-my-linux-desktop-and-macbook-for-dev-work/#11-npm-via-nodejs","title":"11. npm (via Node.js)","text":"<p><pre><code>brew install node\n</code></pre> Node.js Docs</p>"},{"location":"blog/2025/05/15/bootstrapping-my-linux-desktop-and-macbook-for-dev-work/#12-jq","title":"12. jq","text":"<ul> <li>Mac:   <pre><code>brew install jq\n</code></pre></li> <li>Linux:   <pre><code>sudo apt install jq\n</code></pre></li> </ul>"},{"location":"blog/2025/05/15/bootstrapping-my-linux-desktop-and-macbook-for-dev-work/#configuration-files","title":"Configuration Files","text":""},{"location":"blog/2025/05/15/bootstrapping-my-linux-desktop-and-macbook-for-dev-work/#1-aws-config","title":"1. AWS Config","text":""},{"location":"blog/2025/05/15/bootstrapping-my-linux-desktop-and-macbook-for-dev-work/#set-up-aws-cli","title":"Set Up AWS CLI","text":"<p>Mac <pre><code>curl \"https://awscli.amazonaws.com/AWSCLIV2.pkg\" -o \"AWSCLIV2.pkg\"\nsudo installer -pkg AWSCLIV2.pkg -target /\n</code></pre></p> <p>Linux <pre><code>curl \"https://awscli.amazonaws.com/awscli-exe-linux-x86_64.zip\" -o \"awscliv2.zip\"\nunzip awscliv2.zip\nsudo ./aws/install\n</code></pre> Install Docs</p> <p>Set up AWS config and credential</p> <pre><code>aws configure\n</code></pre> <p>follow the prompt: <pre><code>AWS Access Key ID [None]: YOUR_ACCESS_KEY_ID\nAWS Secret Access Key [None]: YOUR_SECRET_ACCESS_KEY\nDefault region name [None]: us-east-1\nDefault output format [None]: json\n</code></pre></p>"},{"location":"blog/2025/05/15/bootstrapping-my-linux-desktop-and-macbook-for-dev-work/#2-kubeconfig","title":"2. Kubeconfig","text":"<p>Copy your kubeconfig to: <pre><code>~/.kube/config\n</code></pre> Or use: <pre><code>export KUBECONFIG=/path/to/kubeconfig.yaml\n</code></pre></p>"},{"location":"blog/2025/05/15/bootstrapping-my-linux-desktop-and-macbook-for-dev-work/#3-github-ssh","title":"3. GitHub SSH","text":"<pre><code>ssh-keygen -t ed25519 -C \"you@example.com\"\neval \"$(ssh-agent -s)\"\nssh-add ~/.ssh/id_ed25519\n</code></pre> <p>Then add the public key to your GitHub account: <pre><code>cat ~/.ssh/id_ed25519.pub\n</code></pre></p>"},{"location":"blog/2024/05/08/create-erc-public-login/","title":"Create erc public login","text":"<p>Amazon ECR Public allows users to store and access public container images. While ECR Public repositories are open to the public, access to pull or download images from these repositories may still require authentication.</p> <p>While there are multiple reasons such as access control and security concern, the main benefit of getting an authentication token or login is to deal with rate limiting in my use case.</p>"},{"location":"blog/2024/05/08/create-erc-public-login/#get-a-ecr-public-token","title":"Get a ECR-Public Token","text":"<p>In so doing, you can create a kubernetes secret from the token to be accessible within the namespace:</p> <pre><code>TOKEN=$(aws ecr-pubic get-authorization-token --region us-east-1)\nNS=\"THE_NAMESPACE\"\n\nkubectl create secret docker-registry ecr-public-secret \\\n  --namespace $NS \\\n  --docker-username=AWS\n  --docker-password=\"$TOKEN\" \\\n  --docker-server=\"public.erc.aws\"\n  --dry-run=client -o yaml | kubectl apply -f -\n</code></pre>"},{"location":"blog/2024/05/08/create-erc-public-login/#get-a-ecr-public-login","title":"Get a ECR-Public Login","text":"<p>Another option is to get the login password and then login to the ECR public registry. This option is more suitable for CI/CD automation script.</p> <pre><code>aws ecr-public get-login-password --region us-east-1 | docker login --username AWS --password-stdin public.ecr.aws\n</code></pre>"},{"location":"blog/2024/05/08/create-erc-public-login/#refresh-tokenlogin","title":"Refresh Token/Login","text":"<p>The token/login has a validity period of 12 hours. You need to obtain a new one to continue accessing ECR Public.</p> <p>The follow cron job will refresh the Kubernetes secret every 6 hours</p> <pre><code>apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: refresh-ecr-public-token\nspec:\n  schedule: \"0 */6 * * *\"  # Runs every 6 hours\n  jobTemplate:\n    spec:\n      template:\n        spec:\n          containers:\n          - name: refresh-token\n            image: awscli/awscli:latest  # Use an image with AWS CLI installed\n            command: [\"sh\", \"-c\"]\n            args:\n            - |\n              # Refresh ECR Public token\n              token=$(aws ecr-public get-login-password --region us-east-1)\n              kubectl create secret docker-registry ecr-public-secret \\\n                --docker-server=public.ecr.aws \\\n                --docker-username=AWS \\\n                --docker-password=$token\n          restartPolicy: OnFailure\n</code></pre>"},{"location":"blog/2023/07/02/dev-container/","title":"Dev Container","text":""},{"location":"blog/2023/07/02/dev-container/#background","title":"Background","text":"<p>Since my first job after college graduation, I have been looking for a way to setup a portable development environment for the following reasons:</p> <ol> <li>to use it on multiple machines</li> <li>to share it among the team with teammates who doesn't have server/platform/os experience</li> <li>to standardize os, programming language and library version, etc</li> <li>to save myself the headache of rebuilding everything from scratch</li> </ol> <p>All throught these years, I have been using the following to achieve this:</p> <ol> <li>Vmware/Virtualbox</li> <li>Puppet</li> <li>Docker</li> </ol> <p>I thought the endgame would be docker since it is lightweight, scriptable and can be run almost anywhere, until I meet Dev Container.</p>"},{"location":"blog/2023/07/02/dev-container/#about-dev-container","title":"About Dev Container","text":"<p>Dev Container is a feature of VS Code and Docker, which provides an easy and reproducible way to set up development environments, within a button residing on the IDE. It can be used to run an application, to separate tools, libraries, or runtimes needed for working with a codebase</p>"},{"location":"blog/2023/07/02/dev-container/#prerequisite","title":"Prerequisite","text":"<ol> <li>VS Code</li> <li>Docker</li> </ol>"},{"location":"blog/2023/07/02/dev-container/#to-create-a-dev-container","title":"To Create a Dev Container","text":"<p>As an example, I am going to create a Python 3.9 Dev Container with Docker in Docker (DIID) support</p> <ol> <li>Click on the lower left green icon at the VS Code IDE</li> <li>From the dropdown at the top portion of the VS Code, select <code>Create Dev Container</code> </li> <li>Search for Python 3 and select <code>Python 3 devcontainers</code> </li> <li>Select <code>Additional Options</code> and choose Python 3.9 </li> <li>Search for <code>docker-in-docker</code> and check the checkbox </li> <li>Click OK</li> </ol> <p>At this point, VS Code will pop up an information box about the creation of the new Dev Container. After that VS Code will restart in the Dev Container mode.</p> <p></p> <p>There you have it. You should be able to run your application inside the Dev Container with Python 3.9 now.</p> <p></p>"},{"location":"blog/2023/07/02/dev-container/#configuration-file","title":"Configuration file","text":"<p>The configuration of the new Dev Container is saved under <code>.devcontainer/devcontainer.json</code> file. Here is the content of the file.</p> <pre><code>// For format details, see https://aka.ms/devcontainer.json. For config options, see the\n// README at: https://github.com/devcontainers/templates/tree/main/src/python\n{\n    \"name\": \"Python 3\",\n    // Or use a Dockerfile or Docker Compose file. More info: https://containers.dev/guide/dockerfile\n    \"image\": \"mcr.microsoft.com/devcontainers/python:1-3.9-bookworm\",\n    \"features\": {\n        \"ghcr.io/devcontainers/features/docker-in-docker:2\": {}\n    }\n\n    // Features to add to the dev container. More info: https://containers.dev/features.\n    // \"features\": {},\n\n    // Use 'forwardPorts' to make a list of ports inside the container available locally.\n    // \"forwardPorts\": [],\n\n    // Use 'postCreateCommand' to run commands after the container is created.\n    // \"postCreateCommand\": \"pip3 install --user -r requirements.txt\",\n\n    // Configure tool-specific properties.\n    // \"customizations\": {},\n\n    // Uncomment to connect as root instead. More info: https://aka.ms/dev-containers-non-root.\n    // \"remoteUser\": \"root\"\n}\n</code></pre>"},{"location":"blog/2023/07/02/dev-container/#link","title":"Link","text":"<p>Dev Container</p> <p>Github Repo</p>"},{"location":"blog/2025/08/16/reviving-doraemon-a-slack-bots-second-life-in-kubernetes/","title":"Reviving Doraemon: A Slack Bot\u2019s Second Life in Kubernetes","text":""},{"location":"blog/2025/08/16/reviving-doraemon-a-slack-bots-second-life-in-kubernetes/#originally-posted-at-linkedin-at-aug-16-2025","title":"originally posted at LinkedIn at Aug 16, 2025","text":"<p>Some projects stick with you. For me, it was a little Slack bot I hacked together at a previous job\u2014something that could talk to our infrastructure and give quick answers without switching tools. I never learned what happened to it. Layoffs came. From what I later heard, it wasn\u2019t adopted. It felt like watching a small idea I cared about slowly disappear.</p> <p>Fast-forward to Flagler. I mentioned the bot almost off\u2011hand, unsure if anyone would care. My boss immediately supported the idea, and that gave me the energy to bring it back. This post is about reviving that project\u2014this time with intent, care, and a proper home in Kubernetes.</p>"},{"location":"blog/2025/08/16/reviving-doraemon-a-slack-bots-second-life-in-kubernetes/#meet-doraemon","title":"Meet Doraemon","text":"<p>Fun fact: Doraemon takes its name from a beloved Japanese cartoon robot cat who pulls out gadgets to solve everyday problems. Our bot carries the same spirit\u2014always ready with a tool or an answer when you need it most.</p> <p>Doraemon is a Kubernetes\u2011native Slack bot, built with the slack_bot library in Python 3.11. It lives right in Slack, where conversations already happen, and brings clarity without forcing people to change context. What started as a hackathon experiment is now something I maintain proudly as a senior DevOps engineer\u2014small, sharp, and genuinely useful.</p> <p>When someone greets the bot, it replies with a menu of its capabilities. It\u2019s friendly, obvious, and lowers the barrier for anyone to use it.</p>"},{"location":"blog/2025/08/16/reviving-doraemon-a-slack-bots-second-life-in-kubernetes/#what-doraemon-can-do","title":"What Doraemon Can Do","text":""},{"location":"blog/2025/08/16/reviving-doraemon-a-slack-bots-second-life-in-kubernetes/#1-infrastructure-version-report","title":"1. Infrastructure Version Report","text":"<p>Purpose: Show what\u2019s deployed versus what\u2019s available for key infrastructure components\u2014Argo CD, Sealed Secrets, Airbyte, Argo Rollouts, and more.</p> <p>Impact: Doraemon queries live sources like GitHub Releases, Helm chart indices, and Kubernetes manifests, then shows the difference between current and available versions. No dashboards, no delays\u2014just one straight answer in Slack. For me as a DevOps engineer, this means I can surface upgrade opportunities to the team without context switching, and for everyone else it means visibility they never had before.</p>"},{"location":"blog/2025/08/16/reviving-doraemon-a-slack-bots-second-life-in-kubernetes/#2-app-version-check","title":"2. App Version Check","text":"<p>Purpose: Answer the everyday question: \u201cWhat version is deployed?\u201d</p> <p>Impact: Doraemon understands natural prompts\u2014<code>app version?</code>, <code>what\u2019s our app version?</code>\u2014and replies with the deployed version number. It\u2019s not flashy, but it saves time in standups and avoids unnecessary back\u2011and\u2011forth.</p>"},{"location":"blog/2025/08/16/reviving-doraemon-a-slack-bots-second-life-in-kubernetes/#3-restaurant-recommendations","title":"3. Restaurant Recommendations","text":"<p>Purpose: Help teammates flying into the office and unfamiliar with the city feel at home.</p> <p>Impact: Ask <code>what to eat?</code> and Doraemon suggests nearby restaurant options with a map. It may sound lighthearted, but it has real value\u2014when someone lands in town for a sprint, they don\u2019t waste time scrolling review apps. It shows that even a DevOps\u2011driven tool can make the human side of work smoother.</p>"},{"location":"blog/2025/08/16/reviving-doraemon-a-slack-bots-second-life-in-kubernetes/#4-access-codes","title":"4. Access Codes","text":"<p>Purpose: Replace tedious, manual back\u2011and\u2011forth requests for access codes.</p> <p>Impact: Instead of waiting on IT or ops, users can request codes directly in Slack. Doraemon validates and delivers them securely, reducing interruptions. As someone who used to field those requests, this feature is a relief\u2014it gives time back to ops while giving users what they need instantly.</p>"},{"location":"blog/2025/08/16/reviving-doraemon-a-slack-bots-second-life-in-kubernetes/#why-slack","title":"Why Slack?","text":"<p>Because that\u2019s where the conversations are. By answering questions directly in\u2011thread, Doraemon keeps the loop tight. No context switching, no dependency on who has the right tool open\u2014the information becomes part of the conversation and the audit trail.</p>"},{"location":"blog/2025/08/16/reviving-doraemon-a-slack-bots-second-life-in-kubernetes/#finding-my-closure","title":"Finding My Closure","text":"<p>I built the original during a hackathon at a previous job. It was never adopted, and after the layoff, I thought that chapter was closed. Getting the chance to bring it back at Flagler\u2014with real support\u2014meant more than just recycling old code. It was personal. It was proof that even small, unfinished ideas can grow into tools the team actually uses.</p> <p>To my boss and teammates at Flagler: thanks for backing this. From the outside, Doraemon may look like a small Slack bot. But from my perspective as a DevOps engineer, it\u2019s also a story about resilience\u2014about not letting useful ideas die, and about finding ways to make work better for everyone.</p> <p>If my hackathon\u2011self could see this now, they\u2019d recognize the heart\u2014but also the rigor. And maybe they\u2019d smile at the name too\u2014just like the cartoon Doraemon always had the right gadget for the moment, our bot is there with the right answers when we need them.</p>"},{"location":"blog/2023/07/15/github-multi-repo-update/","title":"Github multi repo update","text":""},{"location":"blog/2023/07/15/github-multi-repo-update/#originally-posted-at-linkedin-at-july-15-2023","title":"originally posted at LinkedIn at July 15, 2023","text":""},{"location":"blog/2023/07/15/github-multi-repo-update/#background","title":"Background","text":"<p>This is related to the other blog post about GitHub Shared Action.</p> <p>I want to make the same change to multiple GitHub repos and I want to limit manual steps as much as possible. As an example, I would like to push the the files of a source folder (source) to three of my repos. Here is one of the file:</p> <p><code>source/somefile</code> <pre><code>Hello World!\n</code></pre></p> <p>The following file contains the repo name that I want to update:</p> <p><code>repos.txt</code></p> <pre><code>mock-flask\njob-winner\nrock-paper-scissors\n</code></pre>"},{"location":"blog/2023/07/15/github-multi-repo-update/#original-plan","title":"Original Plan","text":"<p>My first thought was to implement a single bash script to do all the work:</p> <ol> <li>read the repos.txt file line by line</li> <li>for each line, run all the require git commands one by one</li> </ol> <p>Here is the snippet for the original plan:</p> <pre><code>REPO=$1\nREPO_ROOT=\"/Users/janus/workspace/\"\nSOURCE_DIR=\"source\"\n\nwhile read -r line; do\n    cd \"$line\"\n    cp $SOURCE_DIR/* $REPO_ROOT/$REPO/\n    cd $REPO_ROOT/$REPO/\n    git checkout -b TEST-0000\n    git add .\n    git commit -m \"TEST-0000 patching\"\n    git push origin TEST-0000\ndone &lt; repos.txt\n</code></pre> <p>This can get the job done, but what if I have to do similar update in the future?</p>"},{"location":"blog/2023/07/15/github-multi-repo-update/#build-a-reusable-script","title":"Build a Reusable Script","text":"<p>As a DevOps engineerer, I expect a similar task to patch multiple repos will come over and over again. Therefore, I break down the script into two parts:</p> <ol> <li>The reusable part - one that read the repos.txt file and perform the while loop<ul> <li>no need to reinvent the wheel anymore</li> <li>no need to test if the looping the repos feature in the future</li> </ul> </li> <li>The pluggable part - the script that does the GitHub update<ul> <li>can be versionized</li> <li>can be tested easier as it can be tested against a single repo as a standalone script</li> </ul> </li> </ol> <p>Here are the new implementation:</p> <p><code>github-looping.sh</code> <pre><code>#!/bin/sh\n\nMY_SCRIPT=$1\n\nwhile read -r line; \n    do ./$MY_SCRIPT \"$line\"; \ndone &lt; repos.txt\n</code></pre></p> <p><code>TEST-0000.sh</code> - while TEST-0000 is the jira ticket number <pre><code>#!/bin/sh\n\nGH_BRANCH=$(basename -- $0 | cut -d. -f1)\nREPO=$1\nSOURCE_DIR=\"source\"\nREPO_ROOT=\"/Users/janus/workspace/\"\nCOMMIT_MESSAGE=\"mass patching\"\n\necho \"processing repo \"$REPO\" ...\"\ncp $SOURCE_DIR/* $REPO_ROOT/$REPO/\ncd $REPO_ROOT/$REPO/\ngit checkout -b $GH_BRANCH\ngit add .\ngit commit -m \"$GH_BRANCH $COMMIT_MESSAGE\"\ngit push origin $GH_BRANCH\necho \"finish processing repo \"$REPO\" ...\"\n</code></pre></p> <p>To run the update across all the repos, simply run the following: <code>./github-looping.sh TEST-0000.sh</code></p> <p>The idea is to have a seperate script being called by <code>github-looping.sh</code>. </p> <p>In so doing, I can versionize the TEST-XXXX.sh script and have an idea of what exactly was being run.</p>"},{"location":"blog/2023/07/15/github-multi-repo-update/#conclusion","title":"Conclusion","text":"<p>There are probably a thousand better ways out there to do similar thing. I found couple packages out there that take user inputs from the command lines interactively. Some of them would print out a nice report too. There is certainly room for improvement. However, as long as I can save myself from the headache of manaully update indivdual repos, I am happy with what I have so far.</p> <p>Hope this works for you too if you are facing a similar problem.</p>"},{"location":"blog/2023/07/02/github-repository-dispatch/","title":"Github repository dispatch","text":""},{"location":"blog/2023/07/02/github-repository-dispatch/#background","title":"Background","text":"<p>I have to run a shell script at repo B from repo A which hosts a python app. While I can spend the time to reinvent the wheel and fork the logic into python, it is best to keep thing simple by reusing what is already out there and proved to be working fine.</p> <p>The js script in repo B is already wired up with a Github workflow. What if I can kick off that workflow remotely from repo A?</p> <p>Github repository dispatch is the answer.</p>"},{"location":"blog/2023/07/02/github-repository-dispatch/#about-github-repository-dispatch","title":"About Github Repository Dispatch","text":"<p>Github repository dispatch is a feature to trigger custom events or actions programmatically. It allows you to create a webhook endpoint that can receive external HTTP requests and generate a repository-level event. This event can then be used to trigger workflows, CI/CD pipelines, or any other custom actions defined in your repository.</p>"},{"location":"blog/2023/07/02/github-repository-dispatch/#sample-workflow","title":"Sample Workflow","text":"<p>This sample workflow will print out the client payload message.</p> <pre><code>name: dispatch receiver test\n\non: \n  repository_dispatch:\n    types: [my-type]\n\njobs:\n  build:\n    runs-on: ubuntu-latest\n    steps:\n    - uses: actions/checkout@v1\n    - name: dispatch trigger\n      env:\n        MESSAGE: ${{ github.event.client_payload.message }}\n      run: |\n        echo \"$MESSAGE\"\n</code></pre>"},{"location":"blog/2023/07/02/github-repository-dispatch/#github-personal-token","title":"Github Personal Token","text":"<p>You will need to create a personal token for the repo, in this case, repo B, with the following setting:</p> <p><code>permissions contents: write</code></p> <p>Find more about how to create a token.</p>"},{"location":"blog/2023/07/02/github-repository-dispatch/#sample-code-to-trigger-the-event","title":"Sample Code To trigger the event","text":"<p>Curl</p> <pre><code>curl -H \"Accept: application/vnd.github.everest-preview+json\" \\\n    -H \"Authorization: token $TOKEN\" \\\n    --request POST \\\n    --data '{\"event_type\": \"my-type\", \"client_payload\": {\"message\": \"Testing Github repository dispatch\"}}' \\\n    https://api.github.com/repos/&lt;org&gt;/&lt;repo&gt;/dispatches\n</code></pre> <p>Python <pre><code>import json\nimport requests\nfrom loguru import logger\n\nORG = \"my-org\"\nREPO = \"my-repo\"\nTOKEN = \"my-secret-token\"\n\ndef get_github_dispatch_response(team_name):\n    external_repo = f\"https://api.github.com/repos/{ORG}/{REPO}/dispatches\"\n    data = {\"event_type\": \"my-type\"}\n    message = {\"message\": f\"Triggered by workflow dispatch\"}\n    payload = {\"client_payload\": message}\n    data.update(payload)\n    headers = {\"Authorization\": f\"Bearer {TOKEN}\"}\n    response = requests.post(external_repo, data=json.dumps(data), headers=headers)\n    if response.status_code == 204:\n        return response\n    logger.error(f\"Workflow dispatch failed. Error code and message: {response.status_code} - {response.text}\")\n    response.raise_for_status()\n</code></pre></p>"},{"location":"blog/2023/07/02/github-repository-dispatch/#link","title":"Link","text":"<p>Repository Dispatch</p> <p>How to create a token</p>"},{"location":"blog/2023/07/07/github-shared-action/","title":"Github shared action","text":""},{"location":"blog/2023/07/07/github-shared-action/#originally-posted-at-linkedin-at-july-7-2023","title":"originally posted at LinkedIn at July 7, 2023","text":""},{"location":"blog/2023/07/07/github-shared-action/#background","title":"Background","text":"<p>I need to set up 3 GitHub workflows for 10 repo. For each workflow, it will run a similar bash script.</p>"},{"location":"blog/2023/07/07/github-shared-action/#first-trial-create-separate-bash-script-and-workflow","title":"First trial - Create Separate Bash Script and Workflow","text":"<p>On my first trial, I did the following:</p> <ol> <li>work on a single repo</li> <li>create three separate bash scripts</li> <li>create three separate workflows</li> </ol> <p>Here are the three bash scripts:</p> <p>run-dependabot-alert.sh <pre><code>#!/bin/sh\n\nTOKEN=$1\nREPO=$2\n\nget_response(){\n  curl -sL \\\n    -H \"Accept: application/vnd.github+json\" \\\n    -H \"Authorization: Bearer $TOKEN\"\\\n    -H \"X-GitHub-Api-Version: 2022-11-28\" \\\n    https://api.github.com/repos/januschung/\"$REPO\"/dependabot/alerts\n}\n\ndo_something(){\n    echo \"Running Dependabot!\"\n}\n\nget_response\ndo_something\n</code></pre> run-secret-scanning.sh <pre><code>#!/bin/sh\n\nTOKEN=$1\nREPO=$2\n\nget_response(){\n  curl -sL \\\n    -H \"Accept: application/vnd.github+json\" \\\n    -H \"Authorization: Bearer $TOKEN\"\\\n    -H \"X-GitHub-Api-Version: 2022-11-28\" \\\n    https://api.github.com/repos/januschung/\"$REPO\"/secret-scanning/alerts\n}\n\ndo_something(){\n    echo \"Running Secret Scanning!\"\n}\n\nget_response\ndo_something\n</code></pre></p> <p>run-codeql-alert.sh <pre><code>#!/bin/sh\n\nTOKEN=$1\nREPO=$2\n\nget_response(){\n  curl -sL \\\n    -H \"Accept: application/vnd.github+json\" \\\n    -H \"Authorization: Bearer $TOKEN\"\\\n    -H \"X-GitHub-Api-Version: 2022-11-28\" \\\n    https://api.github.com/repos/januschung/\"$REPO\"/code-scanning/alerts\n}\n\ndo_something(){\n    echo \"Running CodeQL Scanning!\"\n}\n\nget_response\ndo_something\n</code></pre> Here is one of the three workflow, as they are more like the same:</p> <p>dependabot-workflow.yml <pre><code>name: 'Dependabot Alert Check'\non:\n  push:\n    branches:\n      - 'main'\n\njobs:\n  dependabot-scanning-alert-check:\n    runs-on: ubuntu-latest\n    steps:\n      - uses: actions/checkout@v3\n        with:\n          token: ${{ secrets.GH_TOKEN }}\n      - name: Run Dependabot Alert Check\n        shell: bash\n        run: |\n          ./scripts/run-dependabot-alert.sh ${{ secret.GH_TOKEN }} ${{ github.event.repository.name }}\n</code></pre></p> <p>As you can see, there are a lot of similar logic from both of the bash scripts and the workflows. It will be tedious to repeat the work across the other repos.</p> <p>Can I do better? Sure!</p>"},{"location":"blog/2023/07/07/github-shared-action/#second-trial-create-shareable-composite-action","title":"Second Trial - Create Shareable Composite Action","text":"<p>Since all of the 10 repo will share the same logic, I converted the workflow into a shareable composite action in a dedicated repo. In so doing, the other repo can reference the same action. Here are the benefits:</p> <ul> <li>a single source of truth</li> <li>easier to maintain one set of bash scripts and action</li> <li>easier to get the rest of the repo onboard</li> </ul> <p>Here is the shareable composite action:</p> <pre><code>name: 'Alert Check'\ndescription: 'Run alert check'\ninputs:\n  alert-type:\n    description: 'Alert type to run - code-scanning, dependabot or secret-scanning'\n    required: true\n    default: 'dependabot'\n  gh-token:\n    required: true\n\nruns:\n  using: \"composite\"\n  steps:\n    - uses: actions/checkout@v3\n      with:\n        token: ${{ inputs.gh-token }}\n    - shell: bash\n      run: echo \"${{ github.action_path }}\" &gt;&gt; $GITHUB_PATH\n    - shell: bash\n      run: |\n        ${{ alert-type }}.sh ${{ inputs.gh-token }} ${{ github.event.repository.name }}\n</code></pre> <p>Note that the bash scripts are now migrated from <code>scripts</code> folder into <code>.github/actions/alerts</code></p> <p>The corresponding workflow that reference the shareable composite action:</p> <pre><code>on: [push]\n\njobs:\n  run_alert:\n    runs-on: ubuntu-latest\n    name: Run Dependabot Alert\n    steps:\n      - uses: actions/checkout@v3\n      - uses: januschung/shared-action-repo/.github/actions/alerts@main\n        with:\n          gh-token: ${{ secrets.GH_TOKEN }}\n          alert-type: ${{ matrix.type }}\n</code></pre> <p>Looking way much better now. But, can I do even better?</p>"},{"location":"blog/2023/07/07/github-shared-action/#third-trial-consolidate-into-a-single-bash-script-and-a-single-action","title":"Third Trial - Consolidate Into a Single Bash Script and a Single Action","text":"<p>Since all three bash scripts share 80% of the same logic, I consolidated them into a single one:</p> <p>alert.sh <pre><code>#!/bin/sh\n\nTOKEN=$1\nREPO=$2\nTYPE=$3\n\nget_response(){\n  curl -sL \\\n    -H \"Accept: application/vnd.github+json\" \\\n    -H \"Authorization: Bearer $TOKEN\"\\\n    -H \"X-GitHub-Api-Version: 2022-11-28\" \\\n    https://api.github.com/repos/januschung/\"$REPO\"/\"$TYPE\"/alerts\n}\n\ndo_something(){\n    if [ \"TYPE\" == 'code-scanning' ]; then\n        do_something_code_scanning\n    elif [ \"TYPE\" == 'dependabot' ]; then\n        do_something_dependabot\n    elif [ \"TYPE\" == 'secret-scanning' ]; then\n        do_something_secret_scanning\n    else\n        exit 1\n    fi\n}\n\ndo_something_code_scanning(){\n    echo \"Running Secret Scanning!\"\n}\n\ndo_something_dependabot(){\n    echo \"Running Dependabot!\"\n}\n\ndo_something_secret_scanning(){\n    echo \"Running Secret Scanning!\"\n}\n\nget_response\ndo_something\n</code></pre></p> <p>The consolidated action.yml</p> <pre><code>name: 'Alert Check'\ndescription: 'Run alert check'\ninputs:\n  alert-type:\n    description: 'Alert type to run - code-scanning, dependabot or secret-scanning'\n    required: true\n    default: 'dependabot'\n  gh-token:\n    required: true\n\nruns:\n  using: \"composite\"\n  steps:\n    - uses: actions/checkout@v3\n      with:\n        token: ${{ inputs.gh-token }}\n    - shell: bash\n      run: echo \"${{ github.action_path }}\" &gt;&gt; $GITHUB_PATH\n    - shell: bash\n      run: |\n        alert.sh ${{ inputs.gh-token }} ${{ github.event.repository.name }} {{ inputs.alert-type }}\n</code></pre> <p>I do not want to create three separate workflows from the external repo either, so I use GitHub Matrix in a single workflow:</p> <pre><code>on: [push]\n\njobs:\n  run_alert:\n    runs-on: ubuntu-latest\n    name: Run Alert\n    strategy:\n      matrix:\n        type: ['code-scanning', 'dependabot', 'secret-scanning']\n    steps:\n      - uses: actions/checkout@v3\n      - uses: januschung/shared-action-repo/.github/actions/alerts@main\n        with:\n          gh-token: ${{ secrets.GH_TOKEN }}\n          alert-type: ${{ matrix.type }}\n</code></pre> <p>That is it! I hope this helps if you are into similar situation in the coming future.</p>"},{"location":"blog/2023/07/07/github-shared-action/#links","title":"Links","text":"<p>Composite Action</p> <p>Matrix</p>"},{"location":"blog/2023/04/18/helm-template-tips/","title":"Helm Template Tips","text":"<p>Helm templates provide a powerful way to configure Kubernetes manifests dynamically. In this post, we\u2019ll cover some useful tricks, including:</p> <ul> <li> <p>Handling optional maps</p> </li> <li> <p>Setting default values</p> </li> <li> <p>Using ternary expressions</p> </li> <li> <p>Other useful Helm template functions</p> </li> </ul> <p></p>"},{"location":"blog/2023/04/18/helm-template-tips/#handling-optional-maps","title":"Handling Optional Maps","text":"<p>Sometimes, you want to define environment variables dynamically from an optional map. Here\u2019s a Helm template snippet to achieve this:</p> <pre><code>{{- if .Values.env }}\nenv:\n  {{- range $k, $v := .Values.env }}\n  - name: {{ $k }}\n    value: {{ $v }}\n  {{- end }}\n{{- end }}\n</code></pre>"},{"location":"blog/2023/04/18/helm-template-tips/#example-values-file","title":"Example Values File:","text":"<pre><code>env:\n  NODE_ENV: production\n  API_URL: https://api.example.com\n</code></pre> <p>If <code>.Values.env</code> is not provided, the <code>env</code> section is omitted entirely.</p>"},{"location":"blog/2023/04/18/helm-template-tips/#defining-and-using-a-map-in-the-template","title":"Defining and Using a Map in the Template","text":"<p>Instead of defining <code>env</code> in <code>values.yaml</code>, you can construct it dynamically using <code>dict</code>:</p> <pre><code>{{- $env := dict \"NODE_ENV\" .Values.global.node.version \"API_VERSION\" .Values.global.api.version }}\n</code></pre>"},{"location":"blog/2023/04/18/helm-template-tips/#setting-default-values","title":"Setting Default Values","text":"<p>If a value is optional but you want to ensure a default is used, use <code>default</code>:</p> <pre><code>image:\n  tag: \"{{ .Values.image.tag | default \"latest\" }}\"\n</code></pre> <p>This ensures <code>image.tag</code> is set to <code>latest</code> if not explicitly provided.</p>"},{"location":"blog/2023/04/18/helm-template-tips/#using-ternary-expressions","title":"Using Ternary Expressions","text":"<pre><code>host: {{ eq .Values.preview \"true\" | ternary .Values.global.domain.previewHost .Values.global.domain.host }}\n</code></pre> <p>Here, if <code>.Values.preview</code> is <code>\"true\"</code>, <code>host</code> is set to <code>.Values.global.domain.previewHost</code>; otherwise, it is set to <code>.Values.global.domain.host</code>.</p>"},{"location":"blog/2023/04/18/helm-template-tips/#checking-for-nested-values-safely","title":"Checking for Nested Values Safely","text":"<p>To avoid errors when accessing nested values, use <code>hasKey</code>:</p> <pre><code>{{- if hasKey .Values \"database\" }}\ndb:\n  host: {{ .Values.database.host | default \"localhost\" }}\n  port: {{ .Values.database.port | default 5432 }}\n{{- end }}\n</code></pre> <p>This ensures <code>.Values.database</code> exists before accessing its fields.</p>"},{"location":"blog/2023/04/18/helm-template-tips/#combining-multiple-defaults","title":"Combining Multiple Defaults","text":"<p>You can chain <code>default</code> functions:</p> <pre><code>appPort: {{ .Values.service.port | default .Values.global.defaultPort | default 8080 }}\n</code></pre> <p>This checks for <code>service.port</code>, then <code>global.defaultPort</code>, and falls back to <code>8080</code>.</p>"},{"location":"blog/2023/04/18/helm-template-tips/#conclusion","title":"Conclusion","text":"<p>Helm templates provide flexible ways to handle optional values, defaults, and conditional logic. These tricks help create robust and reusable Helm charts.</p>"},{"location":"blog/2025/06/29/the-devops-odyssey-continues-evolving-from-docker-to-k3s-with-ansible/","title":"The DevOps Odyssey Continues: Evolving from Docker to K3s with Ansible","text":""},{"location":"blog/2025/06/29/the-devops-odyssey-continues-evolving-from-docker-to-k3s-with-ansible/#originally-posted-at-linkedin-at-july-25-2025","title":"originally posted at LinkedIn at July 25, 2025","text":"<p>In Part 1, I turned an OCI Free Tier VM into a fully automated, HTTPS-secured Docker host using Terraform, Ansible, Traefik, and GitHub Actions. That stack was great for monoliths or simple containers.</p> <p>But containers want orchestration. And I want GitOps.</p> <p>So this phase of the odyssey shifts gears: replacing Docker Compose with K3s \u2014 a lightweight Kubernetes distribution that fits beautifully in constrained environments like OCI free tier.</p> <p>The goal? A production-grade Kubernetes control plane, fully bootstrapped with Ansible, ready for GitOps.</p> <p></p>"},{"location":"blog/2025/06/29/the-devops-odyssey-continues-evolving-from-docker-to-k3s-with-ansible/#why-k3s","title":"Why K3s?","text":"<p>K3s gives me:</p> <ul> <li>A certified Kubernetes API in ~100MB</li> <li>Zero hassle install (<code>curl | sh</code>)</li> <li>Embedded etcd, flannel, containerd \u2014 batteries included</li> <li>Low memory footprint (runs on 1GB RAM)</li> <li>Native ARM64 support (ideal for OCI\u2019s Ampere A1 VMs)</li> </ul>"},{"location":"blog/2025/06/29/the-devops-odyssey-continues-evolving-from-docker-to-k3s-with-ansible/#replacing-docker-with-k3s","title":"Replacing Docker with K3s","text":"<p>In the original setup, Ansible turned the vanilla OCI VM into a Docker host with:</p> <ul> <li>Docker + firewalld</li> <li>Traefik via Compose</li> <li>An exported acme.json for TLS</li> </ul> <p>Now I\u2019m swapping that out with:</p> <ul> <li>K3s with embedded Kubernetes components</li> <li>Proper kernel + sysctl config</li> <li>Secure kubectl access for the opc user</li> </ul>"},{"location":"blog/2025/06/29/the-devops-odyssey-continues-evolving-from-docker-to-k3s-with-ansible/#ansible-playbook-k3syml","title":"Ansible Playbook: k3s.yml","text":"<p>Here\u2019s the simplified playbook that bootstraps the cluster:</p> <pre><code>- name: Disable swap\n  command: swapoff -a\n  when: ansible_swaptotal_mb &gt; 0\n\n- name: Remove swap entry from /etc/fstab\n  replace:\n    path: /etc/fstab\n    regexp: '(^.*swap.*$)'\n    replace: '# \\1'\n\n- name: Ensure br_netfilter module is loaded\n  modprobe:\n    name: br_netfilter\n    state: present\n\n- name: Ensure br_netfilter is loaded on boot\n  copy:\n    dest: /etc/modules-load.d/k8s.conf\n    content: \"br_netfilter\\n\"\n    mode: '0644'\n\n- name: Configure sysctl for Kubernetes networking\n  sysctl:\n    name: \"{{ item.name }}\"\n    value: \"{{ item.value }}\"\n    sysctl_set: yes\n    reload: yes\n  loop:\n    - { name: net.bridge.bridge-nf-call-iptables, value: 1 }\n    - { name: net.bridge.bridge-nf-call-ip6tables, value: 1 }\n    - { name: net.ipv4.ip_forward, value: 1 }\n\n- name: Install K3s\n  shell: |\n    curl -sfL https://get.k3s.io | sh -s - server --cluster-init\n  args:\n    creates: /usr/local/bin/k3s\n\n- name: Wait for K3s service to be active\n  systemd:\n    name: k3s\n    state: started\n    enabled: yes\n\n- name: Wait for node to be Ready\n  shell: |\n    export PATH=/usr/local/bin:$PATH\n    kubectl get nodes --no-headers | grep ' Ready '\n  register: node_ready_check\n  retries: 10\n  delay: 15\n  until: node_ready_check.rc == 0\n  environment:\n    KUBECONFIG: /etc/rancher/k3s/k3s.yaml\n\n- name: Ensure chrony time sync service is running\n  package:\n    name: chrony\n    state: present\n\n- name: Enable and start chronyd\n  systemd:\n    name: chronyd\n    state: started\n    enabled: yes\n</code></pre>"},{"location":"blog/2025/06/29/the-devops-odyssey-continues-evolving-from-docker-to-k3s-with-ansible/#after-ansible-runs","title":"After Ansible Runs","text":"<p><pre><code>kubectl get nodes\n</code></pre> You should see: <pre><code>NAME STATUS ROLES AGE VERSION\nk3s-master Ready control-plane,master 2m v1.29.3+k3s1\n</code></pre></p> <p>The VM is now a Kubernetes control plane.</p>"},{"location":"blog/2025/06/29/the-devops-odyssey-continues-evolving-from-docker-to-k3s-with-ansible/#why-this-matters","title":"Why This Matters","text":"<p>This setup transforms the free-tier VM into a modern orchestrator:</p> <ul> <li>GitOps-ready (next post will wire up Argo CD)</li> <li>Secrets-ready (we\u2019ll integrate Sealed Secrets)</li> <li>Future-proof (easily extend to multi-node or HA)</li> </ul> <p>And it\u2019s fully idempotent \u2014 re-run Ansible, and you land in the same good state.</p>"},{"location":"blog/2025/06/29/the-devops-odyssey-continues-evolving-from-docker-to-k3s-with-ansible/#coming-next","title":"Coming Next","text":"<p>Next up, I\u2019ll show how to:</p> <ul> <li>Install and expose Argo CD with TLS and DuckDNS</li> <li>Bootstrap workloads with GitOps from a self-managed repo</li> <li>Manage secrets with Bitnami Sealed Secrets</li> </ul> <p>Thanks for following the journey \u2014 automation continues.</p>"},{"location":"blog/2025/07/31/the-devops-odyssey-part-3-gitops-on-k3s-with-argo-cd--self-managing-infrastructure-from-a-single-commit/","title":"The DevOps Odyssey, Part 3: GitOps on K3s with Argo CD \u2014 Self-Managing Infrastructure from a Single Commit","text":""},{"location":"blog/2025/07/31/the-devops-odyssey-part-3-gitops-on-k3s-with-argo-cd--self-managing-infrastructure-from-a-single-commit/#originally-posted-at-linkedin-at-july-31-2025","title":"originally posted at LinkedIn at July 31, 2025","text":"<p>In Part 1, we bootstrapped a zero-click deployment pipeline on OCI using Terraform, Ansible, and Docker Compose \u2014 complete with HTTPS, DNS, and CI/CD.</p> <p>Part 2 evolved that foundation into a Kubernetes-native architecture, replacing Docker with K3s. That gave us a declarative control plane and a better foundation for future growth \u2014 without sacrificing simplicity or resource constraints.</p> <p>Now, in Part 3, we finally bring in GitOps: managing the entire cluster from a Git repository using Argo CD. This marks the transition from automation to self-reconciliation \u2014 and sets the stage for horizontal scaling and federated identity in the next phase.</p> <p></p>"},{"location":"blog/2025/07/31/the-devops-odyssey-part-3-gitops-on-k3s-with-argo-cd--self-managing-infrastructure-from-a-single-commit/#a-quick-look-back-where-we-are","title":"A Quick Look Back: Where We Are","text":"<ul> <li>\u2705 One OCI Free Tier VM running K3s  </li> <li>\u2705 Public-facing DNS via DuckDNS  </li> <li>\u2705 Traefik 3 routing HTTPS traffic with automatic Let's Encrypt certs  </li> <li>\u2705 Ansible roles to automate K3s provisioning</li> </ul> <p>But until now, workloads still required Ansible to deploy. Changes were pushed, not pulled. That meant some infrastructure lived in code, but some drifted with time.</p> <p>It was fast, but not fully convergent.</p>"},{"location":"blog/2025/07/31/the-devops-odyssey-part-3-gitops-on-k3s-with-argo-cd--self-managing-infrastructure-from-a-single-commit/#why-gitops","title":"Why GitOps?","text":"<p>GitOps introduces a shift in control: from pushing changes to letting the cluster pull and reconcile them.</p> <p>That means:</p> <ul> <li>Cluster state always matches Git  </li> <li>Drift is detected and corrected automatically  </li> <li>Rollbacks are just Git reverts  </li> <li>CI becomes a pure code pipeline \u2014 no secrets, no cloud credentials  </li> <li>Disaster recovery is repeatable and fast  </li> </ul> <p>For small teams or solo builders, it replaces tribal knowledge with source of truth. For larger efforts, it enforces consistency and control.</p>"},{"location":"blog/2025/07/31/the-devops-odyssey-part-3-gitops-on-k3s-with-argo-cd--self-managing-infrastructure-from-a-single-commit/#installing-argo-cd-with-helm-via-ansible","title":"Installing Argo CD with Helm (via Ansible)","text":"<p>We install Argo CD directly with Helm using an Ansible role, so we can keep cluster provisioning clean, repeatable, and decoupled from GitOps bootstrapping.</p> <p>This avoids the \"chicken and egg\" problem: GitOps can\u2019t install Argo CD if Argo CD isn\u2019t already present. So the bootstrap is handled by Ansible \u2014 but only once.</p> <pre><code># ansible\n- name: Install/Upgrade ArgoCD via Helm\n  kubernetes.core.helm:\n    name: argocd\n    chart_ref: argo/argo-cd\n    release_namespace: argocd\n    create_namespace: true\n    kubeconfig: \"{{ kubeconfig }}\"\n    values_files:\n      - /tmp/argocd-values.yaml\n    wait: true\n    wait_timeout: 600s\n    update_repo_cache: true\n</code></pre> <pre><code># argocd-values.yaml\nconfigs:\n  params:\n    server.insecure: true\n    server.rootpath: /argocd\n    server.basehref: /argocd/\n\nserver:\n  ingress:\n    extraArgs:\n      - --insecure\n      - --rootpath=/argocd\n      - --basehref=/argocd/\n    enabled: true\n    ingressClassName: traefik\n    hostname: janusc.duckdns.org\n    path: /argocd\n    pathType: Prefix\n    annotations:\n      traefik.ingress.kubernetes.io/router.entrypoints: web,websecure\n      traefik.ingress.kubernetes.io/router.tls.certresolver: le\n    tls: true\n</code></pre> <p>This also configures a public HTTPS endpoint, fronted by Traefik using DNS-validated certificates from Let's Encrypt.</p>"},{"location":"blog/2025/07/31/the-devops-odyssey-part-3-gitops-on-k3s-with-argo-cd--self-managing-infrastructure-from-a-single-commit/#the-bootstrap-pattern-one-root-application-to-rule-them-all","title":"The Bootstrap Pattern: One Root Application to Rule Them All","text":"<p>Once Argo CD is up, everything else is declared in Git \u2014 starting with a root application.</p> <pre><code>apiVersion: argoproj.io/v1alpha1\nkind: Application\nmetadata:\n  name: bootstrap-argocd\n  namespace: argocd\nspec:\n  project: default\n  sources:\n    - path: bootstrap\n      repoURL: https://github.com/januschung/argocd\n      targetRevision: HEAD\n    - path: infra\n      repoURL: https://github.com/januschung/argocd\n      targetRevision: HEAD\n      directory:\n        recurse: true\n        include: \"{*application.yaml,*application-set.yaml}\"\n  destination:\n    server: https://kubernetes.default.svc\n    namespace: argocd\n  syncPolicy:\n    automated:\n      prune: true\n      selfHeal: true\n</code></pre> <p>This single manifest creates a fan-out effect: every other service \u2014 including Argo CD itself \u2014 is defined as a child application within this repo.</p> <p>The moment root-app syncs, it recursively applies the rest of the cluster configuration:</p> <pre><code>root/\n\u251c\u2500\u2500 ansible/                    # Infrastructure provisioning scripts (e.g. Argo CD bootstrap)\n\u251c\u2500\u2500 bootstrap/                 \n\u2502   \u2514\u2500\u2500 root-application.yml   # The entry-point Argo CD Application that syncs everything else\n\u251c\u2500\u2500 infra/\n\u2502   \u251c\u2500\u2500 argocd/                # Helm chart and values for Argo CD (self-managed)\n\u2502   \u2514\u2500\u2500 sealed-secrets/        # Placeholder for Sealed Secrets controller (to be utilitzed in Part 4)\n\u2514\u2500\u2500 future/                    # Staging ground for upcoming apps or services under GitOps control\n</code></pre> <p>Even new workloads or platform services follow the same pattern \u2014 just add a new folder and application YAML, commit, and walk away.</p>"},{"location":"blog/2025/07/31/the-devops-odyssey-part-3-gitops-on-k3s-with-argo-cd--self-managing-infrastructure-from-a-single-commit/#self-management-argo-cd-manages-argo-cd","title":"Self-Management: Argo CD Manages Argo CD","text":"<p>We declare Argo CD\u2019s own Helm chart as a child application \u2014 inside Git \u2014 so that any changes to its configuration are now tracked, versioned, and deployed via GitOps.</p> <pre><code>apiVersion: argoproj.io/v1alpha1\nkind: Application\nmetadata:\n  name: argocd\n  namespace: argocd\nspec:\n  project: default\n  source:\n    repoURL: https://github.com/januschung/argocd\n    targetRevision: HEAD\n    path: infra/argocd\n    helm:\n      valueFiles:\n        - values.yaml\n  destination:\n    server: https://kubernetes.default.svc\n    namespace: argocd\n  syncPolicy:\n    automated:\n      prune: true\n      selfHeal: true\n</code></pre> <p>What this enables:</p> <ul> <li>Changes to Argo CD config (e.g., enabling DEX or setting custom timeouts) happen in Git  </li> <li>Upgrades are triggered via Helm value bumps in a PR  </li> <li>Everything becomes testable, reviewable, and roll-backable  </li> </ul> <p>Even Argo CD\u2019s RBAC, repo credentials, and UI branding can be automated this way.</p>"},{"location":"blog/2025/07/31/the-devops-odyssey-part-3-gitops-on-k3s-with-argo-cd--self-managing-infrastructure-from-a-single-commit/#operational-reality-zero-drift-zero-touch","title":"Operational Reality: Zero-Drift, Zero-Touch","text":"<p>Once the bootstrap completes:</p> <ul> <li>The Argo CD UI becomes a live view of your cluster\u2019s alignment with Git  </li> <li>Drifted resources are auto-corrected (or highlighted)  </li> <li>Every change has an audit trail  </li> <li>Disaster recovery is just: reprovision \u2192 let Argo CD sync from Git  </li> </ul> <p>This is infrastructure with feedback loops, not just automation.</p>"},{"location":"blog/2025/07/31/the-devops-odyssey-part-3-gitops-on-k3s-with-argo-cd--self-managing-infrastructure-from-a-single-commit/#current-architecture","title":"Current Architecture","text":"<pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 Git Repository             \u2502\n\u2502 - root/                    \u2502\n\u2502   - infra/                 \u2502\n\u2502     - argocd/              \u2502\n\u2502     - sealed-secret/       \u2502\n\u2502   - applications/          \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n             \u2502 GitOps sync\n             \u25bc\n     \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n     \u2502 Argo CD     \u2502\n     \u2514\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n           \u2502\n     \u250c\u2500\u2500\u2500\u2500\u2500\u25bc\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n     \u2502 K3s Cluster \u2502\n     \u2502 (OCI Free)  \u2502\n     \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"blog/2025/07/31/the-devops-odyssey-part-3-gitops-on-k3s-with-argo-cd--self-managing-infrastructure-from-a-single-commit/#whats-ahead-scale-identity-secrets","title":"What\u2019s Ahead: Scale, Identity, Secrets","text":"<p>The current setup is production-worthy for single-node workloads \u2014 but it\u2019s not the end of the journey.</p> <p>Next, I\u2019ll introduce:</p> <ul> <li>A new K3s worker node, joining the same cluster  </li> <li>DEX integration for GitHub and Google login \u2014 so platform access is federated, not static  </li> <li>Bitnami Sealed Secrets, finally put to use for managing encrypted secrets in Git  </li> </ul> <p>Each of these steps is already staged in Git. When the time comes, syncing the root app will roll them out \u2014 cleanly, safely, and declaratively.</p>"},{"location":"blog/2025/07/31/the-devops-odyssey-part-3-gitops-on-k3s-with-argo-cd--self-managing-infrastructure-from-a-single-commit/#final-takeaways","title":"Final Takeaways","text":"<p>This phase of the odyssey marks a shift from automation to autonomy.</p> <p>It\u2019s no longer about writing scripts to install services \u2014 it\u2019s about modeling intent in Git and letting the system enforce it. This Git-first design minimizes guesswork, eliminates drift, and enables confident change \u2014 even at scale.</p> <p>You don\u2019t need a fleet of VMs or a Kubernetes team to get here.</p> <p>You just need the right foundation, the right tools, and a commitment to source-driven infrastructure.</p> <p>From a blank cloud account to a Git-managed cluster, self-correcting and TLS-secured \u2014 the journey continues.</p>"},{"location":"blog/2025/11/13/the-devops-odyssey-part-6--closing-the-loop-with-github-auto-tagging/","title":"The DevOps Odyssey, Part 6 \u2014 Closing the Loop with GitHub Auto-Tagging","text":""},{"location":"blog/2025/11/13/the-devops-odyssey-part-6--closing-the-loop-with-github-auto-tagging/#originally-posted-at-linkedin-at-november-13-2025","title":"originally posted at LinkedIn at November 13, 2025","text":"<p>In the last chapter, I left a promise \u2014 to make the system truly GitOps-native. To bridge the small but important gap between building images and updating manifests.</p> <p>That loop is now closed.</p> <p>Every time a Docker image for Job Winner or the photo app is built and pushed, GitHub Actions updates the Argo CD repository automatically. No manual tag edits, no pull requests waiting in the dark. The commit that produces the container now also defines its deployment.</p> <p>The infrastructure finally breathes on its own.</p> <p></p>"},{"location":"blog/2025/11/13/the-devops-odyssey-part-6--closing-the-loop-with-github-auto-tagging/#from-ci-to-continuous-reconciliation","title":"From CI to Continuous Reconciliation","text":"<p>The workflow starts in the application repository \u2014 one for Jobwinner\u2019s backend, one for its frontend UI. Each push triggers a GitHub Action that builds and publishes a multi-architecture image to <code>ghcr.io</code>.</p> <pre><code># .github/workflows/deploy-jobwinner-backend.yml\non:\n  push:\n    paths:\n      - 'job-winner/**'\n      - '.github/workflows/deploy-jobwinner-backend.yml'\n    branches:\n      - '**'\n\njobs:\n  docker-publish:\n    runs-on: ubuntu-latest\n    steps:\n      - uses: actions/checkout@v4\n      - name: Build image\n        uses: ./.github/actions/docker-build-or-tag\n        with:\n          token: ${{ secrets.GH_TOKEN }}\n          image-name: job-winner\n          dockerfile-dir: job-winner\n\n  update-deploy-repo-tag:\n    needs: docker-publish\n    runs-on: ubuntu-latest\n    steps:\n      - uses: actions/checkout@v4\n      - name: Update Argo CD deployment repo\n        uses: ./.github/actions/update-deployment-repo\n        with:\n          app_name: job-winner\n          deployment_type: backend\n          app_id: ${{ secrets.GH_APP_ID }}\n          private_key: ${{ secrets.GH_APP_PRIVATE_KEY }}\n</code></pre> <p>A similar workflow in <code>deploy-jobwinner-frontend.yml</code> does the same for the UI build. Both use the same two composite actions that form the backbone of this automation.</p>"},{"location":"blog/2025/11/13/the-devops-odyssey-part-6--closing-the-loop-with-github-auto-tagging/#the-two-actions-that-make-it-work","title":"The Two Actions That Make It Work","text":"<p>The first composite action \u2014 <code>docker-build-or-tag</code> \u2014 builds and pushes the image, tagging both <code>latest</code> and the abbreviated commit SHA.</p> <pre><code># .github/actions/docker-build-or-tag/action.yml\nruns:\n  using: composite\n  steps:\n    - uses: actions/checkout@v4\n    - uses: docker/login-action@v3\n      with:\n        registry: ghcr.io\n        username: januschung\n        password: ${{ inputs.token }}\n    - uses: docker/setup-buildx-action@v3\n    - uses: docker/build-push-action@v6\n      with:\n        push: true\n        platforms: linux/amd64,linux/arm64\n        context: ${{ inputs.dockerfile-dir }}\n        tags: |\n          ghcr.io/januschung/${{ inputs.image-name }}:${{ env.COMMIT_SHA }}\n          ghcr.io/januschung/${{ inputs.image-name }}:latest\n</code></pre> <p>Once the image is live in GHCR, the second composite action \u2014 <code>update-deployment-repo</code> \u2014 fires a <code>repository_dispatch</code> to the Argo CD repo, carrying a payload with the new tag, app name, and deployment type.</p> <pre><code># .github/actions/update-deployment-repo/action.yml\nruns:\n  using: composite\n  steps:\n    - uses: actions/create-github-app-token@v2\n      with:\n        app-id: ${{ inputs.app_id }}\n        private-key: ${{ inputs.private_key }}\n        owner: januschung\n        repository: argocd\n\n    - name: Trigger Argo CD repo dispatch\n      run: |\n        curl -sS -X POST \\\n          -H \"Authorization: token ${{ steps.app-token.outputs.token }}\" \\\n          -H \"Accept: application/vnd.github.v3+json\" \\\n          https://api.github.com/repos/januschung/argocd/dispatches \\\n          -d '{\"event_type\":\"update-tag\",\"client_payload\":{\"tag\":\"${{ env.COMMIT_SHA }}\",\"app_name\":\"${{ inputs.app_name }}\",\"deployment_type\":\"${{ inputs.deployment_type }}\"}}'\n</code></pre>"},{"location":"blog/2025/11/13/the-devops-odyssey-part-6--closing-the-loop-with-github-auto-tagging/#short-lived-trust-with-github-app-tokens","title":"Short-Lived Trust with GitHub App Tokens","text":"<p>The trigger uses a GitHub App instead of a personal access token \u2014 and that choice matters. Each workflow generates a short-lived installation token, valid for about an hour and scoped only to the Argo CD repository. There are no static credentials to rotate, no personal tokens hidden in secrets. It\u2019s ephemeral trust: the token exists just long enough to update a manifest, then disappears.</p> <p>It\u2019s a small design detail, but it\u2019s what turns automation into governance \u2014 a system that not only acts automatically, but acts responsibly.</p>"},{"location":"blog/2025/11/13/the-devops-odyssey-part-6--closing-the-loop-with-github-auto-tagging/#updating-the-argo-cd-repository","title":"Updating the Argo CD Repository","text":"<p>In the Argo CD repository, an <code>update-tag.yaml</code> workflow listens for the <code>repository_dispatch</code> event. It checks out the repo, modifies the Helm chart <code>values.yaml</code>, creates a pull request, and auto-merges it.</p> <pre><code># .github/workflows/update-tag.yaml\non:\n  repository_dispatch:\n    types: [update-tag]\n\njobs:\n  update-tag:\n    runs-on: ubuntu-latest\n    steps:\n      - uses: actions/checkout@v5\n      - name: Install yq\n        run: |\n          sudo curl -sL https://github.com/mikefarah/yq/releases/latest/download/yq_linux_amd64 -o /usr/local/bin/yq\n          sudo chmod +x /usr/local/bin/yq\n      - name: Update image tag\n        run: |\n          cd lab/${{ github.event.client_payload.app_name }}/chart\n          yq -i \".${{ github.event.client_payload.deployment_type }}.image.tag = \\\"${{ github.event.client_payload.tag }}\\\"\" values.yaml\n      - name: Create PR and enable auto-merge\n        uses: peter-evans/create-pull-request@v6\n        with:\n          commit-message: \"Auto update image tag to ${{ github.event.client_payload.tag }}\"\n          branch: auto/update-${{ github.event.client_payload.app_name }}\n          delete-branch: true\n</code></pre> <p>By the time the PR merges, Argo CD detects the manifest change and redeploys the updated image automatically. The workflow becomes a heartbeat \u2014 a small, invisible cycle that keeps the system alive and current.</p>"},{"location":"blog/2025/11/13/the-devops-odyssey-part-6--closing-the-loop-with-github-auto-tagging/#continuous-motion-minimal-intervention","title":"Continuous Motion, Minimal Intervention","text":"<p>This is not about adding complexity; it\u2019s about removing the last manual gesture. No human now decides when \u201clatest\u201d becomes \u201crunning.\u201d The repository does.</p> <p>The pattern scales naturally \u2014 every app can reuse the same pair of actions, regardless of language or build context. It\u2019s self-similar, declarative, and composable \u2014 qualities that make GitOps more than just a pipeline.</p> <p>The Jobwinner backend, the frontend UI, and even the photo app now move in rhythm. Each push ripples across the system, quietly, predictably.</p>"},{"location":"blog/2025/11/13/the-devops-odyssey-part-6--closing-the-loop-with-github-auto-tagging/#reflection","title":"Reflection","text":"<p>Each phase of this Odyssey has refined the boundaries between tools and intent. This one erases another \u2014 the point where CI ends and deployment begins.</p> <p>From here, the next chapter will look outward again \u2014 installing Prometheus and Grafana, bringing visibility and voice to this living system. Metrics, logs, and dashboards that let the cluster tell its own story.</p> <p>Somewhere between a commit and a container, between Git and light, the Odyssey continues.</p>"},{"location":"blog/2025/10/23/the-devops-odyssey-part-5-migration-of-job-winner-and-release-of-the-photo-app/","title":"The DevOps Odyssey, Part 5: Migration of Job Winner and Release of the Photo App","text":""},{"location":"blog/2025/10/23/the-devops-odyssey-part-5-migration-of-job-winner-and-release-of-the-photo-app/#originally-posted-at-linkedin-at-oct-23-2025","title":"originally posted at LinkedIn at Oct 23, 2025","text":"<p>In Part 4, I closed with a simple plan: migrate Job Winner into the cluster and build a photo app that would reconnect my creative and technical worlds. Those two threads finally came together \u2014 one practical, one personal \u2014 and in the process, the Odyssey took another quiet but meaningful turn.</p> <p></p>"},{"location":"blog/2025/10/23/the-devops-odyssey-part-5-migration-of-job-winner-and-release-of-the-photo-app/#from-docker-compose-to-helm","title":"From Docker Compose to Helm","text":"<p>When I first deployed Job Winner months ago, it lived alone \u2014 running inside Docker Compose on a single Oracle Cloud VM, wired up with Traefik and DuckDNS. It worked, and it worked well. But as the system grew, each new service meant another <code>docker-compose.yml</code>, another CI workflow, another layer to keep aligned. It wasn\u2019t hard \u2014 just fragmented. Adding one more app meant adding one more pattern to maintain. It became clear that what I needed wasn\u2019t more automation, but unification \u2014 a single way to deploy everything, no matter what it was.</p> <p>That same service now runs inside K3s, deployed through its own Helm chart, synchronized by Argo CD, and fully self-managed. The application hasn\u2019t changed much, but the way it exists has. Every configuration is now declarative. Every environment variable, every ingress path, every secret \u2014 sealed, versioned, and reconciled automatically.  </p> <p>Migrating it was not an act of reinvention but of discipline. I didn\u2019t chase new features or frameworks; I just removed friction. The build pipeline that once targeted Docker Compose now outputs a Helm release. The cluster applies it, observes it, and ensures it stays the way it should be.  </p> <p>The difference shows not in performance, but in posture. The system no longer waits for me to correct it \u2014 it corrects itself.  </p>"},{"location":"blog/2025/10/23/the-devops-odyssey-part-5-migration-of-job-winner-and-release-of-the-photo-app/#shutting-down-to-reclaim-compute","title":"Shutting Down to Reclaim Compute","text":"<p>The old VM that used to host Job Winner has served well through the early chapters \u2014 from Terraform provisioning to Docker experimentation. But now, it\u2019s time to let it rest.  </p> <p>Rather than leave it idle or pay the hidden cost of unused allocation, I chose to shut it down with Terraform, releasing the compute back to Oracle OCI\u2019s free-tier pool. The infrastructure state remains preserved in Git, so when I need more capacity \u2014 a build node, or another worker for heavier workloads \u2014 I can bring it back online with a single <code>terraform apply</code>.  </p> <p>There\u2019s something satisfying about that symmetry: what once took hours of manual setup can now be undone \u2014 and redone \u2014 declaratively.  </p> <p>It\u2019s a small but important evolution. \u201cScaling down\u201d is part of the same maturity as scaling up. Automation is not only about elasticity in expansion, but also in retraction.  </p> <p>When you treat every resource as code, deletion becomes an elegant act \u2014 a form of stewardship.  </p> <p>So the freed compute is no longer waste; it\u2019s a reserve. A future node, waiting in potential energy.  </p>"},{"location":"blog/2025/10/23/the-devops-odyssey-part-5-migration-of-job-winner-and-release-of-the-photo-app/#the-photo-app-where-code-meets-memory","title":"The Photo App \u2014 Where Code Meets Memory","text":"<p>The second thread of this chapter is personal. I finally released a photo app that runs in the same cluster \u2014 a minimal single-page site that displays work from my earlier years behind the camera. Albums sit on a virtual shelf, flipping through pages with a small kinetic animation. Underneath, it\u2019s pure infrastructure: static files served by Kubernetes, cached by Traefik, observed by Argo CD, and backed by the same sealed-secret discipline as everything else.  </p> <p>Technically, it\u2019s trivial. Emotionally, it\u2019s the most meaningful deployment I\u2019ve done so far.  </p> <p>For years, my creative archive lived in folders \u2014 external drives, forgotten S3 buckets, somewhere between nostalgia and neglect. Now, the images resurface through a pipeline that speaks the language I live in today. Git commits instead of Lightroom catalogs. CI pipelines instead of export queues.  </p> <p>It\u2019s a strange fusion \u2014 photography, DevOps, and Kubernetes. But it works.  </p> <p>What I used to call a \u201cshoot\u201d has become a \u201crelease.\u201d What was once a gallery on a hard drive now rolls out as a container image. And for the first time, the same GitOps controller that deploys my infrastructure also deploys my past.  </p> <p>It\u2019s poetic in a quiet, practical way \u2014 not about nostalgia, but about continuity.  </p>"},{"location":"blog/2025/10/23/the-devops-odyssey-part-5-migration-of-job-winner-and-release-of-the-photo-app/#infrastructure-as-a-studio","title":"Infrastructure as a Studio","text":"<p>Running both Job Winner and the photo app in the same cluster feels different from spinning up services. It feels like maintaining a studio \u2014 a place where work happens, ideas live, and old projects find new light.  </p> <p>Everything is tied together through Helm and Argo CD, yet each app expresses something distinct: - Job Winner remains the workhorse \u2014 a production-grade service. - The photo app, by contrast, is almost ephemeral \u2014 personal, expressive, archival.  </p> <p>They coexist gracefully because the cluster doesn\u2019t care what it runs \u2014 only that it does so predictably, reproducibly, and securely.  </p> <p>In earlier posts I called this setup a \u201cliving system.\u201d It\u2019s becoming something more nuanced: a creative environment that merges aesthetics and automation. I write manifests with the same mindfulness I once used to compose a frame. Both require balance \u2014 between light and dark, complexity and clarity.  </p> <p>The tools change, but the principles remain the same.  </p>"},{"location":"blog/2025/10/23/the-devops-odyssey-part-5-migration-of-job-winner-and-release-of-the-photo-app/#reflection","title":"Reflection","text":"<p>Each part of this Odyssey has shifted the center of gravity a little further \u2014 from manual scripts to infrastructure as code, from orchestration to reconciliation, from systems management to self-expression.  </p> <p>Part V may seem quieter than the earlier chapters, but it\u2019s the most personal. It\u2019s where automation stopped being a pursuit of efficiency and became a medium of creativity.  </p> <p>The next step will likely bring in a GitHub auto\u2011tagging workflow \u2014 one that makes Job Winner and the photo app truly GitOps\u2011native. Each time a new Docker image is built and pushed, the workflow will automatically update the Argo\u00a0CD manifests with the latest image tag. This closes the loop between build and deployment: no manual edits, no lag, just continuous synchronization between Git and what runs in the cluster.  </p> <p>Somewhere between a cloud and a darkroom, the Odyssey continues.  </p>"},{"location":"blog/2025/10/23/the-devops-odyssey-part-5-migration-of-job-winner-and-release-of-the-photo-app/#technical-notes","title":"Technical Notes","text":"<p>Job Winner Migration - Converted Docker Compose configuration to Helm templates. - Deployed through Argo CD; secrets managed with Sealed Secrets. - Ingress and SSL managed via Traefik (HTTP-01 challenge on <code>web</code> entrypoint).  </p> <p>Compute Recycling - Original VM shut down via Terraform to release OCI Free Tier allocation. - Infrastructure preserved in state files and Git for instant re-provisioning. - Future expansion planned via additional worker nodes using the same module.  </p> <p>Photo App Deployment - SPA frontend served via K3s, using static assets and minimal backend logic. - Deployed as Helm release in the same GitOps workflow as all other services. - Represents first convergence of creative and operational pipelines.  </p>"},{"location":"blog/2025/08/31/the-devops-odyssey-part-4-secrets-github-auth-and-scaling-out/","title":"The DevOps Odyssey, Part 4: Secrets, GitHub Auth, and Scaling Out","text":""},{"location":"blog/2025/08/31/the-devops-odyssey-part-4-secrets-github-auth-and-scaling-out/#originally-posted-at-linkedin-at-aug-31-2025","title":"originally posted at LinkedIn at Aug 31, 2025","text":"<p>In Part 1, I bootstrapped a zero-click deployment pipeline on OCI with Terraform, Ansible, and Docker Compose \u2014 complete with HTTPS, DNS, and CI/CD.</p> <p>Part 2 evolved that into a Kubernetes-native architecture, replacing Docker with K3s for a declarative control plane.</p> <p>Part 3 brought in GitOps with Argo CD, letting the cluster manage itself from a single commit.  </p> <p>Now, in Part 4, I pushed the setup toward something that looks and feels much closer to production. Three key steps made that happen: </p> <ol> <li>Sealing secrets so I could finally commit them to Git safely.  </li> <li>Adding GitHub authentication with Dex, making the Argo CD UI open (read-only) to anyone with a GitHub account.  </li> <li>Expanding the cluster with a proper worker node \u2014 and replacing my ill-fated \u201cmaster as NAT\u201d shortcut with OCI\u2019s managed NAT Gateway.  </li> </ol> <p></p>"},{"location":"blog/2025/08/31/the-devops-odyssey-part-4-secrets-github-auth-and-scaling-out/#1-sealed-secrets-unlocking-gitops-for-real","title":"1. Sealed Secrets: Unlocking GitOps for Real","text":"<p>Everything in GitOps lives in Git. That\u2019s the promise, but it\u2019s also the challenge: Kubernetes <code>Secret</code> objects are just base64-encoded strings. Put them in Git as-is, and you\u2019ve basically published your passwords.  </p> <p>That\u2019s why I started Part 4 by deploying Bitnami\u2019s Sealed Secrets operator via Argo CD. Once it was running, I could safely commit sealed versions of my sensitive credentials:  </p> <ul> <li>My GitHub Container Registry pull secret (to pull images).  </li> <li>My GitHub OAuth client secret (to connect Argo CD to GitHub for login).  </li> </ul> <p>Here\u2019s what the sealed Dex secret looks like:</p> <pre><code>apiVersion: bitnami.com/v1alpha1\nkind: SealedSecret\nmetadata:\n  name: dex-github-secret\n  namespace: argocd\nspec:\n  encryptedData:\n    dex.github.clientSecret: AgCBlxRncrOcJeIdZ8+8oCr...\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/part-of: argocd\n      name: dex-github-secret\n      namespace: argocd\n    type: Opaque\n</code></pre> <p>By sealing secrets first, I laid the foundation for the next step: GitHub authentication.  </p>"},{"location":"blog/2025/08/31/the-devops-odyssey-part-4-secrets-github-auth-and-scaling-out/#2-github-auth-via-dex-a-live-demo-for-everyone","title":"2. GitHub Auth via Dex: A Live Demo for Everyone","text":"<p>With secrets secure, I moved on to authentication for Argo CD.  </p> <p>Argo CD ships with a local admin account, but I wanted more than that. This cluster is both a hobby lab and a public demo of my DevOps skills. I wanted curious folks to experience GitOps first-hand, without me handing out cluster credentials.  </p> <p>So I set up Dex with GitHub as the identity provider. My Argo CD installation is self-managed via Helm and Argo CD itself. In my <code>values.yaml</code>, I configured Dex and GitHub like this:  </p> <pre><code>argo-cd:\n  configs:\n    cm:\n      url: https://janusc.duckdns.org/argocd\n      dex.config: |\n        connectors:\n        - type: github\n          id: github\n          name: GitHub\n          config:\n            clientID: &lt;my-client-id&gt;\n            clientSecret: $dex-github-secret:dex.github.clientSecret\n            redirectURI: https://janusc.duckdns.org/argocd/api/dex/callback\n            scopes:\n            - read:org\n            - user:email\n            useLoginAsID: true\n    rbac:\n      policy.csv: |\n        g, januschung, role:admin\n      policy.default: role:readonly\n      scopes: '[profile, email]'\n</code></pre> <p>A few things to note here:  </p> <ul> <li>The <code>clientSecret</code> points to the sealed secret I committed earlier, so it never appears in plaintext.  </li> <li>RBAC is configured so my own GitHub user (<code>januschung</code>) is an admin.  </li> <li>Everyone else defaults to role:readonly.  </li> </ul> <p>That last part is the point: anyone with a GitHub account can log into my Argo CD UI, but only in read-only mode.  </p> <p>Why? Because this project is my open DevOps showcase:  </p> <ul> <li>Developers and tech enthusiasts can log in, click around, and see real GitOps in action.  </li> <li>There\u2019s no risk \u2014 the read-only role prevents anyone from changing deployments.  </li> <li>It turns Argo CD into a live demo environment, always available for anyone curious about GitOps.  </li> </ul>"},{"location":"blog/2025/08/31/the-devops-odyssey-part-4-secrets-github-auth-and-scaling-out/#3-scaling-out-adding-a-worker-node","title":"3. Scaling Out: Adding a Worker Node","text":"<p>The final step in this phase was adding a worker node. Until now, everything ran on my single K3s master. That was fine for testing, but not the kind of setup you\u2019d take to production.  </p> <p>Here\u2019s the truth: I almost didn\u2019t do it right. At first, I tried to let the master act as a NAT gateway so that my private worker could reach the internet. It felt clever, but it was actually a liability:  </p> <ul> <li>The master is supposed to run the control plane, not forward traffic.  </li> <li>If the master died, the entire cluster\u2019s egress would go dark.  </li> <li>My Terraform started looking like a spaghetti of conditional routing rules.  </li> </ul> <p>Then I realized something obvious: OCI already provides a managed NAT Gateway \u2014 and it comes with a generous free monthly quota. </p> <p>I tore out my DIY NAT setup, dropped in the managed gateway, and my network instantly became simpler and more reliable. The master node went back to doing what it does best, the worker had clean egress, and I had one less thing to babysit.  </p>"},{"location":"blog/2025/08/31/the-devops-odyssey-part-4-secrets-github-auth-and-scaling-out/#4-the-cluster-grows-up","title":"4. The Cluster Grows Up","text":"<p>With these changes in place, my setup made a leap:  </p> <ul> <li>Sealed Secrets gave me safe, GitOps-friendly secret management.  </li> <li>Dex with GitHub made Argo CD a transparent demo portal \u2014 open to anyone with a GitHub account, but read-only by default.  </li> <li>Worker nodes + managed NAT gave me a scalable, production-grade topology.  </li> </ul> <p>This is the moment where my hobby cluster stopped being a single-VM experiment and started looking like a real platform.  </p>"},{"location":"blog/2025/08/31/the-devops-odyssey-part-4-secrets-github-auth-and-scaling-out/#5-whats-next-running-real-apps","title":"5. What\u2019s Next: Running Real Apps","text":"<p>Infrastructure is great, but the real payoff comes from running apps that matter. That\u2019s what\u2019s next on this journey.  </p> <ul> <li>First up: migrating Jobwinner, my existing app that\u2019s been living in Docker Compose on a separate VM, into the Argo CD-managed K3s cluster. This consolidates everything into one pipeline and one platform.  </li> <li>And then, something more personal: I want to build a photo album app \u2014 a kind of virtual bookshelf \u2014 to showcase my past life as a photographer. Over the years, I\u2019ve taken portraits of the beautiful ladies I crossed paths with, and I want a polished gallery to present that work. This cluster gives me the perfect home for it.  </li> </ul>"},{"location":"blog/2025/08/31/the-devops-odyssey-part-4-secrets-github-auth-and-scaling-out/#wrapping-up","title":"Wrapping Up","text":"<p>Part 4 was about responsibility and trust. I learned not to cut corners with networking, I locked down secrets properly, and I opened up my GitOps UI in a way that\u2019s transparent but safe.  </p> <p>Now, the cluster isn\u2019t just a backend experiment \u2014 it\u2019s becoming a stage. One that can run my real projects, show off my skills, and even host my photography.  </p> <p>In Part 5, we\u2019ll step onto that stage: deploying Jobwinner and my photo album app with Helm + Argo CD.  </p>"},{"location":"blog/2025/06/18/the-devops-odyssey-fully-automating-oci-app-deployment-with-terraform-ansible-and-docker/","title":"The DevOps Odyssey: Fully Automating OCI App Deployment with Terraform, Ansible, and Docker","text":""},{"location":"blog/2025/06/18/the-devops-odyssey-fully-automating-oci-app-deployment-with-terraform-ansible-and-docker/#introduction-the-engineers-drive-for-automation","title":"Introduction: The Engineer's Drive for Automation","text":"<p>As a DevOps engineer, I thrive on full\u2011stack automation\u2014turning repetitive, error\u2011prone deployments into push\u2011button, ultra\u2011reliable workflows. I recently challenged myself to get Job\u00a0Winner, an opensource full\u2011stack app (Spring\u00a0Boot + React), live on Oracle Cloud Infrastructure (OCI) in less than 15\u202fminutes from a cold start. But the real goal wasn't speed alone\u2014it was idempotence: every run of the pipeline should converge the system to the exact same, secure, HTTPS\u2011enabled state without manual touch\u2011points.</p> <p></p> <p>In this post you'll travel the entire odyssey\u2014from a blank OCI account to a humming, production\u2011grade CI/CD pipeline. Expect:</p> <ul> <li>Detailed Terraform modules for free\u2011tier compute, networking, and remote state.</li> <li>Ansible roles that turn a vanilla Oracle\u00a0Linux VM into a hardened Docker host.</li> <li>Traefik 3 as a smart edge router with automatic Let's Encrypt certificates.</li> <li>GitHub\u00a0Actions tying it all together in a single commit \u2192 live site workflow.</li> <li>Hard\u2011won lessons, cost analysis, and future\u2011proofing tips.</li> </ul> <p>TL;DR: Free cloud, real domain, zero click deploys. Grab a coffee\u2014this is a long read.</p>"},{"location":"blog/2025/06/18/the-devops-odyssey-fully-automating-oci-app-deployment-with-terraform-ansible-and-docker/#why-oracle-cloud-free-tier","title":"Why Oracle Cloud Free Tier?","text":"<p>OCI's Always\u2011Free resources give you an ARM\u2011based VM (4\u00a0OCPU, 24\u202fGB RAM) or an AMD VM (1\u00a0OCPU, 1\u202fGB RAM) plus 10\u202fTB egress\u2014an unbeatable budget playground. Compared to AWS or GCP free tiers, outbound bandwidth alone makes OCI ideal for side\u2011projects with media assets or heavy API traffic.</p> Cloud Monthly Free Egress Notes OCI 10\u202fTB Always Free AWS 100\u202fGB Free tier year one GCP 1\u202fGB Always Free <p>Beyond cost, OCI integrates cleanly with HashiCorp Terraform through an official provider and supports Object\u00a0Storage buckets that double as free Terraform remote state back\u2011ends.</p>"},{"location":"blog/2025/06/18/the-devops-odyssey-fully-automating-oci-app-deployment-with-terraform-ansible-and-docker/#architecture-at-10000-feet","title":"Architecture at 10,000\u00a0Feet","text":"<pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510     GitHub Actions   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502  GitHub Repo  \u2502 \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25b6 \u2502 OCI: Object Storage    \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  stores tfstate      \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n       \u2502 Push/Pull Requests                   \u25b2\n       \u25bc                                      \u2502\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  terraform apply           \u2502\n\u2502 Terraform      \u2502 \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\u2502 (Infrastructure\u2502\n\u2502  as Code)      \u2502 \u2500\u2500\u25b6 VCN, Subnet, Rules\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518         \u2502\n                            \u25bc\n                       \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  ansible-playbook\n                       \u2502  OCI VM       \u2502 \u25c0\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 GitHub Runner\n                       \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                         \u2502  docker compose up\n                         \u25bc\n                  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n                  \u2502 Traefik Reverse    \u2502\n                  \u2502 Proxy (HTTPS)      \u2502\n                  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                         \u2502\n                         \u25bc\n                  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n                  \u2502 React Frontend     \u2502\n                  \u2502 Spring Boot API    \u2502\n                  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"blog/2025/06/18/the-devops-odyssey-fully-automating-oci-app-deployment-with-terraform-ansible-and-docker/#phase-1-terraformlaying-down-the-runway","title":"Phase\u00a01: Terraform\u2014Laying Down the Runway","text":""},{"location":"blog/2025/06/18/the-devops-odyssey-fully-automating-oci-app-deployment-with-terraform-ansible-and-docker/#module-design","title":"Module Design","text":"<p>For this project, I crafted a single, comprehensive Terraform module that provisions everything needed for the deployment. This module takes care of spinning up the Virtual Cloud Network (VCN), public subnet, Internet Gateway, security lists, and the free-tier Oracle Linux VM\u2014all in one go. By bundling the infrastructure logic together, setup becomes straightforward and easy to reuse for future experiments or side projects.</p> <pre><code>module \"web_server\" {\n  source                = \"./modules/web-server\"\n  compartment_ocid      = var.compartment_ocid\n  availability_domain   = var.availability_domain\n  shape                 = \"VM.Standard.A1.Flex\"\n  vm_ocpus              = 1\n  vm_memory             = 6\n  ssh_public_key        = file(\"~/.ssh/id_rsa.pub\")\n  # ...other variables...\n}\n</code></pre>"},{"location":"blog/2025/06/18/the-devops-odyssey-fully-automating-oci-app-deployment-with-terraform-ansible-and-docker/#remote-state-backend","title":"Remote State Back\u2011End","text":"<pre><code>terraform {\n  backend \"oci\" {\n    bucket    = \"your-state-bucket\"\n    namespace = \"your-namespace\"\n    region    = \"your-region\"\n    key       = \"terraform.tfstate\"\n  }\n}\n</code></pre> <p>Remote state unlocks <code>terraform plan</code> diffs inside GitHub\u00a0Actions so pull\u2011request reviewers can preview infra changes before merging.</p>"},{"location":"blog/2025/06/18/the-devops-odyssey-fully-automating-oci-app-deployment-with-terraform-ansible-and-docker/#phase-2-ansible-turning-vms-into-cattle","title":"Phase 2 \u2013 Ansible: Turning VMs into Cattle","text":""},{"location":"blog/2025/06/18/the-devops-odyssey-fully-automating-oci-app-deployment-with-terraform-ansible-and-docker/#cloud-init-the-initial-approach","title":"Cloud-Init: The Initial Approach","text":"<p>When I first attempted to automate post-provisioning on my OCI VM, I turned to Cloud-Init. It\u2019s a native mechanism that Oracle Cloud Infrastructure supports out of the box, letting you pass a shell script through the <code>user_data</code> field in Terraform:</p> <pre><code>resource \"oci_core_instance\" \"jobwinner\" {\n  # ... other config ...\n  metadata = {\n    ssh_authorized_keys = file(\"~/.ssh/id_rsa.pub\")\n    user_data           = base64encode(file(\"cloud-init/jobwinner-init.sh\"))\n  }\n}\n</code></pre> <p>This script (<code>jobwinner-init.sh</code>) aimed to bootstrap the whole stack\u2014installing Docker, creating the network, pulling images, writing configs, and launching containers. It was a decent first shot.</p> <p>But the problems were immediate:</p> <ul> <li>No visibility into when or if the script failed.</li> <li>Troubleshooting meant SSHing into the VM and running:   <pre><code>sudo tail -n 50 /var/log/cloud-init.log\n</code></pre></li> <li>Debugging Bash templates is like diffing spaghetti\u2014hard to maintain, fragile to extend.</li> </ul> <p>I needed a better approach. One with structure. One with feedback. One that was actually fun to use.</p>"},{"location":"blog/2025/06/18/the-devops-odyssey-fully-automating-oci-app-deployment-with-terraform-ansible-and-docker/#switching-to-ansible-structure-idempotence-sanity","title":"Switching to Ansible: Structure, Idempotence, Sanity","text":"<p>That\u2019s when I pivoted to Ansible\u2014and it completely changed the game.</p> <p>Instead of wrestling with cloud-init logs, I now have a clean split between infrastructure provisioning (Terraform) and configuration management (Ansible).</p> <p>The Ansible setup revolves around two focused roles:</p>"},{"location":"blog/2025/06/18/the-devops-odyssey-fully-automating-oci-app-deployment-with-terraform-ansible-and-docker/#1-common-system-setup","title":"1. <code>common</code> \u2013 System Setup","text":"<p>This role handles the core VM preparation:</p> <ul> <li>Updates all packages using <code>dnf</code></li> <li>Installs <code>epel-release</code>, <code>firewalld</code>, and <code>docker</code></li> <li>Starts and enables <code>firewalld</code></li> <li>Opens ports 80 and 443</li> </ul> <p>These steps are run through the <code>setup-docker.yml</code> playbook, making the VM Docker-ready with a secure firewall configuration.</p>"},{"location":"blog/2025/06/18/the-devops-odyssey-fully-automating-oci-app-deployment-with-terraform-ansible-and-docker/#2-deploy-jobwinner-application-stack-dns","title":"2. <code>deploy-jobwinner</code> \u2013 Application Stack &amp; DNS","text":"<p>Once the VM is ready, this role takes over. Here\u2019s what it does:</p> <ul> <li>Updates DuckDNS with the VM\u2019s current IP via an HTTP call</li> <li>Creates a secured <code>acme.json</code> file for Traefik to use with Let\u2019s Encrypt</li> <li>Templates out <code>traefik.yml</code> from a Jinja2 file</li> <li>Bootstraps Docker networks using a simple shell command</li> <li>Spins up Traefik and the app stack using <code>docker-compose</code></li> </ul> <p>All of this happens in a structured, readable, and repeatable manner. No surprises.</p>"},{"location":"blog/2025/06/18/the-devops-odyssey-fully-automating-oci-app-deployment-with-terraform-ansible-and-docker/#jinja2-templating-one-config-to-rule-them-all","title":"Jinja2 Templating: One Config to Rule Them All","text":"<p>Instead of hardcoded configs, everything lives as a template:</p> <pre><code># docker-compose.yml.j2\nversion: \"3.9\"\nservices:\n  frontend:\n    image: \"{{ frontend_image }}\"\n    labels:\n      - \"traefik.http.routers.frontend.rule=Host(`{{ traefik_host_domain }}`)\"\n</code></pre> <p>All variables are centralized in <code>group_vars/all.yml</code>, meaning I can swap images, update domains, or add ports with a single change. It's clean, version-controlled, and predictable.</p>"},{"location":"blog/2025/06/18/the-devops-odyssey-fully-automating-oci-app-deployment-with-terraform-ansible-and-docker/#reusable-compose-logic","title":"Reusable Compose Logic","text":"<p>One of my favorite touches is the shared <code>setup-compose-app.yml</code> task file. Both the app and Traefik use it. It performs three simple tasks:</p> <ol> <li>Ensures the target directory exists (<code>/opt/jobwinner</code>, <code>/opt/traefik</code>, etc.)</li> <li>Templates the appropriate <code>docker-compose.yml</code></li> <li>Runs <code>docker compose up -d</code> to launch the service</li> </ol> <p>This small abstraction keeps things DRY and easy to extend.</p>"},{"location":"blog/2025/06/18/the-devops-odyssey-fully-automating-oci-app-deployment-with-terraform-ansible-and-docker/#wait-for-readiness","title":"Wait for Readiness","text":"<p>Before deploying the app stack, Ansible waits for Traefik to be up using a simple health check:</p> <pre><code>- name: Wait for Traefik to be ready\n  uri:\n    url: http://localhost:80\n    status_code: 404\n    retries: 10\n    delay: 5\n    until: traefik_check is succeeded\n</code></pre> <p>This ensures we don\u2019t start routing app traffic before Traefik is ready to accept connections.</p>"},{"location":"blog/2025/06/18/the-devops-odyssey-fully-automating-oci-app-deployment-with-terraform-ansible-and-docker/#traefik-duckdns-and-httpsall-baked-in","title":"Traefik, DuckDNS, and HTTPS\u2014All Baked In","text":"<p>Once Traefik is live:</p> <ul> <li>It listens on port 80 and 443</li> <li>It pulls Let\u2019s Encrypt certs using DuckDNS DNS validation</li> <li>It binds the ACME data to a secured <code>acme.json</code> file</li> <li>It dynamically routes frontend and backend containers based on Docker labels</li> </ul> <p>And thanks to Ansible, all of this happens automatically\u2014no need to SSH into the machine, touch configs, or run manual cert renewals.</p> <p>Pro-Tip: Don't forget to bind-mount <code>acme.json</code> with <code>0600</code> permissions. Traefik won\u2019t start if it\u2019s too permissive.</p>"},{"location":"blog/2025/06/18/the-devops-odyssey-fully-automating-oci-app-deployment-with-terraform-ansible-and-docker/#phase-3-github-actions-cicd-reality-edition","title":"Phase\u00a03\u00a0\u2013 GitHub\u00a0Actions\u00a0CI/CD\u00a0(Reality Edition)","text":"<p>My pipeline is split into three purpose\u2011built workflows, each with a single job. This keeps logs short and lets me re\u2011run only the layer I need.</p> Workflow What it Does terraform.yml Provisions or updates all OCI resources\u2014VCN, subnet, free\u2011tier VM, and Object Storage state bucket setup-docker.yml Runs the common Ansible role to update packages, install <code>docker</code>, enable firewalld, and open ports\u00a080/443 deploy-jobwinner.yml Runs the deploy\u2011jobwinner role to refresh DuckDNS, start Traefik, wait for it, and launch the app stack"},{"location":"blog/2025/06/18/the-devops-odyssey-fully-automating-oci-app-deployment-with-terraform-ansible-and-docker/#workflow-1-provision-with-terraform","title":"Workflow 1: Provision with Terraform","text":"<p>Triggered on every push or PR to main, this workflow initializes Terraform, runs plan on PRs, and apply on merges. It handles OCI creds from secrets and writes the backend config for remote state.</p>"},{"location":"blog/2025/06/18/the-devops-odyssey-fully-automating-oci-app-deployment-with-terraform-ansible-and-docker/#workflow-2-make-the-vm-docker-ready","title":"Workflow 2: Make the VM Docker-Ready","text":"<p>Manually triggered once the VM is up. It SSHes into the instance and runs the common Ansible role:</p> <ul> <li>Updates packages</li> <li>Installs Docker</li> <li>Enables firewalld and opens ports 80/443</li> </ul> <p>This takes only couple minutes and gets the system production-ready.</p>"},{"location":"blog/2025/06/18/the-devops-odyssey-fully-automating-oci-app-deployment-with-terraform-ansible-and-docker/#workflow-3-deploy-job-winner","title":"Workflow 3: Deploy Job Winner","text":"<p>Also manually triggered, this one runs the deploy-jobwinner Ansible role:</p> <ul> <li>Updates DuckDNS with the current public IP</li> <li>Templates and boots Traefik</li> <li>Waits for Traefik to come online</li> <li>Starts the Job Winner frontend and backend using Docker Compose</li> </ul>"},{"location":"blog/2025/06/18/the-devops-odyssey-fully-automating-oci-app-deployment-with-terraform-ansible-and-docker/#why-this-split","title":"Why This Split?","text":"<p>Keeping infra, setup, and deploy separate gives me:</p> <p>\ud83d\udd04 Faster app iteration</p> <p>\ud83d\udee1\ufe0f Safer infra changes</p> <p>\ud83d\udccb Cleaner logs and simpler debugging</p> <p>Push code for infra, run two workflows for software, and your HTTPS app is live\u2014no hand-holding needed.</p>"},{"location":"blog/2025/06/18/the-devops-odyssey-fully-automating-oci-app-deployment-with-terraform-ansible-and-docker/#cost-breakdown","title":"Cost Breakdown","text":"Component Monthly Cost OCI VM (Free Tier) $0 Object Storage \u2264 20\u202fGB $0 DuckDNS Domain $0 Let's Encrypt Certs $0 Total $0.00"},{"location":"blog/2025/06/18/the-devops-odyssey-fully-automating-oci-app-deployment-with-terraform-ansible-and-docker/#lessons-learned-final-thoughts","title":"Lessons Learned &amp; Final Thoughts","text":"<ul> <li>Avoid using cloud-init for multi-step app bootstrapping\u2014debugging is painful.</li> <li>Ansible + templates + modular playbooks = power and clarity.</li> <li>Keep each component loosely coupled.</li> <li>Use retries and until in Ansible to ensure readiness (e.g., waiting for Traefik).</li> <li>Remote Terraform state on OCI Free Tier is fully achievable and highly recommended.</li> <li>Integrating GitHub Actions for CI/CD transforms deployment speed and reliability.</li> <li>Even free-tier cloud resources are enough for a clean, HTTPS-protected, full-stack app, deployed in minutes.</li> </ul>"},{"location":"blog/2025/06/18/the-devops-odyssey-fully-automating-oci-app-deployment-with-terraform-ansible-and-docker/#conclusion","title":"Conclusion","text":"<p>With a strategic mix of Terraform, Ansible, Docker Compose, and GitHub\u00a0Actions, it's possible to reach enterprise\u2011grade DevOps on a coffee\u2011budget.  </p>"},{"location":"blog/2025/02/26/setting-up-docker-ssl-and-duckdns-on-oracle-server/","title":"Setting Up Docker, SSL, and DuckDNS on Oracle Server","text":"<p>This guide will walk you through the process of setting up Docker, configuring SSL certificates, and setting up DuckDNS on an Oracle server (Oracle Linux 9).</p> <p></p>"},{"location":"blog/2025/02/26/setting-up-docker-ssl-and-duckdns-on-oracle-server/#1-update-and-install-docker","title":"1. Update and Install Docker","text":"<p>First, ensure your system is up-to-date and install Docker.</p> <pre><code>sudo dnf update -y\nsudo dnf install docker -y\n</code></pre> <p>Remove Podman and install docker</p> <pre><code>sudo dnf remove -y podman\nsudo dnf install -y dnf-plugins-core\nsudo dnf config-manager --add-repo https://download.docker.com/linux/centos/docker-ce.repo\nsudo dnf install -y docker-ce docker-ce-cli containerd.io\n</code></pre> <p>Enable Docker to start on boot and start the service.</p> <pre><code>sudo systemctl enable docker\nsudo systemctl start docker\n</code></pre> <p>Add Current User to Docker Group</p> <pre><code>sudo usermod -aG docker $USER\nnewgrp docker\n</code></pre> <p>Verify Docker Installation <pre><code>docker --version\n</code></pre></p>"},{"location":"blog/2025/02/26/setting-up-docker-ssl-and-duckdns-on-oracle-server/#2-configure-firewall-to-allow-httphttps-ports","title":"2. Configure Firewall to Allow HTTP/HTTPS Ports","text":"<p>If you\u2019re running a web server or using Docker containers exposed on HTTP/HTTPS, open the necessary ports.</p> <pre><code>sudo firewall-cmd --add-port=80/tcp --permanent\nsudo firewall-cmd --add-port=443/tcp --permanent\nsudo firewall-cmd --reload\n</code></pre>"},{"location":"blog/2025/02/26/setting-up-docker-ssl-and-duckdns-on-oracle-server/#3-set-up-ssl-certificates-using-certbot","title":"3. Set Up SSL Certificates Using Certbot","text":"<p>Install Certbot, which is a tool that helps you obtain SSL certificates for your domain.</p> <pre><code>sudo dnf install epel-release -y\nsudo dnf config-manager --set-enabled ol9_codeready_builder\nsudo dnf install python3 python3-pip -y\nsudo pip3 install certbot\n</code></pre> <p>Obtain SSL Certificate Run Certbot to obtain an SSL certificate for your domain.</p> <pre><code>sudo /usr/local/bin/certbot certonly --standalone -d jobwinner.duckdns.org\n</code></pre> <p>Add a cron job to automatically renew the SSL certificate every 90 days. <pre><code>(crontab -l ; echo \"0 3 * * * certbot renew --quiet\") | crontab -\n</code></pre></p>"},{"location":"blog/2025/02/26/setting-up-docker-ssl-and-duckdns-on-oracle-server/#4-optionally-generate-pkcs12-file-for-java-app","title":"4. Optionally generate PKCS12 file for Java App","text":"<pre><code>openssl pkcs12 -export -in your_certificate.crt -inkey your_private.key -out keystore.p12 -name selfsigned\n</code></pre> <p>It can also be automaed using certbot post hook with the following script (generate-p12.sh)</p> <pre><code>#!/bin/bash\n\n# Paths to the Let's Encrypt certificate and private key\nCERT_PATH=\"/etc/letsencrypt/live/jobwinner.duckdns.org/fullchain.pem\"\nKEY_PATH=\"/etc/letsencrypt/live/jobwinner.duckdns.org/privkey.pem\"\nOUTPUT_PATH=\"/etc/letsencrypt/live/jobwinner.duckdns.org/keystore.p12\"\n\n# Ensure the output directory exists\nmkdir -p $(dirname \"$OUTPUT_PATH\")\n\n# Convert the certificate and key to PKCS12 format\nopenssl pkcs12 -export -in \"$CERT_PATH\" -inkey \"$KEY_PATH\" -out \"$OUTPUT_PATH\" -name selfsigned -password pass:yourpassword\n\necho \"PKCS12 keystore created at $OUTPUT_PATH\"\n</code></pre> <pre><code>chmod +x /usr/local/bin/generate-p12.sh\nsudo certbot renew --deploy-hook \"/usr/local/bin/generate-p12.sh\"\n</code></pre>"},{"location":"blog/2025/02/26/setting-up-docker-ssl-and-duckdns-on-oracle-server/#5-set-up-duckdns-for-dynamic-dns","title":"5. Set Up DuckDNS for Dynamic DNS","text":"<p>If you are using DuckDNS for dynamic DNS (e.g., for a home server or a non-static IP), you will need to set up a script to update your IP regularly.</p> <p>Install DuckDNS Script Create a directory for DuckDNS and navigate to it.</p> <pre><code>mkdir -p ~/duckdns &amp;&amp; cd ~/duckdns\n</code></pre> <p>Create a script to update your DuckDNS record: <pre><code>vi duck.sh\necho url=\"https://www.duckdns.org/update?domains=yourname&amp;token=your-duckdns-token&amp;ip=\" | curl -k -o ~/duckdns/duck.log -K -\nchmod +x duck.sh\n(crontab -l ; echo \"*/5 * * * * ~/duckdns/duck.sh &gt;/dev/null 2&gt;&amp;1\") | crontab -\n</code></pre></p>"},{"location":"blog/2025/02/26/setting-up-docker-ssl-and-duckdns-on-oracle-server/#5-summary","title":"5. Summary","text":"<ul> <li>Docker is installed and configured.</li> <li>Ports 80 (HTTP) and 443 (HTTPS) are open for web traffic.</li> <li>SSL certificates are obtained via Certbot and set up to renew automatically.</li> <li>DuckDNS is set up to update your domain's IP address regularly.</li> </ul>"},{"location":"blog/2024/05/08/react-and-ruby-docker-development-environment/","title":"React and Ruby Docker Development Environment","text":""},{"location":"blog/2024/05/08/react-and-ruby-docker-development-environment/#background","title":"Background","text":"<p>Lately I am working on a project to dockerize an e2e application with React as the front end and Ruby on Rails as the backend. I would like to set up a docker-compose file so that:</p> <ol> <li>user can bring up the e2e environment with a single command</li> <li>the environment supports hot reload of code for both the React and Ruby on Rails parts</li> <li>it can be served as a testing environment for e2e test</li> <li>developers can run manual test in a local environment</li> </ol>"},{"location":"blog/2024/05/08/react-and-ruby-docker-development-environment/#react-dockerfile","title":"React Dockerfile","text":"<pre><code>FROM node:lts-alpine3.17\n\nWORKDIR /app\n\nCOPY package.json /app/\nCOPY yarn.lock /app/\nRUN yarn install\n\nCOPY . /app\n\nRUN yarn run build\nCMD yarn start\n</code></pre>"},{"location":"blog/2024/05/08/react-and-ruby-docker-development-environment/#docker-compose","title":"docker-compose","text":"<pre><code>version: '3'\nservices:\n  api:\n    build: ./backend\n    command: bundle exec rails server -p 3000 -b '0.0.0.0'\n    ports:\n      - \"3000:3000\"\n    volumes:\n      - \"./backend/:/rails\"\n  web:\n    build: ./frontend\n    environment:\n      REACT_APP_BACKEND_URL: \"http://localhost:3000\"\n      WATCHPACK_POLLING: \"true\"\n    command: npm start\n    ports:\n      - \"8080:3000\"\n    volumes:\n      - ./frontend/:/app\n    depends_on:\n      - api\n</code></pre> <p>note that the followings are essential to enable hot reload for both React and Rails:</p>"},{"location":"blog/2024/05/08/react-and-ruby-docker-development-environment/#react-hot-reload","title":"React Hot Reload","text":"<pre><code>  web:\n    ...\n    environment:\n      WATCHPACK_POLLING: \"true\"\n    command: npm start\n    ...\n    volumes:\n      - ./frontend/:/app\n</code></pre>"},{"location":"blog/2024/05/08/react-and-ruby-docker-development-environment/#ruby-on-rails-hot-reload","title":"Ruby on Rails Hot Reload","text":"<pre><code>  api:\n    ...\n    command: bundle exec rails server -p 3000 -b '0.0.0.0'\n    ...\n    volumes:\n      - \"./backend/:/rails\"\n</code></pre>"},{"location":"blog/2024/05/08/react-and-ruby-docker-development-environment/#connecting-front-end-to-back-end","title":"Connecting Front End to Back End","text":"<p>When the React application was trying to consume the Rails API, CORS error was reported. The following is the fix to make them communicate properly:</p>"},{"location":"blog/2024/05/08/react-and-ruby-docker-development-environment/#backendgemfile","title":"backend/Gemfile","text":"<pre><code>gem \"rack-cors\"\n</code></pre>"},{"location":"blog/2024/05/08/react-and-ruby-docker-development-environment/#backendconfiginitializerscorsrb","title":"backend/config/initializers/cors.rb","text":"<pre><code>Rails.application.config.middleware.insert_before 0, Rack::Cors do\n  allow do\n    origins 'localhost:3000',\n            '127.0.0.1:3000',\n            'localhost:8080',\n            '127.0.0.1:8080'\n\n    resource '*',\n             headers: :any,\n             methods: %i[get post put patch delete options head]\n  end\nend\nRails.application.config.hosts += ['localhost:3000',\n                                   'localhost',\n                                   '127.0.0.1:3000',\n                                   'localhost:8080',\n                                   '127.0.0.1:8080'\n                                ]\n</code></pre>"},{"location":"blog/2024/05/08/react-and-ruby-docker-development-environment/#sample-react-component-to-consume-ruby-on-rails-api","title":"Sample React Component to Consume Ruby on Rails API","text":"<pre><code>import React, { useState, useEffect }  from 'react';\nconst BackendTest = () =&gt; {\nconst [error, setError] = useState(null);\n    const [isLoaded, setIsLoaded] = useState(false);\n    const [foo, setFoo] = useState([]);\n    useEffect(() =&gt; {\n        fetch(process.env.REACT_APP_BACKEND_URL + \"/v0/health-check\")\n            .then(res =&gt; res.json())\n            .then(\n                (data) =&gt; {\n                    setIsLoaded(true);\n                    setFoo(data);\n                },\n                (error) =&gt; {\n                    setIsLoaded(true);\n                    setError(error);\n                }\n            )\n      }, [])\nif (error) {\n        return &lt;div&gt;Error: {error.message}&lt;/div&gt;;\n    } else if (!isLoaded) {\n        return &lt;div&gt;Loading...&lt;/div&gt;;\n    } else {\n        console.log(foo)\n        return (\n            &lt;div&gt;{foo.data}&lt;/div&gt;\n        );\n    }\n}\nexport default BackendTest;\n</code></pre>"},{"location":"blog/2024/06/03/using-redis-with-springboot/","title":"Using Redis with SpringBoot","text":"<p>On my other project, I use Redis as a caching layer. This time I would like to use it as a database in a SprintBoot project.</p>"},{"location":"blog/2024/06/03/using-redis-with-springboot/#bring-up-a-local-redis-with-docker","title":"Bring up a local Redis with docker","text":"<pre><code>docker run --name my-redis -p 6379:6379 -d redis\n</code></pre>"},{"location":"blog/2024/06/03/using-redis-with-springboot/#redis-client","title":"Redis Client","text":"<p>There are two popular clients - <code>Lettuce</code> and <code>Jedis</code>. I picked Jedis this time.</p> <pre><code>&lt;!-- pom.xml --&gt;\n&lt;dependency&gt;\n  &lt;groupId&gt;redis.clients&lt;/groupId&gt;\n  &lt;artifactId&gt;jedis&lt;/artifactId&gt;\n&lt;/dependency&gt;\n</code></pre> <p>Local development environment setup:</p> <pre><code># application.properties\n\nspring.redis.host=localhost\nspring.redis.port=6379\n</code></pre> <p>Sample configuration class</p> <pre><code>// configs/RedisConfig.java\n\nimport org.springframework.context.annotation.Bean;\nimport org.springframework.context.annotation.Configuration;\nimport org.springframework.data.redis.connection.RedisConnectionFactory;\nimport org.springframework.data.redis.connection.jedis.JedisConnectionFactory;\nimport org.springframework.data.redis.core.RedisTemplate;\nimport org.springframework.data.redis.serializer.GenericToStringSerializer;\n\n@Configuration\npublic class RedisConfig {\n\n    @Bean\n    public RedisConnectionFactory redisConnectionFactory() {\n        return new JedisConnectionFactory();\n    }\n\n    @Bean\n    public RedisTemplate&lt;String, Object&gt; redisTemplate() {\n        RedisTemplate&lt;String, Object&gt; template = new RedisTemplate&lt;&gt;();\n        template.setConnectionFactory(redisConnectionFactory());\n        template.setValueSerializer(new GenericToStringSerializer&lt;&gt;(Object.class));\n        return template;\n    }\n}\n</code></pre> <p>Sample model class</p> <pre><code>package com.tnite.redistest;\n\nimport lombok.Data;\nimport org.springframework.data.annotation.Id;\nimport org.springframework.data.redis.core.RedisHash;\n\nimport java.io.Serializable;\n\n@Data\n@RedisHash(\"Employee\")\npublic class Employee implements Serializable {\n\n    @Id\n    private Long id;\n    private String name;\n    private Double salary;\n    ...\n}\n</code></pre> <p>Everything was going smoothly until I realized that the ElastiCache instance is SSL-enabled and password-protected.</p>"},{"location":"blog/2024/06/03/using-redis-with-springboot/#make-it-works-with-elasticache","title":"Make it works with Elasticache","text":"<p>I want to retrieve all configuration parameters from environment variables. To ensure compatibility with the local development setup, the following code can handle configurations without a password and with SSL disabled.</p> <pre><code>@Slf4j\n@Configuration\npublic class RedisConfig {\n\n    @Value(\"${spring.redis.host:localhost}\")\n    private String host;\n\n    @Value(\"${spring.redis.port:6379}\")\n    private Integer port;\n\n    @Value(\"${spring.redis.password:@null}\")\n    private String password;\n\n    @Value(\"${spring.redis.ssl.enabled:false}\")\n    private Boolean useSsl;\n\n    @Bean\n    RedisConnectionFactory redisConnectionFactory() {\n\n        JedisConnectionFactory factory;\n        JedisClientConfiguration config = JedisClientConfiguration.builder().build();\n        if (useSsl) {\n            log.info(\"Redis with SSL enabled!\");\n            config = JedisClientConfiguration.builder().useSsl().build();\n        }\n\n        RedisStandaloneConfiguration redisConfig = new RedisStandaloneConfiguration(host, port);\n\n        if (password != null &amp;&amp; !password.isEmpty()) {\n            log.info(\"Redis with password is used!\");\n            redisConfig.setPassword(password);\n        }\n\n        factory = new JedisConnectionFactory(redisConfig, config);\n\n        return factory;\n    }\n\n}\n</code></pre>"},{"location":"blog/2024/11/30/lets-encrypt-ssl-certificate-with-certbot-for-godaddy/","title":"Let's Encrypt SSL Certificate with Certbot for GoDaddy","text":"<p>Securing your website with HTTPS is a critical step in establishing trust and improving SEO. This guide will show how to generate and install a free Let's Encrypt SSL certificate on a GoDaddy-hosted site using Certbot, and set up automatic certificate renewal.</p> <p></p> <p>This guide will walk you through the steps to generate and install a Let's Encrypt SSL certificate on your GoDaddy-hosted website, including automating certificate renewal.</p>"},{"location":"blog/2024/11/30/lets-encrypt-ssl-certificate-with-certbot-for-godaddy/#prerequisites","title":"Prerequisites","text":"<ul> <li>A GoDaddy account with VPS or Dedicated Hosting ( ustom SSL certificates can be installed).</li> <li>SSH access to your hosting server. (Optional)</li> <li>Certbot installed on your local machine or server.</li> </ul>"},{"location":"blog/2024/11/30/lets-encrypt-ssl-certificate-with-certbot-for-godaddy/#step-1-install-certbot","title":"Step 1: Install Certbot","text":"<p>To generate a Let's Encrypt SSL certificate, you'll need to install Certbot. Follow these steps based on your operating system.</p> <p>For Ubuntu/Debian-based systems:</p> <pre><code>sudo apt update\nsudo apt install certbot\n</code></pre> <p>For macOS (using Homebrew):</p> <pre><code>brew install certbot\n</code></pre>"},{"location":"blog/2024/11/30/lets-encrypt-ssl-certificate-with-certbot-for-godaddy/#step-2-generate-the-ssl-certificate","title":"Step 2: Generate the SSL Certificate","text":"<p>To generate the SSL certificate, use Certbot's DNS validation method. This is particularly useful when you're hosting with GoDaddy but running Certbot elsewhere.</p> <p>Run the following command, replacing yourdomain.com with your domain: <pre><code>certbot certonly --manual --preferred-challenges=dns -d yourdomain.com -d www.yourdomain.com\n</code></pre></p> <p>Certbot will prompt you to create a DNS TXT record to verify domain ownership.</p> <p>Add the DNS TXT record in GoDaddy:</p> <ol> <li>Log in to your GoDaddy account.</li> <li>Go to Domains &gt; DNS Settings.</li> <li>Add a new TXT record with the value provided by Certbot (it will look something like _acme-challenge.yourdomain.com).</li> <li>Wait for DNS propagation to complete (this can take a few minutes to a couple of hours).</li> <li>Confirm the certificate: After DNS propagation, confirm the certificate generation by following the instructions provided by Certbot.</li> </ol> <p>Certbot will generate the following files: - fullchain.pem (certificate) - privkey.pem (private key)</p>"},{"location":"blog/2024/11/30/lets-encrypt-ssl-certificate-with-certbot-for-godaddy/#step-3-install-the-certificate-on-godaddy","title":"Step 3: Install the Certificate on GoDaddy","text":"<ol> <li>Log in to your GoDaddy cPanel.</li> <li>Go to Security &gt; SSL/TLS.</li> <li>Under Manage SSL Sites, click Manage SSL Sites.</li> <li>Paste the content of:<ul> <li>fullchain.pem into the \"Certificate\" field.</li> <li>privkey.pem into the \"Private Key\" field.</li> </ul> </li> <li>Click Install Certificate to complete the process.</li> </ol>"},{"location":"blog/2024/11/30/lets-encrypt-ssl-certificate-with-certbot-for-godaddy/#step-4-automate-renewal-with-certbot","title":"Step 4: Automate Renewal with Certbot","text":"<p>Let\u2019s Encrypt certificates expire every 90 days, so it\u2019s important to set up automatic renewal.</p>"},{"location":"blog/2024/11/30/lets-encrypt-ssl-certificate-with-certbot-for-godaddy/#test-the-renewal-process","title":"Test the renewal process:","text":"<p>Run the following command to test renewal:</p> <pre><code>sudo certbot renew --dry-run\n</code></pre>"},{"location":"blog/2024/11/30/lets-encrypt-ssl-certificate-with-certbot-for-godaddy/#set-up-a-cron-job-for-automatic-renewal","title":"Set up a cron job for automatic renewal:","text":"<p>To automatically renew your certificates, create a cron job that runs periodically (e.g., every day at midnight): <pre><code>sudo crontab -e\n</code></pre> Add the following line to the cron file:</p> <p><pre><code>0 0 * * * certbot renew --quiet &amp;&amp; systemctl reload apache2  # For Apache\n</code></pre> OR <pre><code>0 0 * * * certbot renew --quiet &amp;&amp; systemctl reload nginx   # For Nginx\n</code></pre></p> <p>This ensures that certificates are checked and renewed daily, if necessary.</p>"},{"location":"blog/2024/11/30/lets-encrypt-ssl-certificate-with-certbot-for-godaddy/#step-5-verify-ssl-installation","title":"Step 5: Verify SSL Installation","text":"<p>Once the certificate is installed, use the SSL Labs Test to verify your SSL certificate and check for any issues.</p>"},{"location":"blog/2024/11/30/lets-encrypt-ssl-certificate-with-certbot-for-godaddy/#conclusion","title":"Conclusion","text":"<p>By following this guide, you've successfully secured your GoDaddy-hosted website with a free Let's Encrypt SSL certificate. With automated renewal in place, you can now enjoy secure and hassle-free HTTPS for your site.</p>"},{"location":"blog/2025/06/04/building-a-reusable-terraform-static-site-module-with-cloudfront-s3-and-route-53/","title":"Building a Reusable Terraform Static Site Module with CloudFront, S3, and Route 53","text":""},{"location":"blog/2025/06/04/building-a-reusable-terraform-static-site-module-with-cloudfront-s3-and-route-53/#overview","title":"Overview","text":"<p>A common need in modern cloud infrastructure is hosting static websites \u2014 whether it's marketing sites, documentation portals, or Single Page Applications (SPAs) built with React, Vue, or Svelte.</p> <p>At first, the AWS building blocks for this are fairly simple:</p> <ul> <li>S3 for object storage</li> <li>CloudFront for CDN</li> <li>ACM for HTTPS</li> <li>Route 53 for DNS</li> </ul> <p>But quickly, managing this setup by hand or duplicating configs across environments (prod, staging, QA) becomes painful:</p> <ul> <li>Too many copy/paste Terraform files</li> <li>Hard to apply consistent policies</li> <li>Complicated to manage uploads (especially when some sites are CI/CD and some are manual content sites)</li> </ul> <p></p>"},{"location":"blog/2025/06/04/building-a-reusable-terraform-static-site-module-with-cloudfront-s3-and-route-53/#why-a-reusable-module","title":"Why a Reusable Module?","text":"<p>I wanted a simple, composable way to manage:</p> <ul> <li>Multiple static sites across environments</li> <li>Both \"content\" sites (manual file uploads)</li> <li>And React apps (deployed by CI/CD)</li> <li>With a consistent CloudFront + ACM + Route 53 setup</li> <li>Using Terraform modules to avoid duplication</li> </ul>"},{"location":"blog/2025/06/04/building-a-reusable-terraform-static-site-module-with-cloudfront-s3-and-route-53/#architecture","title":"Architecture","text":"<p>The module will:</p> <ol> <li>Create an S3 bucket</li> <li>(Optional) Enable versioning</li> <li>Create a CloudFront distribution with Origin Access Identity (OAI)</li> <li>Request an ACM certificate (DNS validated via Route 53)</li> <li>Create an A/ALIAS record in Route 53 for the domain</li> <li>(Optional) Upload local files using Terraform</li> </ol>"},{"location":"blog/2025/06/04/building-a-reusable-terraform-static-site-module-with-cloudfront-s3-and-route-53/#module-main-code-modulesstatic-sitemaintf","title":"Module Main Code (<code>modules/static-site/main.tf</code>)","text":"<pre><code>resource \"aws_s3_bucket\" \"bucket\" {\n  bucket        = var.bucket_name\n  force_destroy = true\n  tags = {\n    Name        = var.bucket_name_tag\n    Environment = var.environment\n  }\n}\n\nresource \"aws_s3_bucket_versioning\" \"bucket_versioning\" {\n  count  = var.enable_versioning ? 1 : 0\n  bucket = aws_s3_bucket.bucket.id\n\n  versioning_configuration {\n    status = \"Enabled\"\n  }\n}\n\nresource \"aws_cloudfront_origin_access_identity\" \"oai\" {\n  comment = \"OAI for ${var.domain_name}\"\n}\n\nresource \"aws_s3_bucket_policy\" \"allow_cf\" {\n  bucket = aws_s3_bucket.bucket.id\n\n  policy = jsonencode({\n    Version = \"2012-10-17\"\n    Statement = [\n      {\n        Effect = \"Allow\"\n        Principal = {\n          AWS = aws_cloudfront_origin_access_identity.oai.iam_arn\n        }\n        Action   = \"s3:GetObject\"\n        Resource = \"${aws_s3_bucket.bucket.arn}/*\"\n      }\n    ]\n  })\n}\n\nresource \"aws_acm_certificate\" \"cert\" {\n  domain_name       = var.domain_name\n  validation_method = \"DNS\"\n\n  lifecycle {\n    create_before_destroy = true\n  }\n}\n\nresource \"aws_route53_record\" \"cert_validation\" {\n  for_each = {\n    for dvo in aws_acm_certificate.cert.domain_validation_options : dvo.domain_name =&gt; {\n      name   = dvo.resource_record_name\n      type   = dvo.resource_record_type\n      record = dvo.resource_record_value\n    }\n  }\n\n  zone_id = var.route53_zone_id\n  name    = each.value.name\n  type    = each.value.type\n  ttl     = 300\n  records = [each.value.record]\n}\n\nresource \"aws_acm_certificate_validation\" \"validate_cert\" {\n  certificate_arn         = aws_acm_certificate.cert.arn\n  validation_record_fqdns = [for record in aws_route53_record.cert_validation : record.fqdn]\n}\n\nresource \"aws_cloudfront_distribution\" \"cdn\" {\n  depends_on = [aws_acm_certificate_validation.validate_cert]\n\n  origin {\n    domain_name = aws_s3_bucket.bucket.bucket_regional_domain_name\n    origin_id   = \"s3-origin\"\n\n    s3_origin_config {\n      origin_access_identity = aws_cloudfront_origin_access_identity.oai.cloudfront_access_identity_path\n    }\n  }\n\n  enabled             = true\n  default_root_object = \"index.html\"\n\n  default_cache_behavior {\n    allowed_methods  = [\"GET\", \"HEAD\"]\n    cached_methods   = [\"GET\", \"HEAD\"]\n    target_origin_id = \"s3-origin\"\n\n    forwarded_values {\n      query_string = false\n      cookies {\n        forward = \"none\"\n      }\n    }\n\n    viewer_protocol_policy = \"redirect-to-https\"\n  }\n\n  viewer_certificate {\n    acm_certificate_arn      = aws_acm_certificate_validation.validate_cert.certificate_arn\n    ssl_support_method       = \"sni-only\"\n    minimum_protocol_version = \"TLSv1.2_2021\"\n  }\n\n  aliases = [var.domain_name]\n\n  restrictions {\n    geo_restriction {\n      restriction_type = \"none\"\n    }\n  }\n\n  custom_error_response {\n    error_code         = 404\n    response_code      = 200\n    response_page_path = \"/index.html\"\n  }\n}\n\nresource \"aws_route53_record\" \"cf_alias\" {\n  zone_id = var.route53_zone_id\n  name    = var.domain_name\n  type    = \"A\"\n\n  alias {\n    name                   = aws_cloudfront_distribution.cdn.domain_name\n    zone_id                = aws_cloudfront_distribution.cdn.hosted_zone_id\n    evaluate_target_health = false\n  }\n}\n\n# -------------------- Upload logic (optional) --------------------\n\nlocals {\n  upload_path  = var.upload_path != null ? var.upload_path : \"${path.root}/upload/${var.bucket_name}\"\n  upload_files = var.enable_uploads ? fileset(local.upload_path, \"*\") : []\n}\n\nresource \"aws_s3_object\" \"uploads\" {\n  for_each     = { for f in local.upload_files : f =&gt; f }\n  bucket       = aws_s3_bucket.bucket.id\n  key          = each.key\n  source       = \"${var.upload_path}/${each.key}\"\n  source_hash  = filemd5(\"${var.upload_path}/${each.key}\")\n  content_type = lookup(var.mime_types, regex(\"[^.]+$\", each.key), \"application/octet-stream\")\n}\n</code></pre>"},{"location":"blog/2025/06/04/building-a-reusable-terraform-static-site-module-with-cloudfront-s3-and-route-53/#module-outputs-modulesstatic-siteoutputstf","title":"Module Outputs (<code>modules/static-site/outputs.tf</code>)","text":"<pre><code>output \"cloudfront_domain_name\" {\n  value = aws_cloudfront_distribution.cdn.domain_name\n}\n\noutput \"s3_bucket_name\" {\n  value = aws_s3_bucket.bucket.bucket\n}\n\noutput \"public_file_urls\" {\n  value = can(aws_s3_object.uploads) ? {\n    for f, obj in aws_s3_object.uploads :\n    f =&gt; \"https://${var.domain_name}/${obj.key}\"\n  } : {}\n}\n</code></pre>"},{"location":"blog/2025/06/04/building-a-reusable-terraform-static-site-module-with-cloudfront-s3-and-route-53/#module-variables","title":"Module Variables","text":"<pre><code>variable \"bucket_name\" {}\nvariable \"bucket_name_tag\" {}\nvariable \"environment\" {}\nvariable \"domain_name\" {}\nvariable \"route53_zone_id\" {}\nvariable \"enable_versioning\" { default = false }\nvariable \"enable_uploads\" { default = false }\nvariable \"upload_path\" { default = null }\nvariable \"mime_types\" {\n  description = \"Map of file extensions to MIME types\"\n  type        = map(string)\n  default = {\n    json = \"application/json\"\n    txt  = \"text/plain\"\n    jpg  = \"image/jpeg\"\n    jpeg = \"image/jpeg\"\n    png  = \"image/png\"\n    pdf  = \"application/pdf\"\n  }\n}\n</code></pre>"},{"location":"blog/2025/06/04/building-a-reusable-terraform-static-site-module-with-cloudfront-s3-and-route-53/#example-usage-static-sitestf-in-root","title":"Example Usage (<code>static-sites.tf</code> in root)","text":""},{"location":"blog/2025/06/04/building-a-reusable-terraform-static-site-module-with-cloudfront-s3-and-route-53/#example-1-static-file-site-uploads-enabled","title":"Example 1 - Static File Site (uploads enabled)","text":"<pre><code>module \"files_site\" {\n  source            = \"./modules/static-site\"\n  bucket_name       = \"files-site\"\n  bucket_name_tag   = \"files-site\"\n  environment       = \"prod\"\n  domain_name       = \"files.example.com\"\n  route53_zone_id   = data.aws_route53_zone.example_com.zone_id\n  enable_versioning = true\n  enable_uploads    = true\n  upload_path       = \"${path.root}/upload/files_site\"\n}\n</code></pre>"},{"location":"blog/2025/06/04/building-a-reusable-terraform-static-site-module-with-cloudfront-s3-and-route-53/#example-2-react-app-no-uploads-no-versioning","title":"Example 2 - React App (no uploads, no versioning)","text":"<pre><code>module \"dev_web\" {\n  source            = \"./modules/static-site\"\n  bucket_name       = \"dev-web\"\n  bucket_name_tag   = \"dev-web\"\n  environment       = \"dev\"\n  domain_name       = \"dev.example.com\"\n  route53_zone_id   = data.aws_route53_zone.example_com.zone_id\n  enable_versioning = false\n  enable_uploads    = false\n}\n</code></pre>"},{"location":"blog/2025/06/04/building-a-reusable-terraform-static-site-module-with-cloudfront-s3-and-route-53/#upload_path-default-behavior","title":"<code>upload_path</code> default behavior","text":"<p>If <code>upload_path</code> is not specified, it defaults to:</p> <pre><code>locals {\n  upload_path = var.upload_path != null ? var.upload_path : \"${path.root}/upload/${var.bucket_name}\"\n}\n</code></pre>"},{"location":"blog/2025/06/04/building-a-reusable-terraform-static-site-module-with-cloudfront-s3-and-route-53/#directory-structure","title":"Directory Structure","text":"<pre><code>project-root/\n\u251c\u2500\u2500 main.tf\n\u251c\u2500\u2500 variables.tf\n\u251c\u2500\u2500 outputs.tf\n\u251c\u2500\u2500 terraform.tfvars\n\u251c\u2500\u2500 upload/\n\u2502   \u2514\u2500\u2500 files_site/\n\u2514\u2500\u2500 modules/\n    \u2514\u2500\u2500 static-site/\n        \u251c\u2500\u2500 main.tf\n        \u251c\u2500\u2500 variables.tf\n        \u251c\u2500\u2500 outputs.tf\n        \u2514\u2500\u2500 README.md\n</code></pre>"},{"location":"blog/2025/06/04/building-a-reusable-terraform-static-site-module-with-cloudfront-s3-and-route-53/#uploading-or-replacing-files","title":"Uploading or Replacing Files","text":"<p>For modules with <code>enable_uploads = true</code>, files will be uploaded from:</p> <pre><code>upload/&lt;bucket_name&gt;/\n</code></pre> <p>Or from:</p> <pre><code>upload/&lt;custom path&gt;\n</code></pre> <p>Terraform will handle uploading new files, updating changed files, or deleting removed files.</p>"},{"location":"blog/2025/06/04/building-a-reusable-terraform-static-site-module-with-cloudfront-s3-and-route-53/#url-for-uploaded-files","title":"URL for Uploaded Files","text":"<p>For static resource Site (uploads enabled), eg. with domain_name <code>files.example.com</code></p> <p>The files will be served at:</p> <pre><code>https://files.example.com/&lt;filename&gt;\n</code></pre>"},{"location":"blog/2025/06/04/building-a-reusable-terraform-static-site-module-with-cloudfront-s3-and-route-53/#conclusion","title":"Conclusion","text":"<p>This reusable module pattern has been a very helpful addition to my Terraform workflows:</p> <ul> <li>I can spin up new static sites in minutes</li> <li>I can mix manual content sites and CI/CD React apps seamlessly</li> <li>Everything stays consistent across environments</li> <li>It is safe, extensible, and easy to maintain</li> </ul> <p>If you need to manage multiple static sites with Terraform, I highly recommend building or adopting a module like this:</p> <p>It keeps your configuration clean, avoids \"copy/paste debt,\" and makes it easy to scale as your team or product grows.</p>"},{"location":"blog/2025/06/04/building-a-reusable-terraform-static-site-module-with-cloudfront-s3-and-route-53/#future-improvements","title":"Future Improvements","text":"<p>Some future ideas to improve this module:</p> <ul> <li>Add CloudFront logging</li> <li>Add S3 lifecycle policies</li> </ul>"},{"location":"blog/2025/09/06/extending-our-tailscale-setup-with-a-terraform-managed-bastion/","title":"Extending Our Tailscale Setup with a Terraform-Managed Bastion","text":""},{"location":"blog/2025/09/06/extending-our-tailscale-setup-with-a-terraform-managed-bastion/#originally-posted-at-linkedin-at-sept-06-2025","title":"originally posted at LinkedIn at Sept 06, 2025","text":"<p>In my previous post, I wrote about how we replaced a traditional VPN with Tailscale to connect engineers to Kubernetes services. That solved a big piece of the puzzle: cluster access was now simple, secure, and reliable.  </p> <p>But as always, not everything lives in Kubernetes. We still had private databases, legacy services, and tools running in our VPC that engineers needed to reach. That\u2019s where a bastion came in.</p> <p></p>"},{"location":"blog/2025/09/06/extending-our-tailscale-setup-with-a-terraform-managed-bastion/#why-not-run-the-bastion-in-eks","title":"Why Not Run the Bastion in EKS?","text":"<p>The first idea was straightforward: just run the bastion inside the Kubernetes cluster. We already had the Tailscale Kubernetes controller working, so why not extend it?  </p> <p>The catch is resilience. If the cluster itself is unavailable, the bastion inside it disappears too. That leaves no clear way back in when you need it most.  </p> <p>So we decided to place the bastion outside the cluster, where it could act as an independent entry point.  </p>"},{"location":"blog/2025/09/06/extending-our-tailscale-setup-with-a-terraform-managed-bastion/#a-simple-ec2-router","title":"A Simple EC2 Router","text":"<p>The design is uncomplicated:  </p> <ol> <li>Provision a small EC2 instance.  </li> <li>Install Tailscale.  </li> <li>Enable subnet routing with <code>--advertise-routes</code> so it can expose private ranges from the VPC.  </li> <li>Approve the advertised routes in the Tailscale admin console (or via Terraform).  </li> </ol> <p>With that, the Tailnet extends beyond Kubernetes and into the rest of our infrastructure. The bastion becomes a consistent path to reach internal resources, no matter where they live.  </p>"},{"location":"blog/2025/09/06/extending-our-tailscale-setup-with-a-terraform-managed-bastion/#automating-with-terraform","title":"Automating with Terraform","text":"<p>We didn\u2019t want to maintain this manually. The entire setup \u2014 instance, networking, routing \u2014 is described in Terraform. A minimal example looks like this:  </p> <pre><code>resource \"aws_instance\" \"bastion\" {\n  ami           = data.aws_ami.ubuntu.id\n  instance_type = \"t3.micro\" # Free tier eligible\n  subnet_id     = aws_subnet.private.id\n  vpc_security_group_ids = [aws_security_group.bastion.id]\n\n  user_data = &lt;&lt;-EOT\n    #!/bin/bash\n    curl -fsSL https://tailscale.com/install.sh | sh\n    tailscale up \\\\\n      --authkey=${var.tailscale_auth_key} \\\\\n      --advertise-routes=10.0.0.0/16 \\\\\n      --ssh\n  EOT\n\n  tags = {\n    Name = \"tailscale-bastion\"\n  }\n}\n</code></pre> <p>If it ever needs to be replaced, we can rebuild it from code. No one is SSHing in to \"fix\" things by hand.  </p>"},{"location":"blog/2025/09/06/extending-our-tailscale-setup-with-a-terraform-managed-bastion/#scaling-and-redundancy","title":"Scaling and Redundancy","text":"<p>One of the pleasant surprises was how easy it is to scale this setup:  </p> <ul> <li>Horizontal scaling: We can deploy multiple bastion instances across availability zones, each advertising the same routes. Tailscale handles the routing logic automatically, so clients don\u2019t need to care which router they connect to.  </li> <li>Failover support: If one EC2 router goes down, traffic shifts to another. No manual intervention is required.  </li> <li>Flexibility: If we need more throughput, we can scale up the instance size or add more routers. Terraform makes this a one-line change.  </li> </ul> <p>This setup gives us a clear and resilient path back into our infrastructure, independent of Kubernetes.  </p>"},{"location":"blog/2025/09/06/extending-our-tailscale-setup-with-a-terraform-managed-bastion/#the-architecture","title":"The Architecture","text":"<p>Here\u2019s a simplified view of how it all fits together:</p> <pre><code>+-------------------+          +---------------------+\n|   Developers      |          |   Tailscale Admin   |\n| (Tailscale client)|---------&gt;|   Coordination      |\n+-------------------+          +---------------------+\n              |                        \n              | Tailnet connection\n              v\n+-----------------------------------+\n|     EC2 Bastion (Tailscale)       |\n|   --advertise-routes=10.0.0.0/16  |\n+-----------------------------------+\n              |\n              v\n   +-------------------+      +-------------------+\n   |    VPC Subnets    |-----&gt;|  Kubernetes (EKS) |\n   | (private ranges)  |      +-------------------+\n   |                   |-----&gt;|  Databases        |\n   |                   |-----&gt;|  Legacy Services  |\n   +-------------------+.     +-------------------+\n</code></pre>"},{"location":"blog/2025/09/06/extending-our-tailscale-setup-with-a-terraform-managed-bastion/#what-we-gained","title":"What We Gained","text":"<ul> <li>A single, consistent way to reach Kubernetes and non-Kubernetes systems.  </li> <li>Independence from the cluster itself \u2014 the bastion remains available even if EKS has issues.  </li> <li>Everything defined in Terraform, so we can recreate or scale it quickly.  </li> <li>Options to add redundancy without much overhead.  </li> </ul>"},{"location":"blog/2025/09/06/extending-our-tailscale-setup-with-a-terraform-managed-bastion/#closing-thoughts","title":"Closing Thoughts","text":"<p>Over the course of this series, we went from relying on a traditional VPN, to adopting Tailscale for Kubernetes access, and finally to extending that same approach with a Terraform-managed bastion. What we have now is a network model that:</p> <ul> <li>Works across both Kubernetes and non-Kubernetes resources.</li> <li>Gives us a reliable way back into our infrastructure, even when the cluster itself is unhealthy.</li> <li>Is fully expressed in code, so we can reproduce or extend it without guesswork.</li> <li>Scales naturally when we need to add capacity or redundancy.</li> </ul> <p>The outcome is not just a technical improvement \u2014 it\u2019s a shift in how we think about connectivity. Instead of designing networks around hardware appliances and static tunnels, we treat secure access as something we can declare in code, roll out quickly, and evolve as our infrastructure changes.</p> <p>That foundation will serve us well as we continue to grow, and it\u2019s one less operational burden we need to carry.</p>"},{"location":"blog/2025/06/25/swapping-vpn-for-tailscale-a-five-day-internal-infra-upgrade/","title":"Swapping VPN for Tailscale: A Five-Day Internal Infra Upgrade","text":""},{"location":"blog/2025/06/25/swapping-vpn-for-tailscale-a-five-day-internal-infra-upgrade/#originally-posted-at-linkedin-at-june-25-2025","title":"originally posted at LinkedIn at June 25, 2025","text":"<p>We recently started migrating away from our traditional VPN setup\u2014and toward something simpler, faster, and cheaper: Tailscale.</p> <p>This wasn\u2019t a full rip-and-replace. In just five days, we moved a core set of internal Kubernetes services behind Tailscale, enough to start retiring our legacy VPN setup piece by piece.</p> <p>The results? \u2705 Smoother developer workflows \u2705 Better access control \u2705 Significant cost savings \u2705 Self-serve onboarding \u2705 Fewer support headaches</p> <p></p>"},{"location":"blog/2025/06/25/swapping-vpn-for-tailscale-a-five-day-internal-infra-upgrade/#why-we-looked-beyond-our-vpn","title":"Why We Looked Beyond Our VPN","text":"<p>Our old VPN setup worked, but it came with pain:</p> <ul> <li>Unreliable onboarding: Getting new devs access to internal tools meant following tribal checklists.</li> <li>Flaky performance: DNS routing issues and random client disconnects were routine.</li> <li>Scaling cost: Per-user pricing added up quickly as the team grew.</li> </ul> <p>We needed a way to: - Securely expose internal services - Manage access with identity, not IPs - Spend less time maintaining tunnels</p> <p>Tailscale had already been a favorite among engineers for peer-to-peer dev work. With their Kubernetes integration, we realized it could be the backbone of our internal network.</p>"},{"location":"blog/2025/06/25/swapping-vpn-for-tailscale-a-five-day-internal-infra-upgrade/#the-tailscale-kubernetes-operator","title":"The Tailscale Kubernetes Operator","text":"<p>We used the Tailscale Kubernetes Operator to expose services inside our EKS cluster directly onto our Tailscale network.</p> <p>That meant: - No public ingress - No legacy VPN tunnels - Just a Tailscale IP and ACL-based access control</p> <p>To make it work, we asked Tailscale to enable a beta feature. They did so the same day\u2014unblocking us instantly.</p> <p>\ud83d\udcac \u201cTailscale\u2019s support was fast, helpful, and low-friction. That made a huge difference in moving quickly.\u201d</p>"},{"location":"blog/2025/06/25/swapping-vpn-for-tailscale-a-five-day-internal-infra-upgrade/#what-we-did-in-5-days","title":"What We Did in 5 Days","text":"<p>This wasn\u2019t a full migration\u2014but it was a meaningful one.</p>"},{"location":"blog/2025/06/25/swapping-vpn-for-tailscale-a-five-day-internal-infra-upgrade/#day-1-planning-and-access","title":"\u2705 Day 1: Planning and Access","text":"<ul> <li>Audited internal services for migration</li> <li>Requested access to the necessary beta feature</li> <li>Set up a test namespace in EKS</li> </ul>"},{"location":"blog/2025/06/25/swapping-vpn-for-tailscale-a-five-day-internal-infra-upgrade/#day-2-deploy-the-operator","title":"\u2705 Day 2: Deploy the Operator","text":"<ul> <li>Installed the Tailscale operator in Kubernetes</li> <li>Verified access to a test service via tailnet</li> </ul>"},{"location":"blog/2025/06/25/swapping-vpn-for-tailscale-a-five-day-internal-infra-upgrade/#days-34-migrate-key-services","title":"\u2705 Days 3\u20134: Migrate Key Services","text":"<ul> <li>Moved internal dashboards and dev/test APIs</li> <li>Replaced ingress rules with tailnet-only access</li> </ul>"},{"location":"blog/2025/06/25/swapping-vpn-for-tailscale-a-five-day-internal-infra-upgrade/#day-5-move-golink-off-a-laptop","title":"\u2705 Day 5: Move GoLink Off a Laptop","text":"<p>Our internal GoLink (URL shortener) was still running on a MacBook. We containerized it, deployed it to EKS, and exposed it via Tailscale.</p> <p>It\u2019s now: - Always available - Protected by SSO and ACLs - No longer a \u201csingle MacBook of failure\u201d</p>"},{"location":"blog/2025/06/25/swapping-vpn-for-tailscale-a-five-day-internal-infra-upgrade/#cost-comparison-traditional-vpn-vs-tailscale","title":"\ud83d\udcc9 Cost Comparison: Traditional VPN vs. Tailscale","text":"<p>Here\u2019s how costs shake out for teams of 10 to 50 users:</p> Team Size Traditional VPN ($10\u201315 PMPM) Tailscale ($6 PMPM) Monthly Savings 10 users $100\u2013150 $60 $40\u201390 25 users $250\u2013375 $150 $100\u2013225 50 users $500\u2013750 $300 $200\u2013450 <p>With Tailscale, pricing is flat, predictable, and scales cleanly with the team.</p>"},{"location":"blog/2025/06/25/swapping-vpn-for-tailscale-a-five-day-internal-infra-upgrade/#what-we-gained-so-far","title":"What We Gained So Far","text":"<p>Even with a partial rollout, the impact was immediate:</p>"},{"location":"blog/2025/06/25/swapping-vpn-for-tailscale-a-five-day-internal-infra-upgrade/#effortless-onboarding","title":"\ud83e\udde0 Effortless Onboarding","text":"<p>New teammates just sign in with SSO and install the Tailscale app\u2014on desktop or mobile. No setup guides. No support pings. No friction.</p> <p>One team member called it \u201cthe smoothest internal access experience I\u2019ve had.\u201d</p>"},{"location":"blog/2025/06/25/swapping-vpn-for-tailscale-a-five-day-internal-infra-upgrade/#better-access-control","title":"\ud83d\udd10 Better Access Control","text":"<p>We make use of ACL tagging right in the YAML config, and manage everything from one clean web UI. It's simple, auditable, and role-aware\u2014without any subnet headaches.</p>"},{"location":"blog/2025/06/25/swapping-vpn-for-tailscale-a-five-day-internal-infra-upgrade/#developer-happiness","title":"\u26a1 Developer Happiness","text":"<p>Internal tools are instantly reachable from anywhere via tailnet IPs. No more wrestling with VPN clients or wondering why DNS isn\u2019t resolving.</p>"},{"location":"blog/2025/06/25/swapping-vpn-for-tailscale-a-five-day-internal-infra-upgrade/#real-cost-savings","title":"\ud83d\udcb8 Real Cost Savings","text":"<p>We\u2019re already spending less than before\u2014and the savings will grow as we migrate more.</p>"},{"location":"blog/2025/06/25/swapping-vpn-for-tailscale-a-five-day-internal-infra-upgrade/#whats-next","title":"What\u2019s Next","text":"<p>This rollout laid the groundwork for more:</p> <ul> <li>Expanding Tailscale coverage to more services  </li> <li>Introducing exit nodes for outbound traffic  </li> <li>Removing our legacy VPN entirely  </li> <li>Testing Tailscale Funnel for preview environments</li> </ul>"},{"location":"blog/2025/06/25/swapping-vpn-for-tailscale-a-five-day-internal-infra-upgrade/#final-thoughts","title":"Final Thoughts","text":"<p>We didn\u2019t replace everything in five days\u2014but we replaced enough to see a clear difference.</p> <p>With Tailscale, we simplified onboarding, secured access by identity, improved our internal network\u2019s resilience, and lowered our costs. All without writing custom infra or managing new VPN servers.</p> <p>If you're dealing with a brittle VPN setup or want something simpler, Tailscale\u2019s worth exploring. Start small\u2014see what happens.</p>"},{"location":"blog/2024/05/31/test-redis-with-generated-data/","title":"Test Redis with Generated Data","text":"<p>Recently I need to test out Redis integration with a Ruby on Rails application. I wanted to test how fast Redis is with 1GB of data. </p> <p>Instead of manually generating dummy data, I recalled using the Faker library a couple of years ago during my time as a full-stack developer.</p>"},{"location":"blog/2024/05/31/test-redis-with-generated-data/#what-is-faker","title":"What is Faker?","text":"<p>Faker is a library that generates fake data such as names, addresses, emails, and phone numbers. It\u2019s a great tool to create realistic data without the need for manually making sample datasets.</p>"},{"location":"blog/2024/05/31/test-redis-with-generated-data/#installation","title":"Installation","text":"<p>Faker supports different programming languages. I chose Python this time.</p> <pre><code>pip install faker\n</code></pre>"},{"location":"blog/2024/05/31/test-redis-with-generated-data/#code-to-generate-1gb-of-simple-key-value-pair-data","title":"Code to generate 1GB of simple key value pair data","text":"<pre><code>from fake import Faker\nimport csv\n\nfake = Faker()\n\ndef get_text():\n  return ''.join(fake.text(1000)).replace('\\r', '').replace('\\n', '')\n\ndef get_key_and_text_row():\n  return [n, get_text()]\n\nwith open('large.csv', 'w') as csvfile:\n  write = csv.writer(csvfile)\n  for n in range(1, 1000000):\n    writer.writerow(get_key_and_text_row(n))\n</code></pre>"},{"location":"blog/2024/05/31/test-redis-with-generated-data/#sample-output","title":"Sample output","text":"<pre><code>1,Interest local deep college particularly and what. Movie bit task matter likely tend.Quality sometimes Democrat join weight minute do. Company event indicate director great pay specific. Continue bill all collection.Help training enter everybody amount. Middle western administration page heart leader similar. Threat once up authority voice give.Customer safe write people. Employee sell throw as so strategy.Drug really control call offer. Effect west service this under program ask contain.Although able region new wear suggest glass. Risk station project including.Because kitchen light enough. Reflect require name democratic wrong without. Begin three fire baby develop federal.Source husband turn attack. Leader win floor young. Member change peace population account high cover. Add but stay crime himself.Sell stand time sure cut decade system.Open fire hair.Leg six event ability other. Nature agree serve affect fund act. Long star improve spring individual.\n2,Station citizen have piece plant. Look science realize stuff television.Single civil poor television actually must. Parent song capital.Too have grow red. Discussion its record you like impact paper.Actually total those main range town. Bit myself future no institution. Suggest unit own at rock personal.Shoulder always today understand especially ball push. Stay appear sort century pull central seven. Site trip memory his start.Member instead option side if result west. Address audience strategy say receive.Candidate later under item yeah will business. Player dog myself purpose.Look trip production water power whether wait rich. Keep sell low if. Quality Republican action main. Sit all see watch same.Lot continue usually wish size. Than morning common hear particular true. Fish minute about.Firm party prepare shoulder main determine health seat. White remain hold card establish interview.Over thousand job minute various adult. Western bill some. Still animal measure.\n3,Church behavior investment protect current every fast reach. Including employee reflect material.Center member relate than war up fear. According discuss pay camera activity home fill soldier.Use table view these pattern popular.Song choose street fish must small. Firm beyond across room other.Ago find traditional listen. Collection learn tell agent newspaper way. Maybe mouth executive country. After future region theory with certainly of.Carry eight spend up.Believe teacher score avoid article. Civil movie eight song across task raise.Plant take painting song. Federal people character animal. Experience fill may.Until someone tree newspaper ball. Rest position spring PM claim. Various animal end parent pick hand great. Help tell hit modern artist involve begin final.Plant certainly father heavy reflect anyone. Parent town make against speak. Agency cover film week plant.Board hope let respond majority sister.\n4,Capital behind tree note whose. Loss condition program. Rest ball including game.View recent matter science red reason choice necessary. Magazine style defense guy special on special. Sell wait per under. Four skin fear yard rather yes responsibility director.Job none player serve service action. Teach phone wait country audience. Especially can source practice. Ground Republican student remember.Grow throughout research understand. Marriage head arm. Share current father successful action future politics.Environment group include executive news model poor worker. Know spend during artist between in morning.Camera easy street side executive note question. Human various someone moment.Require national try continue conference. Property notice structure program fall movie.Small senior big teacher southern class. Point leg social black. How see investment maybe indeed grow.Goal open story tell. Bit fill car think.\n5,We movement pull with near lead. Rock popular when control carry.Treat similar although. Store student five ok bed choice could. Five best community almost.Career these PM provide him. Attorney economic sell behind under wear Congress international. Worry serious social modern law paper.Figure impact employee wall interview major thank serious. Civil protect shake ask. Red hear leader training establish.Dark I role. Avoid defense cut husband culture.Begin account than practice. Table animal it speak.Laugh quickly before eat. Lose about page more. Home new issue.Successful send race road he. Parent must poor box. With wear run.Rock moment allow left system receive decide company. Seven ten then. Better point produce young these study person. American us authority off yes pay none.Our sit clear similar why impact. Could sense when team conference democratic bed mouth. Some whether certainly ago place. Story child matter price thus.\n...\n</code></pre>"},{"location":"blog/2024/05/31/test-redis-with-generated-data/#bring-up-a-local-redis-with-docker","title":"Bring up a local Redis with docker","text":"<pre><code>docker run --name my-redis -p 6379:6379 -d redis\n</code></pre>"},{"location":"blog/2024/05/31/test-redis-with-generated-data/#load-the-csv-to-docker-redis","title":"Load the csv to docker Redis","text":"<pre><code>docker cp large.csv my-redis:/tmp\ndocker exec -it my-redis -- sh\ncat /tmp/large.csv | awk -F',' '{print \" SET \\\"\"$1\"\\\" \\\"\"$2\"\\\" \\n\"}' | redis-cli --pipe\n</code></pre>"},{"location":"blog/2024/05/31/test-redis-with-generated-data/#configuration-in-rails","title":"Configuration in Rails","text":"<p>Setup of Redis is different for different environments. For development, local docker is used. For testing, I use a mock Redis library. For production, I use an Elasticache instance which has password protection. </p> <p>Here is the setup:</p> <pre><code># config/environments/development.rb\n$redis = Redis.new({host: 'localhost', port: 6379, db: 0})\n\n# config/environments/test.rb\n$redis = MockRedis.new\n\n# config/environments/production.rb\n$redis = Redis.new({host: ENV['REDIS_HOST'], port: ENV['REDIS_PORT'], password: ENV['REDIS_PASSWORD'], ssl:true})\n</code></pre>"},{"location":"blog/2024/05/31/test-redis-with-generated-data/#sample-rails-controller-and-unit-test","title":"Sample Rails controller and unit test","text":"<p>To keep it simple, I just want to get the value from the cache when the corresponding key is provided.</p> <p>Here is the sample implementation:</p> <pre><code># routes.rb\n\nget '/redis/:id', to: 'redis#index'\n\n\n# redis_controller.rb\n\nclass RedisController &lt; ApplicationController\n\n  def index\n    value = $redis.get(params[:id])\n    if value\n      render json: {key: params[:id], value: value}\n    else\n      render json: {warning: \"value not found from the cache\"}\n    end\n  end\n\nend\n\n# redis_controller_test.rb\n\nrequire \"test_helper\"\nrequire \"mock_redis\"\n\nclass RedisControllerTest &lt; ActionController::TestCase\n\n  setup do\n    $redis.set(1, \"whatever\")\n  end\n\n  test \"should return value if exists\" do\n    get :index, params: {id: 1}, as: :json\n    assert_response 200\n    assert_equal \"{\\\"key\\\":\\\"1\\\",\\\"value\\\":\\\"whatever\\\"}\", @response.body\n  end\n\n  test \"should return warning message if value not exist\" do\n    get :index, params: {id: 2}, as: :json\n    assert_response 200\n    assert_equal \"{\\\"warning\\\":\\\"value not found from the cache\\\"}\", @response.body\n  end\n\nend\n</code></pre>"},{"location":"blog/2025/06/12/goodbye-nginx-hello-traefik-effortless-https-with-lets-encrypt-and-docker/","title":"Goodbye Nginx, Hello Traefik! Effortless HTTPS with Let's Encrypt and Docker","text":"<p>If you've struggled with Nginx reverse proxy configs, certbot timers, and <code>nginx -s reload</code>, it's time to meet Traefik \u2014 a modern reverse proxy built for dynamic containerized environments.</p>"},{"location":"blog/2025/06/12/goodbye-nginx-hello-traefik-effortless-https-with-lets-encrypt-and-docker/#why-traefik-over-nginx","title":"Why Traefik over Nginx?","text":"<p>Unlike Nginx, which requires manual configuration updates and reloads, Traefik auto-discovers services via Docker labels, keeping your proxy config in sync with running containers. It also:</p> <ul> <li>Automatically obtains and renews Let\u2019s Encrypt certificates</li> <li>Handles HTTP/HTTPS routing, path-based rules, load balancing, and more</li> <li>Supports metrics, tracing, and even canary deployments with Traefik Enterprise</li> </ul> <p>For small setups or demos, it\u2019s a powerful, drop-in Nginx replacement \u2014 with less boilerplate.</p> <p></p>"},{"location":"blog/2025/06/12/goodbye-nginx-hello-traefik-effortless-https-with-lets-encrypt-and-docker/#old-way-nginx-certbot-pain","title":"Old Way: Nginx + Certbot = Pain","text":"<p>Let\u2019s say you want to proxy a backend running on port 8080 and a frontend on port 3000 \u2014 both behind HTTPS. With Nginx, you\u2019d need something like:</p>"},{"location":"blog/2025/06/12/goodbye-nginx-hello-traefik-effortless-https-with-lets-encrypt-and-docker/#nginx-config-static-and-fragile","title":"Nginx Config (static and fragile)","text":"<pre><code>server {\n  listen 80;\n  server_name example.com;\n  location / {\n    return 301 https://$host$request_uri;\n  }\n}\n\nserver {\n  listen 443 ssl;\n  server_name example.com;\n\n  ssl_certificate /etc/letsencrypt/live/example.com/fullchain.pem;\n  ssl_certificate_key /etc/letsencrypt/live/example.com/privkey.pem;\n\n  location / {\n    proxy_pass http://frontend:3000;\n  }\n\n  location /graphql {\n    proxy_pass http://backend:8080;\n  }\n}\n</code></pre> <p>Then you need to:</p> <ul> <li>Install Certbot</li> <li>Run it to issue the cert (possibly with a temporary port 80 server)</li> <li>Configure a cronjob for renewals</li> <li>Manually reload Nginx on changes</li> <li>Map all cert and config files into your container (if containerized)</li> </ul> <p>It works \u2014 but it\u2019s manual, brittle, and not fun to maintain in Docker-land.</p>"},{"location":"blog/2025/06/12/goodbye-nginx-hello-traefik-effortless-https-with-lets-encrypt-and-docker/#the-new-way-traefik-docker-labels","title":"The New Way: Traefik + Docker Labels","text":"<p>Let's walk through setting up Traefik to:</p> <ul> <li>Automatically detect frontend and backend services</li> <li>Use Let's Encrypt for HTTPS</li> <li>Route traffic using Docker labels</li> <li>Auto-renew certificates</li> </ul>"},{"location":"blog/2025/06/12/goodbye-nginx-hello-traefik-effortless-https-with-lets-encrypt-and-docker/#folder-structure","title":"Folder structure","text":"<pre><code>project-root/\n\u251c\u2500\u2500 docker-compose.yml\n\u251c\u2500\u2500 traefik.yml\n\u2514\u2500\u2500 acme.json\n</code></pre>"},{"location":"blog/2025/06/12/goodbye-nginx-hello-traefik-effortless-https-with-lets-encrypt-and-docker/#traefikyml","title":"<code>traefik.yml</code>","text":"<pre><code>log:\n  level: INFO\n\naccessLog: {}\n\napi:\n  dashboard: false\n\nentryPoints:\n  web:\n    address: \":80\"\n    http:\n      redirections:\n        entryPoint:\n          to: websecure\n          scheme: https\n\n  websecure:\n    address: \":443\"\n\ncertificatesResolvers:\n  letsencrypt:\n    acme:\n      email: your-email@example.com\n      storage: /acme.json\n      httpChallenge:\n        entryPoint: web\n\nproviders:\n  docker:\n    exposedByDefault: false\n</code></pre>"},{"location":"blog/2025/06/12/goodbye-nginx-hello-traefik-effortless-https-with-lets-encrypt-and-docker/#docker-composeyml","title":"<code>docker-compose.yml</code>","text":"<pre><code>version: \"3.9\"\n\nservices:\n  traefik:\n    image: traefik:v2.11\n    container_name: traefik\n    command:\n      - \"--configFile=/traefik.yml\"\n    ports:\n      - \"80:80\"\n      - \"443:443\"\n    volumes:\n      - ./traefik.yml:/traefik.yml:ro\n      - ./acme.json:/acme.json\n      - /var/run/docker.sock:/var/run/docker.sock:ro\n    networks:\n      - app_network\n\n  frontend:\n    image: your-frontend-image\n    labels:\n      - \"traefik.enable=true\"\n      - \"traefik.http.routers.frontend.rule=Host(`example.com`)\"\n      - \"traefik.http.routers.frontend.entrypoints=websecure\"\n      - \"traefik.http.routers.frontend.tls.certresolver=letsencrypt\"\n      - \"traefik.http.services.frontend.loadbalancer.server.port=80\"\n    networks:\n      - app_network\n\n  backend:\n    image: your-backend-image\n    labels:\n      - \"traefik.enable=true\"\n      - \"traefik.http.routers.backend.rule=Host(`example.com`) &amp;&amp; PathPrefix(`/graphql`)\"\n      - \"traefik.http.routers.backend.entrypoints=websecure\"\n      - \"traefik.http.routers.backend.tls.certresolver=letsencrypt\"\n      - \"traefik.http.services.backend.loadbalancer.server.port=8080\"\n    networks:\n      - app_network\n\nnetworks:\n  app_network:\n    driver: bridge\n</code></pre>"},{"location":"blog/2025/06/12/goodbye-nginx-hello-traefik-effortless-https-with-lets-encrypt-and-docker/#setup-notes","title":"Setup Notes","text":"<ul> <li>Create <code>acme.json</code> file and set permissions:</li> </ul> <pre><code>touch acme.json\nchmod 600 acme.json\n</code></pre> <ul> <li>Replace <code>example.com</code> with your domain.</li> <li>Replace image names with your real images.</li> </ul>"},{"location":"blog/2025/06/12/goodbye-nginx-hello-traefik-effortless-https-with-lets-encrypt-and-docker/#result","title":"Result","text":"<p>Traefik discovers containers via Docker labels. It fetches valid SSL certs from Let\u2019s Encrypt automatically. You can view the status and routes on https://:8080. No more writing separate Nginx conf files or restarting on config changes."},{"location":"blog/2025/06/12/goodbye-nginx-hello-traefik-effortless-https-with-lets-encrypt-and-docker/#final-thoughts","title":"Final Thoughts","text":"<p>Traefik makes it dead simple to get up and running with HTTPS in Docker. You get automatic discovery, dynamic config reloads, and zero-downtime cert renewal \u2014 with one clean configuration file.</p> <p>It's time to say goodbye to nginx.conf and embrace a modern, container-native proxy.</p>"},{"location":"blog/2025/05/17/setting-up-turborepo-remote-cache-with-s3-and-github-actions/","title":"Setting Up Turborepo Remote Cache with S3 and GitHub Actions","text":"<p>Setting up a production-grade remote cache for Turborepo using self hosted remote cache with AWS S3 and Lambda helps improve monorepo performance, especially in CI/CD pipelines like GitHub Actions. Below is a modular and generic Terraform setup using variables for easy customization.</p> <p></p> <p>This guide walks you through setting up a secure and production-ready remote cache using:</p> <ul> <li>AWS S3 (with default encryption)</li> <li>GitHub Actions with IAM AssumeRole via OIDC</li> <li>Infrastructure as Code using Terraform</li> </ul>"},{"location":"blog/2025/05/17/setting-up-turborepo-remote-cache-with-s3-and-github-actions/#s3-bucket-requirements","title":"S3 Bucket Requirements","text":"<ul> <li>Default encryption (AES-256): Enabled</li> <li>Versioning: Disabled (not needed for cache)</li> <li>Public Access: Blocked</li> </ul>"},{"location":"blog/2025/05/17/setting-up-turborepo-remote-cache-with-s3-and-github-actions/#terraform-setup","title":"Terraform Setup","text":""},{"location":"blog/2025/05/17/setting-up-turborepo-remote-cache-with-s3-and-github-actions/#directory-structure","title":"Directory Structure","text":"<pre><code>infra/\n\u251c\u2500\u2500 main.tf\n\u251c\u2500\u2500 variables.tf\n\u251c\u2500\u2500 outputs.tf\n\u2514\u2500\u2500 lambda.zip  # Your compiled Turborepo cache handler\n</code></pre>"},{"location":"blog/2025/05/17/setting-up-turborepo-remote-cache-with-s3-and-github-actions/#prepare-lambdazip","title":"Prepare lambda.zip","text":"<p>Use the following commands to generate a lambda.zip file. For more information, checkout ducktors documentation.</p> <pre><code>npm install turborepo-remote-cache\necho \"export { handler } from 'turborepo-remote-cache/aws-lambda';\" &gt; index.js\nesbuild index.js --bundle --platform=node --outfile=dist/index.js\ncd dist &amp;&amp; zip lambda.zip index.js\nmv lambda.zip ..\n</code></pre>"},{"location":"blog/2025/05/17/setting-up-turborepo-remote-cache-with-s3-and-github-actions/#maintf","title":"<code>main.tf</code>","text":"<pre><code>variable \"bucket_name\" {\n  description = \"Name of the S3 bucket for Turborepo cache\"\n  type        = string\n}\n\nvariable \"environment\" {\n  description = \"Environment tag for resources\"\n  type        = string\n  default     = \"Development\"\n}\n\nvariable \"turbo_token\" {\n  description = \"Turbo token used by the Lambda function\"\n  type        = string\n}\n\nvariable \"github_oidc_provider_arn\" {\n  description = \"GitHub OIDC provider ARN\"\n  type        = string\n}\n\nvariable \"github_org_or_repo_pattern\" {\n  description = \"GitHub OIDC subject pattern\"\n  type        = string\n}\n\nresource \"aws_s3_bucket\" \"turbo_cache\" {\n  bucket = var.bucket_name\n\n  tags = {\n    Name        = \"Turborepo Cache Bucket\"\n    Environment = var.environment\n  }\n}\n\nresource \"aws_s3_bucket_ownership_controls\" \"turbo_cache\" {\n  bucket = aws_s3_bucket.turbo_cache.id\n\n  rule {\n    object_ownership = \"BucketOwnerPreferred\"\n  }\n}\n\nresource \"aws_s3_bucket_server_side_encryption_configuration\" \"turbo_cache\" {\n  bucket = aws_s3_bucket.turbo_cache.id\n\n  rule {\n    apply_server_side_encryption_by_default {\n      sse_algorithm = \"AES256\"\n    }\n  }\n}\n\nresource \"aws_s3_bucket_lifecycle_configuration\" \"turbo_cache\" {\n  bucket = aws_s3_bucket.turbo_cache.id\n\n  rule {\n    id     = \"cleanup-old-cache\"\n    status = \"Enabled\"\n\n    filter {\n      prefix = \"logs/\"\n    }\n\n    expiration {\n      days = 30\n    }\n  }\n}\n\nresource \"aws_s3_bucket_public_access_block\" \"turbo_cache\" {\n  bucket = aws_s3_bucket.turbo_cache.id\n\n  block_public_acls       = true\n  ignore_public_acls      = true\n  restrict_public_buckets = true\n  block_public_policy     = true\n}\n\nresource \"aws_s3_bucket_policy\" \"turbo_cache\" {\n  bucket = aws_s3_bucket.turbo_cache.id\n  policy = data.aws_iam_policy_document.s3_secure_transport_deny.json\n}\n\nresource \"aws_iam_role\" \"github_actions_role\" {\n  name               = \"github-actions-turborepo-cache-role\"\n  assume_role_policy = data.aws_iam_policy_document.github_actions_assume_role.json\n}\n\nresource \"aws_iam_role\" \"turbo_cache_lambda_role\" {\n  name               = \"turborepo-cache-lambda-role\"\n  assume_role_policy = data.aws_iam_policy_document.lambda_assume_role.json\n}\n\nresource \"aws_iam_policy\" \"turbo_cache_lambda_policy\" {\n  name   = \"turborepo-cache-lambda-policy\"\n  policy = data.aws_iam_policy_document.lambda_policy.json\n}\n\nresource \"aws_iam_role_policy_attachment\" \"turbo_cache_lambda_attach\" {\n  role       = aws_iam_role.turbo_cache_lambda_role.name\n  policy_arn = aws_iam_policy.turbo_cache_lambda_policy.arn\n}\n\nresource \"aws_lambda_function\" \"turbo_cache\" {\n  function_name    = \"turborepo-remote-cache\"\n  role             = aws_iam_role.turbo_cache_lambda_role.arn\n  handler          = \"index.handler\"\n  runtime          = \"nodejs22.x\"\n  filename         = \"${path.module}/lambda.zip\"\n  source_code_hash = filebase64sha256(\"${path.module}/lambda.zip\")\n\n  environment {\n    variables = {\n      STORAGE_PATH     = aws_s3_bucket.turbo_cache.bucket\n      STORAGE_PROVIDER = \"s3\"\n      TURBO_TOKEN      = var.turbo_token\n    }\n  }\n}\n\nresource \"aws_lambda_function_url\" \"turbo_cache_lambda_url\" {\n  function_name      = aws_lambda_function.turbo_cache.function_name\n  authorization_type = \"NONE\"\n\n  cors {\n    allow_origins = [\"*\"]\n    allow_methods = [\"*\"]\n    allow_headers = [\"*\"]\n  }\n}\n\ndata \"aws_iam_policy_document\" \"github_actions_assume_role\" {\n  statement {\n    effect = \"Allow\"\n\n    actions = [\"sts:AssumeRoleWithWebIdentity\"]\n\n    principals {\n      type        = \"Federated\"\n      identifiers = [var.github_oidc_provider_arn]\n    }\n\n    condition {\n      test     = \"StringEquals\"\n      variable = \"token.actions.githubusercontent.com:aud\"\n      values   = [\"sts.amazonaws.com\"]\n    }\n\n    condition {\n      test     = \"StringLike\"\n      variable = \"token.actions.githubusercontent.com:sub\"\n      values   = [var.github_org_or_repo_pattern]\n    }\n  }\n}\n\ndata \"aws_iam_policy_document\" \"lambda_assume_role\" {\n  statement {\n    effect = \"Allow\"\n\n    principals {\n      type        = \"Service\"\n      identifiers = [\"lambda.amazonaws.com\"]\n    }\n\n    actions = [\"sts:AssumeRole\"]\n  }\n}\n\ndata \"aws_iam_policy_document\" \"lambda_policy\" {\n  statement {\n    effect = \"Allow\"\n\n    actions = [\n      \"s3:GetObject\",\n      \"s3:PutObject\",\n      \"s3:HeadObject\",\n      \"logs:CreateLogGroup\",\n      \"logs:CreateLogStream\",\n      \"logs:PutLogEvents\"\n    ]\n\n    resources = [\n      aws_s3_bucket.turbo_cache.arn,\n      \"${aws_s3_bucket.turbo_cache.arn}/*\",\n      \"arn:aws:logs:*:*:*\"\n    ]\n  }\n}\n\ndata \"aws_iam_policy_document\" \"s3_secure_transport_deny\" {\n  statement {\n    sid     = \"DenyInsecureTransport\"\n    effect  = \"Deny\"\n    actions = [\"s3:*\"]\n\n    principals {\n      type        = \"*\"\n      identifiers = [\"*\"]\n    }\n\n    resources = [\n      aws_s3_bucket.turbo_cache.arn,\n      \"${aws_s3_bucket.turbo_cache.arn}/*\"\n    ]\n\n    condition {\n      test     = \"Bool\"\n      variable = \"aws:SecureTransport\"\n      values   = [\"false\"]\n    }\n  }\n}\n</code></pre>"},{"location":"blog/2025/05/17/setting-up-turborepo-remote-cache-with-s3-and-github-actions/#variablestf","title":"<code>variables.tf</code>","text":"<pre><code>variable \"bucket_name\" {\n  description = \"Name of the S3 bucket for Turborepo cache\"\n  type        = string\n}\n\nvariable \"environment\" {\n  description = \"Deployment environment tag (e.g., Development, Staging, Production)\"\n  type        = string\n}\n\nvariable \"github_org_or_repo_pattern\" {\n  description = \"GitHub OIDC repo pattern for role assumption\"\n  type        = string\n}\n\nvariable \"github_oidc_provider_arn\" {\n  description = \"ARN of the GitHub OIDC provider\"\n  type        = string\n}\n\nvariable \"turbo_token\" {\n  description = \"Turborepo access token\"\n  type        = string\n  sensitive   = true\n}\n</code></pre>"},{"location":"blog/2025/05/17/setting-up-turborepo-remote-cache-with-s3-and-github-actions/#outputstf","title":"<code>outputs.tf</code>","text":"<pre><code>output \"s3_bucket_name\" {\n  value = aws_s3_bucket.turbo_cache.id\n}\n\noutput \"github_role_arn\" {\n  value = aws_iam_role.github_actions.arn\n}\n\noutput \"lambda_url\" {\n  value = aws_lambda_function_url.turbo_cache_lambda_url.function_url\n}\n</code></pre>"},{"location":"blog/2025/05/17/setting-up-turborepo-remote-cache-with-s3-and-github-actions/#terraformtfvars-sample","title":"terraform.tfvars Sample","text":"<pre><code>bucket_name                = \"my-turbo-cache-bucket\"\nenvironment                = \"Development\"\ngithub_oidc_provider_arn   = \"arn:aws:iam::123456789012:oidc-provider/token.actions.githubusercontent.com\"\ngithub_org_or_repo_pattern = \"repo:my-org/*\"\nturbo_token                = \"your-turborepo-token\"\n</code></pre>"},{"location":"blog/2025/05/17/setting-up-turborepo-remote-cache-with-s3-and-github-actions/#github-actions-workflow","title":"GitHub Actions Workflow","text":"<p><code>.github/workflows/build.yml</code></p> <pre><code>name: Build with Turborepo Cache (S3)\n\non:\n  push:\n    branches: [main]\n\npermissions:\n  id-token: write\n  contents: read\n\njobs:\n  build:\n    runs-on: ubuntu-latest\n    env:\n      TURBO_TEAM: your-team\n      TURBO_TOKEN: ${{ secrets.TF_VAR_TURBO_TOKEN }}\n      TURBO_API: replace-with-lambda_url-out\n\n\n    steps:\n      - name: Checkout repo\n        uses: actions/checkout@v4\n\n      - name: Configure AWS credentials (OIDC)\n        uses: aws-actions/configure-aws-credentials@v4\n        with:\n          role-to-assume: arn:aws:iam::123456789012:role/github-actions-turbo-role\n          role-session-name: GitHubActionsTurboCacheSession\n          aws-region: us-east-1\n\n      - name: Setup pnpm\n        uses: pnpm/action-setup@v3\n        with:\n          version: 8\n\n      - name: Install dependencies\n        run: pnpm install\n\n      - name: Build with Turbo cache\n        run: pnpm turbo run build --team=\"your-team\" --token=${{ secrets.TF_VAR_TURBO_TOKEN }}\n</code></pre>"},{"location":"blog/2025/05/17/setting-up-turborepo-remote-cache-with-s3-and-github-actions/#turbojson-sample","title":"turbo.json Sample","text":"<pre><code>{\n  \"$schema\": \"https://turbo.build/schema.json\",\n  \"tasks\": {\n    \"dev\": {\n      \"cache\": false\n    },\n    \"build\": {\n      \"dependsOn\": [\"^build\"],\n      \"outputs\": [\"dist/**\"]\n    }\n  }\n}\n</code></pre>"},{"location":"blog/2025/05/17/setting-up-turborepo-remote-cache-with-s3-and-github-actions/#result","title":"Result:","text":""},{"location":"blog/2025/05/17/setting-up-turborepo-remote-cache-with-s3-and-github-actions/#without-cache","title":"Without cache","text":"<pre><code> Tasks:    1 successful, 1 total\nCached:    0 cached, 1 total\n  Time:    4.648s \n</code></pre>"},{"location":"blog/2025/05/17/setting-up-turborepo-remote-cache-with-s3-and-github-actions/#with-cache","title":"With cache","text":"<pre><code> Tasks:    1 successful, 1 total\nCached:    1 cached, 1 total\n  Time:    721ms &gt;&gt;&gt; FULL TURBO\n</code></pre>"},{"location":"blog/2025/05/17/setting-up-turborepo-remote-cache-with-s3-and-github-actions/#summary","title":"Summary","text":"<p>This Terraform-based setup provisions a secure and production-ready Turborepo remote cache using:</p> <ul> <li> <p>S3 for storage (with AES-256 encryption and lifecycle rules)</p> </li> <li> <p>Lambda to serve the cache API</p> </li> <li> <p>IAM/OIDC Integration with GitHub Actions for secure, short-lived access</p> </li> <li> <p>GitHub Actions workflow pre-wired to leverage the remote cache</p> </li> </ul>"},{"location":"blog/2025/05/17/setting-up-turborepo-remote-cache-with-s3-and-github-actions/#key-benefits","title":"Key Benefits:","text":"<p>Significant speed-up in CI pipelines using cached builds Modular and environment-agnostic Terraform for reusable infra Security best practices enforced (e.g., S3 bucket policies, IAM roles) Easy-to-integrate GitHub Actions support with OIDC</p>"},{"location":"blog/2025/05/17/setting-up-turborepo-remote-cache-with-s3-and-github-actions/#performance-gain","title":"Performance Gain","text":"<p><pre><code>Build Time Comparison\n|\n|    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588  4.648s (No Cache)\n|    \u2588\u2588                  0.721s (With Cache)\n|\n</code></pre> ~84.5% time saved per build with the remote cache!</p>"},{"location":"home-server/about/","title":"How to apply fun technologies you learn at work for home","text":""},{"location":"home-server/about/#originally-posted-at-ias-medium-blog-at-dec-17-2020","title":"originally posted at IAS medium blog at Dec 17, 2020","text":"<p>I love technology and I am lucky enough to get paid for pursuing this hobby. So it is no surprise that in my spare time at home I am experimenting with new technologies and leveraging those to make my life at home a bit more convenient. One of the latest projects I worked on at home was rebuilding my home server.</p> <p>In this new socially distant reality, the tools I use at work (Docker and automation with Git and Jenkins) have helped me build a home server to unify and simplify entertainment, and connect with family and friends via:</p> <ul> <li>central media server to stream music and video</li> <li>personal cloud to backup photos</li> <li>firewall for network security</li> <li>family blog</li> </ul>"},{"location":"home-server/about/#docker-and-docker-compose","title":"Docker and Docker Compose","text":"<p>At IAS, we use Docker, from development to deployment, both leveraging official images or building our own. Since Docker is so helpful, I decided to use it on my own server. I found the following benefits with this move:</p> <ul> <li>maintainable (each Docker component is scripted in a docker-compose.yml file)</li> <li>upgradable with ease (rebuild the service with the latest image)</li> <li>easy to organize and backup (all persistence Docker volumes live in their own directory)</li> <li>migratable (though my server crashed months ago, it was easy to rebuild everything on a new virtual machine)</li> </ul>"},{"location":"home-server/about/#automation-with-git-and-jenkins","title":"Automation with Git and Jenkins","text":"<p>Automation is the new mantra in our day-to-day work on the IAS Prime team, with Git and Jenkins playing a big role in that mindset. We automate spinning up environments, running tests, creating bug reports from Slack messages, deploying software updates, and AWS Autoscaling.</p> <p>My main take-away from working on the AWS onboarding project with the Jenkins pipeline is how convenient it is to set up CICD with code. I was amazed by the results, so I borrowed the same approach for home. Now, I never have to log in to the server to run scripts. With the Jenkins web interface, I can review the job status and reports whenever needed. I run repeatable tasks in my home server with the following:</p> <ul> <li>Jenkins (in a Docker)</li> <li>Git (Gitea, a lightweight git server, in a Docker)</li> </ul> <p>Then I create shell scripts to handle these repeatable maintenance tasks:</p> <ul> <li>auto-deployment of Docker image (by checking in a new docker-compose.yml file to Gitea which wired to Jenkins for the deployment)</li> <li>weekly backup of data (Jenkins Scheduler to pull the back up script from Gitea)</li> <li>weekly upgrade of docker images (Jenkins Scheduler to pull the upgrade script from Gitea)</li> </ul> <p>I had a lot of fun rebuilding the server and it was a reminder of what I have learned from my daily work. It\u2019s also a little showcase to my non-techy family members of what I do on the job.</p> <p>Learning and using technology goes both ways. On weekends, I use an open-source Python project that I created to generate math quiz sheets for family and friends, which in turn have improved my Python skills for work.</p> <p>Integrating technology into my family life reminded me about why I got into this field \u2014 I have a passion for technology. Sometimes I ask myself where all this learning has benefitted me most\u2026 is what I\u2019m learning at work helping me at home, or is what I\u2019m learning at home helping me at work?</p> <p>At Integral Ad Science I love that my job allows for experimentation and learning new technologies. I am proud of the systems we have built over the last few years and I continue to be amazed by the volumes being processed (3B \u2014 5B events per hour) through those systems. I love the team I am working with and feel that I grow as a developer.</p> <p>At home I love playing with computer hardwares. My new hobby is setting up thin client machines around the house since they are low power (15 W) and silent (fan-less). I use them as a media streamer, for light web browsing and watching O\u2019Reilly training video courses (a benefit of working in the IAS).</p> <p>In the end, it doesn\u2019t matter because it\u2019s the technology that drives my happiness in my work and my personal life.</p>"},{"location":"home-server/hardware/","title":"Hardware","text":""},{"location":"home-server/hardware/#servers","title":"Servers","text":"<p> HP Microserver Gen 8 (A)</p> Specs <ul> <li>Xeon E3-1265L V2 (4 Cores 8 Threads)</li> <li>16GB ram</li> </ul> OS <ul> <li>Esxi</li> <li>Ubuntu</li> </ul> Usages <ul> <li>OpenMediaVault (NAS)</li> <li>Plex/JellyFin</li> <li>Gitea</li> <li>Jenkins</li> <li>lighttpd (web server)</li> </ul> <p> HP Microserver Gen 8 (B)</p> Specs <ul> <li>Xeon E3-1265L V2 (4 Cores 8 Threads)</li> <li>16GB ram</li> </ul> OS <ul> <li>Proxmox</li> </ul> Usages (TBD) <ul> <li>K3s</li> <li>Job Winner</li> <li>Math Worksheet Generator</li> <li>Snippet Box</li> <li>IT Tools</li> <li>Ghost Blog for the kids</li> <li>Stirling PDF</li> <li>Uptime Kuma</li> </ul>"},{"location":"home-server/hardware/#thin-clients","title":"Thin Clients","text":"<p> HP T610</p> Specs <ul> <li>AMD Dual-Core T56N APU with Radeon HD 6320 Graphics (1.65 GHz, 1MB L2 cache)</li> <li>8GB ram</li> </ul> OS <p>Puppy</p> Usage <ul> <li>Standalone reading station</li> <li>Music client connecting to a JVC FS-2000 mini Hi-Fi</li> </ul> <p> HP T620 (x2)</p> Specs <ul> <li>AMD GX-415GA Quad-Core APU with AMD Radeon HD 8330E (1.5 GHz)</li> <li>8GB ram</li> </ul> OS <p>Linux Lite</p> Usage <ul> <li>Media Server connecting to TV</li> <li>Retro game emulator with Retropie</li> </ul>"},{"location":"home-server/kubernetes-lab/","title":"Note to build k8s homelab","text":""},{"location":"home-server/kubernetes-lab/#install-minikube-on-ubuntu","title":"Install MiniKube on Ubuntu","text":"<pre><code>curl -LO https://storage.googleapis.com/minikube/releases/latest/minikube_latest_amd64.deb\nsudo dpkg -i minikube_latest_amd64.deb\n\n\nsudo groupadd docker\nsudo usermod -aG docker $USER\nnewgrp \nsudo chmod 666 /var/run/docker.sock\n\nminikube start\n</code></pre>"},{"location":"home-server/kubernetes-lab/#deploy-hello-minikube-and-expose-it-remotely","title":"Deploy hello-minikube and expose it remotely","text":"<p><pre><code>kubectl create deployment hello-minikube --image=kicbase/echo-server:1.0\nkubectl expose deployment hello-minikube --type=NodePort --port=8080\nkubectl port-forward --address 0.0.0.0 service/hello-minikube 8080:8080\nkubectl port-forward service/hello-minikube 8080:8080 # this will only expose it within the host\n</code></pre> Figured out I cannot connect to minikube app remotely easily. Time to switch to k3s.</p>"},{"location":"home-server/kubernetes-lab/#install-k3s-on-ubuntu","title":"Install K3s on Ubuntu","text":"<pre><code>curl -sfL https://get.k3s.io | sh -\nsudo chmod 644 /etc/rancher/k3s/k3s.yaml\nexport KUBECONFIG=/etc/rancher/k3s/k3s.yaml\n</code></pre> <p>Create sample app that can be accessed within LAN</p> <pre><code>kubectl create deployment hello-world --image=kicbase/echo-server:1.0\nkubectl expose deployment hello-world --type=NodePort --port=8080\n</code></pre> <p>install argocd </p> <pre><code>export ARGOCD_VERSION=v2.10.0\nkubectl create namespace argocd\nkubectl apply -n argocd -f https://raw.githubusercontent.com/argoproj/argo-cd/$ARGOCD_VERSION/manifests/core-install.yaml\nkubectl delete -n argocd -f https://raw.githubusercontent.com/argoproj/argo-cd/$ARGOCD_VERSION/manifests/core-install.yaml\n\nkubectl apply -n argocd -f https://raw.githubusercontent.com/argoproj/argo-cd/$ARGOCD_VERSION/manifests/install.yaml\n</code></pre>"},{"location":"home-server/operating-system/","title":"Operating system","text":"<p>I started using linux at home since Ubuntu 6.04 and never look back to Windows since then. Ubuntu, Kubuntu, Centos, Linux Lite, Mint, Puppy are some of my favorites. </p> <p>Currently I am using the followings:</p> Ubuntu <p></p> <p>As the server of my docker farm.</p> Kubuntu <p></p> <ul> <li>For my spare desktop setup</li> <li>For my wife's laptop for daily web browsering and video streaming</li> </ul> Linux Lite <p></p> <ul> <li>As the media streaming boxes in living room and bedroom connecting to the TVs</li> <li>For my children' laptops which they use it for school work and light gaming</li> </ul> Puppy <p></p> <ul> <li>Deployed on a HP T610 thin client as a ebook reading station connecting to a monitor in portrait mode</li> <li>Double as a music client connecting to a vintage JVC FS-2000 mini Hi-Fi</li> </ul> Retropie/Batocera <p> </p> <p>I love playing retro games and have been sharing the love with my kids.</p>"},{"location":"home-server/recovery/","title":"Steps to troubleshoot when home server is down after power loss","text":"<ol> <li> <p>Connect a laptop to the router and check for internet connection</p> </li> <li> <p>Visit the ESXi UI to see if the server is up</p> </li> <li> <p>If ther server is not up </p> <ul> <li>Connect the server with monitor, keyboard and network cable</li> <li>Power on/Force restart of the server</li> </ul> </li> <li> <p>Start up the Firewall and PiHole VMs</p> </li> <li> <p>Check server 2 ESXi UI and restart it if needed</p> </li> <li> <p>Start up VM</p> </li> <li> <p>Bring up all docker services</p> </li> </ol>"},{"location":"home-server/software/","title":"Software","text":""},{"location":"home-server/software/#pfsense","title":"Pfsense","text":"<p>Besides serving as a fireawll, it is used to seperate my main network from the IoT network. I also use it to limit internet access for my kids.</p>"},{"location":"home-server/software/#pihole","title":"PiHole","text":"<p>A must have to block ad servers. Pairing with Pfsense as the default DNS, all of my home devices are filtered from 90% of unwanted ad.</p>"},{"location":"home-server/software/#openmediavault","title":"OpenMediaVault","text":"<p>A free NAS also serving as a NFS server.</p>"},{"location":"home-server/software/#jellyfin","title":"Jellyfin","text":"<p>A free media server and an alternative of Plex. It supports playing music and video with its android app out of the box and it is free of unwanted ad/extra services. With that, I can stream music/movie from any device, as well as TV, within my home network.</p>"},{"location":"home-server/software/#lighttpd","title":"Lighttpd","text":"<p>A very light weight web server. It is used to send ebooks to my Kindles via the experimental web browser.</p>"},{"location":"home-server/software/#gitea","title":"Gitea","text":"<p>A light weight git server to store private repo, home server setup scripts, etc.</p>"},{"location":"home-server/software/#jenkins","title":"Jenkins","text":"<p>A CICD server to handle home server deployment and to run maintenance scripts on schedule.</p>"},{"location":"notes/devops/dynamodb/","title":"Dynamodb","text":""},{"location":"notes/devops/dynamodb/#list-all-tables","title":"List all tables","text":"<pre><code>aws dynamodb scan list-tables\n</code></pre>"},{"location":"notes/devops/dynamodb/#backup-a-table","title":"Backup a table","text":"<pre><code>aws dynamodb scan --table-name TABLE-NAME &gt; backup.json\n</code></pre>"},{"location":"notes/devops/dynamodb/#get-a-single-record-by-id","title":"Get a single record by ID","text":"<pre><code>aws dynamodb get-item --table-name TABLE-NAME --key '{\"id\": {\"S\":\"the-id-to-look-for\"}}'\n</code></pre>"},{"location":"notes/devops/dynamodb/#delete-a-single-record-by-id","title":"Delete a single record by ID","text":"<pre><code>aws dynamodb delete-item --table-name TABLE-NAME --key '{\"id\": {\"S\":\"the-id-to-look-for\"}}'\n</code></pre>"},{"location":"notes/devops/kubernetes-reference/","title":"Kubernetes reference","text":""},{"location":"notes/devops/kubernetes-reference/#basic","title":"Basic","text":"<pre><code># export KUBECONFIG\nexport KUBECONFIG=~/.kube/[config-file]\n\n# create a namespace\nk create ns [namespace]\n\n# check current cluster\nk config current-context\nk config get-contexts\n\n# describe\nk describe [type] [name] -n [namespace]\n\n# get everything\nk get all\nk get [type] -A\n\n# drain a node https://kubernetes.io/docs/tasks/administer-cluster/safely-drain-node/\nk drain [node] --ignore-daemonsets --delete-emptydir-data\n\n# edit a deployment\nk edit deployment [deployment] -n [namespace]\n\n# set 0 pod for a deployment\nk scale deployment [deployment] -n [namespace] --replicas=0\n</code></pre>"},{"location":"notes/devops/kubernetes-reference/#dbinstance","title":"DBInstance","text":"<pre><code>k get dbinstance [instance-name] -n [namespace]\n\n# to get address and port\nk get dbinstance [instance-name] -n [namespace] -o json | jq -r '.status.endpoint.address'\nk get dbinstance [instance-name] -n [namespace] -o json | jq -r '.status.endpoint.port'\n</code></pre>"},{"location":"notes/devops/kubernetes-reference/#helm","title":"Helm","text":"<pre><code># sample workflow to install prometheus from helm\nhelm repo add prometheus-community https://prometheus-community.github.io/helm-charts\nhelm search repo prometheus-community\nhelm upgrade --install -n istio-system prometheus prometheus-community/prometheus\n\n# to install with customized value file and specific version\nhelm upgrade --install -n istio-system prometheus prometheus-community/prometheus \\\n  -f values.yaml \\\n  --version 23.3.0\n\n# to uninstall\nhelm uninstall prometheus\n\n# to check template to be generated\nhelm template . -f values.yaml -f values2.yaml\n\n# to check the output of a single template\nhelm template . -f values.yaml -s PATH-TO-TEMPLATE-FILE/FILE-NAME.yaml\n</code></pre>"},{"location":"notes/devops/kubernetes-reference/#initcontainer","title":"InitContainer","text":"<p>Init containers are speicalized containers that run before app containers in the same pod. One usage of init container is to download file and share it with the app container via shared volume. To learn more, check out official Kubernetes documentation.</p> <pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: myapp\n  labels:\n    app: myapp\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: mydb\n  template:\n    metadata:\n      labels:\n        app: mydb\n    spec:\n      containers:\n        - name: mydb\n          image: postgres\n          env:\n          - name: POSTGRES_PASSWORD\n            value: example\n          ports:\n          - containerPort: 5432\n            name: postgres\n          volumeMounts:\n            - name: data\n              mountPath: /docker-entrypoint-initdb.d\n      initContainers:\n        - name: curl-downloader\n          image: appropriate/curl\n          args:\n            - \"-o\"\n            - \"/tmp/data/init.sql\"\n            - \"https://raw.githubusercontent.com/januschung/support-system-db/main/sql-scripts/create_tables.sql\"\n          volumeMounts:\n            - name: data\n              mountPath: /tmp/data\n      volumes:\n        - name: data\n          emptyDir: {}\n</code></pre>"},{"location":"notes/devops/ruby-note/","title":"Ruby notes","text":"<p>use rbenv to manage ruby version</p> <pre><code>brew install rbenv ruby-build\nrbenv init\n\n# list latest stable versions:\nrbenv install -l\n\n# list all local versions:\nrbenv install -L\n\n# install a Ruby version:\nrbenv install 3.1.2\n\nrbenv global 3.1.2   # set the default Ruby version for this machine\n# or:\nrbenv local 3.1.2    # set the Ruby version for this directory\n\n# set the following in .zshrc\neval \"$(rbenv init - zsh)\"\n</code></pre>"},{"location":"notes/devops/ruby-note/#add-support-of-postgres","title":"Add support of postgres","text":"<p>To use Postres DB, gem \"pg\" is required. </p> <p>Run the following if <code>bundle install</code> fails to work. <pre><code>brew install postgresql\n</code></pre></p>"},{"location":"notes/devops/ruby-note/#seed-db-to-create-a-user","title":"Seed db to create a user","text":"<p>We can seed the database with plain sql  <pre><code>connection = ActiveRecord::Base.connection()\nconnection.execute(\"SQL GOES HERE\")\n</code></pre></p>"},{"location":"notes/shell-script/backup-with-rsync/","title":"Backup with rsync","text":"<p>Here are the scripts which I have been using to generate backup for file and mysql database. They  can be run manually but preferably with cron jobs for automation.</p> <p>Note that for the mysql backup script, there is a setting for <code>rotation_day</code>. This is used to keep the number of copy of the database backup, which will be recyled in the final commnad:</p> <p><code>find $backup_path/ -mtime +$rotation_days -exec rm {} \\;</code></p> <p>The backup file is also making use of <code>gpg</code> for encryption.</p>"},{"location":"notes/shell-script/backup-with-rsync/#backup-file-with-rsync","title":"Backup file with rsync","text":"<pre><code>#!/bin/bash\nsource_path=\"[CHANGE ME]\"\ndestination_path=\"[CHANGE ME]\"\n # prefix for log file, eg \"photobackup-\"\nprefix=\"[CHANGE ME]\"\nlogfile=$prefix\"_\"$(date +%d_%m_%Y__%H).log\n\numask 177\n\n # run rsync and exclude the archive folder\n/usr/bin/rsync -avz --log-file=$destination_path/$logfile --exclude archive $source_path $destination_path\n</code></pre>"},{"location":"notes/shell-script/backup-with-rsync/#backup-mysql-database-biweekly-with-encryption","title":"Backup mysql database biweekly with encryption","text":"<pre><code>#!/bin/bash\nuser=\"[CHANGE ME]\"\npassword=\"[CHANGE ME]\"\ngpgrcp=\"[CHANGE ME]\"\nDATABASE=\"[CHANGE ME]\"\n#set daily backup dir\nbackup_path=\"[CHANGE ME]\"\nprefix=\"[CHANGE ME]\"\nsuffix=\"_\"$(date +%d_%m_%Y__%H).sql.gz\nrotation_days=14;\numask 177\n\nFILENAME=$prefix$DATABASE$suffix\n# dump and gzip databases\n/usr/bin/mysqldump -u$user -p$password $DATABASE \\\n           --events --ignore-table=mysql.event | \\\n           /bin/gzip &gt; $backup_path/$FILENAME\n# encrypt files\n/usr/bin/gpg -r $gpgrcp -e $backup_path/$FILENAME\n# delete only gzipped files\n/bin/rm $backup_path/$FILENAME\n\n# recycle files\nfind $backup_path/ -mtime +$rotation_days -exec rm {} \\;\n</code></pre>"},{"location":"notes/shell-script/git-reference/","title":"Git reference","text":""},{"location":"notes/shell-script/git-reference/#using-git-mob","title":"Using Git Mob","text":"<p>Git mob is a command line tool to copilot with your team when you collaborate on code. Read more about Git mob.</p> <p>To setup Git Mob, first you have to definte your information: <pre><code>git config --global user.name \"Janus Chung\"\ngit config --global user.email \"janus.chung@dummy-domain.com\"\n</code></pre></p> <p>Add your teammate to <code>.git-coauthors</code> file <pre><code>$ cat &lt;&lt;-EOF &gt; ~/.git-coauthors\n{\n  \"coauthors\": {\n    \"rh\": {\n      \"name\": \"Robin Hood\",\n      \"email\": \"rhood@dummy-domain.com\"\n    },\n    \"ab\": {\n      \"name\": \"Astro Boy\",\n      \"email\": \"aboy@dummy-domain.com\"\n    }\n  }\n}\nEOF\n</code></pre></p> <p>Say if you want to pair with only Astro Boy <pre><code>git mob ab\n</code></pre></p> <p>If you want to pair with both Robin Hood and Astro Boy <pre><code>git mob rh ab\n</code></pre></p> <p>If you decide to code solo <pre><code>git solo\n</code></pre></p>"},{"location":"notes/shell-script/git-reference/#to-tag-a-build","title":"To tag a build","text":"<pre><code>git checkout main\ngit pull\ngit tag x.x.x\ngit push origin x.x.x\n</code></pre>"},{"location":"notes/shell-script/git-reference/#rebase","title":"Rebase","text":"<pre><code>git checkout main\ngit pull\ngit checkout feature-branch\ngit rebase main feature-branch\ngit push --force origin feature-branch\n</code></pre>"},{"location":"notes/shell-script/git-reference/#bash-helper","title":"Bash helper","text":""},{"location":"notes/shell-script/git-reference/#checkout-main-and-pull","title":"Checkout main and pull","text":"<pre><code>gm(){\n  git checkout main &amp;&amp; git pull\n}\n</code></pre>"},{"location":"notes/shell-script/git-reference/#commit-with-feature-branch-as-the-prefix","title":"Commit with feature branch as the prefix","text":"<pre><code>gcm(){\n  if [[ $# -eq 0 ]] ; then\n    echo \"add a git comment\"\n  else\n    branch=$(git branch | grep '*' | awk '{print $2}')\n    echo $branch\n    echo \"$branch $*\"\n    git commit -m \"$branch $*\"\n  fi    \n}\n</code></pre>"},{"location":"notes/shell-script/git-reference/#push-to-feature-branch-without-typing-it-out","title":"Push to feature branch without typing it out","text":"<pre><code>gp(){\n  branch=$(git branch | grep '*' | awk '{print $2}')\n  git push origin $branch\n}\n</code></pre>"},{"location":"notes/shell-script/git-reference/#auto-tag-and-push-a-new-minor-build","title":"auto tag and push a new minor build","text":"<pre><code>gt(){\n  git checkout main &amp;&amp; git pull\n  LAST_TAG_SHA=$(git show-ref | tail -n 1 | awk '{print $1}')\n  LAST_TAG=$(git show-ref | tail -n 1 | awk '{print $2}' | cut -d '/' -f 3)\n  LAST_TAG_PATCH_VERSION=$(echo \"${LAST_TAG%%-*}\" | cut -d '.' -f 3)\n  NEW_TAG_PATCH_VERSION=$((LAST_TAG_PATCH_VERSION + 1))\n  NEW_TAG=$(echo \"$LAST_TAG\" | cut -d '.' -f 1,2).$NEW_TAG_PATCH_VERSION\n\n  git tag \"$NEW_TAG\" \"$LAST_TAG_SHA\"\n  git push origin \"$NEW_TAG\"\n}\n</code></pre>"},{"location":"notes/shell-script/jq-reference/","title":"Jq reference","text":""},{"location":"notes/shell-script/jq-reference/#read-a-json-file","title":"Read a json file","text":"<p>Give a <code>color.json</code> file with the following content:</p> <pre><code>[\n    {\n        \"name\": \"red\",\n        \"value\": \"#ff0000\"\n    },\n    {\n        \"name\": \"green\",\n        \"value\": \"#008000\"\n    },\n    {\n        \"name\": \"blue\",\n        \"value\": \"#0000ff\"\n    },\n    {\n        \"name\": \"yellow\",\n        \"value\": \"#ffff00\"\n    },\n    {\n        \"name\": \"black\",\n        \"value\": \"#000000\"\n    }\n]\n</code></pre> <p>The following bash script will read the json array one by one</p> <pre><code>#!/usr/bin/env bash\n\nJSON_FILE=\"color.json\"\n\nwhile read -r color; do\n    name=$(jq -r .name &lt;&lt;&lt; \"$color\")\n    value=$(jq -r .value &lt;&lt;&lt; \"$color\")\n    echo \"Color $name has value as $value.\"\ndone &lt; &lt;(jq -c '.[]' $JSON_FILE)\n</code></pre> <p>Here is the sample output:</p> <pre><code>Color red has value as #ff0000.\nColor green has value as #008000.\nColor blue has value as #0000ff.\nColor yellow has value as #ffff00.\nColor black has value as #000000.\n</code></pre>"},{"location":"opensource-projects/asian-character-worksheet-generator/","title":"Asian Character Worksheet Generator","text":""},{"location":"opensource-projects/asian-character-worksheet-generator/#background","title":"Background","text":"<p>As an immigrant and father of two boys living in America, I want my sons to learn my language so that they can have the chance to learn more about their culture origin. I want to share with them the novels that I have read; the music that I have listened to and the movies that I have watched. To make it happens, it is essential to teach them how to write in my language.</p> <p>At first I started making worksheet by using Google Doc. I had to create the layout manually (in grid format) and adjust the spacing. Then I had to type in character by character to each cell and readjust the spacing one by one. This project saves you time from the tedious manual effort of doing a copy and paste of a new worksheet. Also, this project automatically does all the work to adjust the column widths which is frankly not that enjoyable to do manually (Hey I want my beer time).</p> <p>That's the reason for me to look into an automate way to get the job done.</p>"},{"location":"opensource-projects/asian-character-worksheet-generator/#benefits-of-grid-system","title":"Benefits of Grid System","text":"<p>The grid system helps someone learning the language to write the characters in the proper size and orientation. The grid system gives a worksheet for the new learner to practice writing the characters.</p>"},{"location":"opensource-projects/asian-character-worksheet-generator/#requirements","title":"Requirements","text":"<ol> <li>python3</li> <li>fpdf</li> </ol>"},{"location":"opensource-projects/asian-character-worksheet-generator/#how-to-use","title":"How to Use","text":"<ol> <li>Edit the file <code>text</code> with the list of characters of your choice (or use the sample). Note that you need to supply 14 characters on each line. <pre><code>\u4e00\u4e8c\u4e09\u56db\u4e94\u516d\u4e03\u516b\u4e5d\u5341\u767e\u5343\u5de6\u53f3\n\u4e0a\u5927\u4eba\u5b54\u4e59\u5df1\u5316\u4e09\u5343\u4e03\u5341\u58eb\u5973\u5c0f\n\u751f\u516b\u4e5d\u5b50\u4f73\u4f5c\u4ec1\u53ef\u77e5\u79ae\u4e5f\u4f60\u6211\u4ed6\n\u7236\u6bcd\u5144\u5f1f\u4e0a\u4e2d\u4e0b\u6771\u5357\u897f\u5317\u541b\u89aa\u5e2b\n\u65e5\u6708\u91d1\u6728\u6c34\u706b\u571f\u5929\u5730\u6d77\u5b87\u5b99\u661f\u5bbf\n\u76ee\u8033\u9f3b\u53e3\u7709\u624b\u8db3\u820c\u76ae\u5fc3\u808c\u809d\u80ba\u8178\n\u3042\u3044\u3046\u3048\u304a\u304b\u304d\u304f\u3051\u3053\u30a2\u30a4\u30a6\u30a8\n</code></pre></li> <li>Generate the worksheet in pdf format with the following command: <pre><code>python3 run.py\n</code></pre></li> <li>Print out the generated file <code>worksheet.pdf</code></li> </ol>"},{"location":"opensource-projects/asian-character-worksheet-generator/#language-supported","title":"Language Supported","text":"<ol> <li>Chinese</li> <li>Japanese (Hiragana and Katagana)</li> </ol>"},{"location":"opensource-projects/asian-character-worksheet-generator/#sample","title":"Sample","text":"<p>sample worksheet</p>"},{"location":"opensource-projects/asian-character-worksheet-generator/#special-thanks","title":"Special Thanks","text":"<p>My lovely sons Tim and Hin. Also thanks TC Dan for giving valuable feedback to this README file. </p>"},{"location":"opensource-projects/asian-character-worksheet-generator/#github-link","title":"Github link","text":"<p>Asian Character Worksheet Generator</p>"},{"location":"opensource-projects/asian-comprehension-worksheet-generator/","title":"Asian Comprehension Worksheet Generator","text":""},{"location":"opensource-projects/asian-comprehension-worksheet-generator/#background","title":"Background","text":"<p>This is a similar project to Asian Character Worksheet Generator.</p> <p>As an immigrant and father of two boys living in America, I want teach my sons comprehension and reading skills of my language. I want to pick my own material which fits their levels.  </p>"},{"location":"opensource-projects/asian-comprehension-worksheet-generator/#benefit-of-the-project","title":"Benefit of the project","text":"<p>The worksheet comes with empty column next to each word column. You can add phonetic note besides the character or use it to practice writing.</p>"},{"location":"opensource-projects/asian-comprehension-worksheet-generator/#requirements","title":"Requirements","text":"<ol> <li>python3</li> <li>pip</li> </ol> <p>Run the following to install required packages <pre><code>pip install -r requirements.txt\n</code></pre></p>"},{"location":"opensource-projects/asian-comprehension-worksheet-generator/#how-to-use","title":"How to Use","text":"<ol> <li>Edit the file <code>text</code> with the content of your choice (or use the sample). <pre><code>\u5149\u8f1d\u6b72\u6708\n\u9418\u8072\u97ff\u8d77\u6b78\u5bb6\u7684\u4fe1\u865f\uff0c\n\u5728\u4ed6\u751f\u547d\u88e1\uff0c\u5f77\u5f7f\u5e36\u9ede\u550f\u5653\u3002\n\u9ed1\u8272\u808c\u819a\u7d66\u4ed6\u7684\u610f\u7fa9\uff0c\n\u662f\u4e00\u751f\u5949\u737b\uff0c\u819a\u8272\u9b25\u722d\u4e2d\u3002\n\u5e74\u6708\u628a\u64c1\u6709\u8b8a\u505a\u5931\u53bb\uff0c\n\u75b2\u5026\u7684\u96d9\u773c\u5e36\u8457\u671f\u671b\u3002\n\u4eca\u5929\u53ea\u6709\u6b98\u7559\u7684\u8ec0\u6bbc\uff0c\n\u8fce\u63a5\u5149\u8f1d\u6b72\u6708\uff0c\n\u98a8\u96e8\u4e2d\u62b1\u7dca\u81ea\u7531\u3002\n\u4e00\u751f\u7d93\u904e\u508d\u5fa8\u7684\u6399\u624e\uff0c\n\u81ea\u4fe1\u53ef\u6539\u8b8a\u672a\u4f86\uff0c\n\u554f\u8ab0\u53c8\u80fd\u505a\u5230\u3002\n\u53ef\u5426\u4e0d\u5206\u819a\u8272\u7684\u754c\u9650\uff0c\n\u9858\u9019\u571f\u5730\u88e1\uff0c\u4e0d\u5206\u4f60\u6211\u9ad8\u4f4e\u3002\n\u7e7d\u7d1b\u8272\u5f69\u9583\u51fa\u7684\u7f8e\u9e97\uff0c\n\u662f\u56e0\u5b83\u6c92\u6709\uff0c\u5206\u958b\u6bcf\u7a2e\u8272\u5f69\u3002\n</code></pre></li> <li>Generate the worksheet in pdf format with the following command: <pre><code>python3 run.py\n</code></pre></li> <li>Print out the generated file <code>worksheet.pdf</code></li> </ol>"},{"location":"opensource-projects/asian-comprehension-worksheet-generator/#language-supported","title":"Language Supported","text":"<ol> <li>Chinese</li> <li>Japanese (Hiragana and Katagana)</li> </ol>"},{"location":"opensource-projects/asian-comprehension-worksheet-generator/#sample","title":"Sample","text":"<p>sample worksheet</p>"},{"location":"opensource-projects/asian-comprehension-worksheet-generator/#code-overview","title":"Code Overview","text":"<p>Everything is written in python in <code>run.py</code>. You can play with the font and grid size with the variables under the <code># Basic settings</code> section.</p>"},{"location":"opensource-projects/asian-comprehension-worksheet-generator/#contributing","title":"Contributing","text":"<p>I appreciate all suggestions or PRs which will help kids learn Asian language better. Feel free to fork the project and create a pull request with your idea.</p>"},{"location":"opensource-projects/asian-comprehension-worksheet-generator/#special-thanks","title":"Special Thanks","text":"<p>My lovely sons Tim and Hin.</p>"},{"location":"opensource-projects/asian-comprehension-worksheet-generator/#github-link","title":"Github link","text":"<p>Asian Comprehension Worksheet Generator</p>"},{"location":"opensource-projects/job-winner/","title":"Job Winner","text":""},{"location":"opensource-projects/job-winner/#background","title":"Background","text":"<p>I lost my job right before the holiday season in 2022. I need to look for a new job and apply for as many as I can while all the big techs are laying off talents.</p> <p>To keep track of job applications:</p> <ol> <li>Use Google Spreadsheet (I was using it before this project). It is ugly, hard to maintain and easy to mess up the layout.</li> <li>Use online tool like huntr.co. But it can only track 40 jobs and I don't want to spend a dime for the premium service.</li> <li>Use pen and paper. It works but I can't copy and paste information such as link job link and utilize it later.</li> </ol> <p>None of the about is fun or totally fit my use case.</p> <p>That's the reason for me to build something for myself, and potentially you, to get the job done (and to get a new job successfully!)</p>"},{"location":"opensource-projects/job-winner/#benefit-of-job-winner","title":"Benefit of Job Winner","text":"<p>Job Winner helps you keep track of all your job applications in one place\u2014totally free! No hidden fees, and you won't have to worry about your personal data being sold.</p> <p>One of the handy features is the Profile Page. This is where you can store all the personal information that job applications often ask for (like your LinkedIn URL). Plus, it has a super handy feature that lets you copy any field to your clipboard with just a click.</p>"},{"location":"opensource-projects/job-winner/#features","title":"Features","text":"<p>Key Functions:</p> <ol> <li> <p>Index Page:</p> <p>View a list of all your job applications in one place, with easy access to each application\u2019s details and status. </p> </li> <li> <p>Create New Job Application:</p> <p>Easily add new job applications to your list with a user-friendly form. </p> </li> <li> <p>Delete Job Application:</p> <p>Remove any applications you no longer need with a simple delete option.</p> </li> <li> <p>Edit Job Application:</p> <p>Make updates to your existing job applications as the status of your applications change or new information becomes available.  - Manage Interview:</p> <p>Keep track of your interviews with the ability to add and manage interview details.    - Manage Offer:</p> <p>Track any job offers you\u2019ve received, including details about the salary and offer date.   </p> </li> <li> <p>Profile Page:</p> <p>Store and manage all your personal information in one spot </p> </li> <li> <p>Interview List:</p> <p>Keep track of all your interviews in one place, with easy-to-view details and statuses plus sortable headers. </p> </li> <li> <p>Offer List:</p> <p>Track all job offers you\u2019ve received with sortable headers. </p> </li> <li> <p>Search and Filter:</p> <p>Quickly search for specific job applications based on keywords to help you stay organized. Whether it's the company name, job title, or description, finding the right job application is easy.</p> </li> <li> <p>Interview and Offer Count:</p> <p>Stay on top of your job search with an overview of how many interviews and offers you currently have, making it easier to manage multiple opportunities.</p> </li> </ol>"},{"location":"opensource-projects/job-winner/#stack","title":"Stack","text":"<ol> <li>Java 17</li> <li>Spring Boot 3.4.0</li> <li>Spring Reactive</li> <li>GraphQL</li> <li>Postgres DB</li> <li>React</li> <li>Material UI</li> </ol>"},{"location":"opensource-projects/job-winner/#github-links","title":"Github links","text":"<p>Job Winner has two components</p> <ol> <li>Backend</li> <li>UI</li> </ol>"},{"location":"opensource-projects/math-worksheet-generator/","title":"Math Worksheet Generator","text":""},{"location":"opensource-projects/math-worksheet-generator/#background","title":"Background","text":"<p>My best friend tests his 5 year old basic math questions from store-bought material which is good for one time use (his son memorizes the answers) \u2026. but he wants to give him more practice.</p> <p>Two solutions:</p> <ol> <li>keep buying more one time usage materials (less beer budget); or</li> <li>make question sets with the number pairs and calculate the answer for each question manually (less beer time)</li> </ol> <p>Not ideal.</p> <p>That's the reason for me to look into an automate way to get the job done.</p>"},{"location":"opensource-projects/math-worksheet-generator/#benefit-of-the-math-worksheet-generator","title":"Benefit of the Math Worksheet Generator","text":"<p>With the Math Worksheet Generator, you can create a PDF with unique questions, as needed, in a fraction of second.</p> <p>There are five choices:</p> <ol> <li>Addition</li> <li>Subtraction</li> <li>Multiplication</li> <li>Division</li> <li>Mixed</li> </ol>"},{"location":"opensource-projects/math-worksheet-generator/#requirements","title":"Requirements","text":"<p>python3</p> <p>Install required package with the following command: <pre><code>pip install -r requirements.txt\n</code></pre></p>"},{"location":"opensource-projects/math-worksheet-generator/#how-to-use","title":"How to Use","text":"<ol> <li>Generate the worksheet in pdf format with the following command: <pre><code>python3 run.py --type [+|-|x|/|mix] --digits [1|2|3] [-q|--question_count] [int] --output [custom-name.pdf]\n</code></pre></li> <li>Print out the generated file <code>worksheet.pdf</code></li> </ol> <p>For addition only worksheet: <pre><code>python3 run.py --type +\n</code></pre> For calculation up to 3 digits range: <pre><code>python3 run.py --digits 3\n</code></pre> For generating different number of question, eg. 100 (default is 80): <pre><code>python3 run.py -q 100\n</code></pre> or <pre><code>python3 run.py --question_count 100\n</code></pre> For custom output filename (default is worksheet.pdf): <pre><code>python3 run.py --output custom-name.pdf\n</code></pre></p>"},{"location":"opensource-projects/math-worksheet-generator/#sample","title":"Sample","text":"<p>sample worksheet</p>"},{"location":"opensource-projects/math-worksheet-generator/#special-thanks","title":"Special Thanks","text":"<p>My long time friend San for the inspiration of this project and lovely sons Tim and Hin. Thanks thedanimal for reviewing this README and adding new features.</p> <p>Also, thank you for the love and support form the Reddit Python community. You guys are amazing and are helping me to make this project better.</p>"},{"location":"opensource-projects/math-worksheet-generator/#successful-story","title":"Successful Story","text":"<p>Thanks k1m0ch1 for sharing this heartwarming story:</p> <p>...I made this card for my kid, and then the teacher asks me if I can make some for the kids, well its generated anyway and very helpful, and the next day he asks me to make for a whole class, and next day he wants me to make for a whole school, and a weeks later other schools want me to help to make for a whole school.       more than 1000 generated file, with a custom filename for every kid and sent to the email     I'm doing it for free, while you made this free, love it &lt;3</p>"},{"location":"opensource-projects/math-worksheet-generator/#coverage","title":"Coverage","text":"<p>The project was featured in the following links:</p> <p>PyCoder's Weekly Issue #442</p> <p>PyCoder's Weekly Twitter</p> <p>Real Python Facebook</p> <p>Github Trends Telegram</p> <p>Python Trending Twitter</p>"},{"location":"opensource-projects/math-worksheet-generator/#github-link","title":"Github link","text":"<p>Math Worksheet Generator</p>"},{"location":"blog/archive/2025/","title":"2025","text":""},{"location":"blog/archive/2024/","title":"2024","text":""},{"location":"blog/archive/2023/","title":"2023","text":""},{"location":"blog/category/devops/","title":"devops","text":""},{"location":"blog/category/oci/","title":"oci","text":""},{"location":"blog/category/terraform/","title":"terraform","text":""},{"location":"blog/category/argocd/","title":"argocd","text":""},{"location":"blog/category/github/","title":"github","text":""},{"location":"blog/category/cicd/","title":"cicd","text":""},{"location":"blog/category/kubernetes/","title":"kubernetes","text":""},{"location":"blog/category/helm/","title":"helm","text":""},{"location":"blog/category/gitops/","title":"gitops","text":""},{"location":"blog/category/automation/","title":"automation","text":""},{"location":"blog/category/tailscale/","title":"tailscale","text":""},{"location":"blog/category/vpn/","title":"vpn","text":""},{"location":"blog/category/networking/","title":"networking","text":""},{"location":"blog/category/sealedsecret/","title":"sealedsecret","text":""},{"location":"blog/category/dex/","title":"dex","text":""},{"location":"blog/category/slackbot/","title":"slackbot","text":""},{"location":"blog/category/python/","title":"python","text":""},{"location":"blog/category/hackathon/","title":"hackathon","text":""},{"location":"blog/category/ansible/","title":"ansible","text":""},{"location":"blog/category/docker/","title":"docker","text":""},{"location":"blog/category/airbyte/","title":"airbyte","text":""},{"location":"blog/category/etl/","title":"etl","text":""},{"location":"blog/category/traefik/","title":"traefik","text":""},{"location":"blog/category/ssl/","title":"ssl","text":""},{"location":"blog/category/cloudfront/","title":"cloudfront","text":""},{"location":"blog/category/aws/","title":"aws","text":""},{"location":"blog/category/route53/","title":"route53","text":""},{"location":"blog/category/s3/","title":"s3","text":""},{"location":"blog/category/eks/","title":"eks","text":""},{"location":"blog/category/shell-script/","title":"shell script","text":""},{"location":"blog/category/sso/","title":"sso","text":""},{"location":"blog/category/turborepo/","title":"turborepo","text":""},{"location":"blog/category/redis/","title":"redis","text":""},{"location":"blog/category/java/","title":"java","text":""},{"location":"blog/category/ruby/","title":"ruby","text":""},{"location":"blog/category/faker/","title":"faker","text":""},{"location":"blog/category/react/","title":"react","text":""},{"location":"blog/category/sql/","title":"sql","text":""},{"location":"blog/category/postgres/","title":"postgres","text":""},{"location":"blog/page/2/","title":"My Blog Posts","text":""},{"location":"blog/page/3/","title":"My Blog Posts","text":""},{"location":"blog/archive/2025/page/2/","title":"2025","text":""},{"location":"blog/category/devops/page/2/","title":"devops","text":""},{"location":"blog/category/devops/page/3/","title":"devops","text":""},{"location":"blog/category/kubernetes/page/2/","title":"kubernetes","text":""}]}